{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae67ee5-5791-4c70-8e23-5dc9e3d1b388",
   "metadata": {},
   "source": [
    "## ASSISITIVE SEARCH ON https://platform.openai.com/docs/\n",
    "\n",
    "- The scraped content retrieved from APIFY actor is used for Question & answering **[pls refer Scraping_screenshots_Apify.ppt]**\n",
    "\n",
    "- The process begins by breaking down the contents of the page into smaller segments (chunks), which are then transformed into embeddings using the Huggingface embedding model **(BAAI/bge-base-en-v1.5)**.\n",
    "- These embeddings, along with metadata, are stored within the Pinecone DB. Leveraging Pinecone's semantic search capabilities, similar matches can be extracted from the database.\n",
    "- Subsequently, a prompt, combined with the user's query and the identified similar matches, is provided as input to the **Mistral7b model**, an open-source LLM. This model generates responses and answers based on the context for the given user query.\n",
    "- Question recommendation utilizes **FAISS** through langchain to fetch similar questions that are stored in a separate file. This approach leverages FAISS, a similarity search library, to identify questions that closely match the user's query, enhancing the recommendation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "908da8f4-7f5b-4621-98ab-326365f417b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "import os\n",
    "import dotenv\n",
    "import pinecone\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e64348-9b48-4571-9671-2cef6f5b8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraped content obtained via APIFY actor\n",
    "openai_docs_data = pd.read_excel(\"./data/openai_docs.xlsx\")\n",
    "\n",
    "# Remove duplicates from the 'text' column\n",
    "openai_docs_final =openai_docs_data .drop_duplicates(subset='text')\n",
    "\n",
    "# Display the DataFrame without duplicates\n",
    "openai_docs_final.info()\n",
    "openai_docs_final.to_excel('./data/openai_docs_final.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877700ad-37e4-4ab3-8dc7-1ff60ee6949a",
   "metadata": {},
   "source": [
    "#### Huggingface embedding model \"BAAI/bge-base-en-v1.5\" is used to convert text into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd707d1-3d62-4c5d-833f-af76ea3b8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HF_API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce4420a-20d8-44d1-ab4e-3cac889cb9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-base-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-base-en-v1.5')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be521dd8-ad05-4eca-a651-a4f4cc8cdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        # Perform pooling. In this case, cls pooling.\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "    # normalize embeddings\n",
    "    text_embeddings = torch.nn.functional.normalize(sentence_embeddings).tolist()    \n",
    "    return text_embeddings\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "84200a1c-0ec7-4645-9f82-e3e6a2ecfae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32 entries, 0 to 31\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   crawl/depth            32 non-null     int64  \n",
      " 1   crawl/httpStatusCode   32 non-null     int64  \n",
      " 2   crawl/loadedTime       32 non-null     object \n",
      " 3   crawl/loadedUrl        32 non-null     object \n",
      " 4   crawl/referrerUrl      32 non-null     object \n",
      " 5   markdown               32 non-null     object \n",
      " 6   metadata/author        0 non-null      float64\n",
      " 7   metadata/canonicalUrl  32 non-null     object \n",
      " 8   metadata/description   32 non-null     object \n",
      " 9   metadata/keywords      0 non-null      float64\n",
      " 10  metadata/languageCode  32 non-null     object \n",
      " 11  metadata/title         32 non-null     object \n",
      " 12  screenshotUrl          0 non-null      float64\n",
      " 13  text                   32 non-null     object \n",
      " 14  url                    32 non-null     object \n",
      "dtypes: float64(3), int64(2), object(10)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#All of the pages in the OpenAi documentation are scraped and saved in an excel spreadsheet using Apify. Each record contains the page's text and relevant metadata.\n",
    "openai_docs = pd.read_excel(\"./data/openai_docs_final.xlsx\")\n",
    "openai_docs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0257d0-dc09-4033-90ae-093d6a7ea4f7",
   "metadata": {},
   "source": [
    "#### CHUNKING AND CONVERTING THE CHUNK INTO EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "91522cab-2b54-49f6-87e4-0eec43ffbef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Introduction\n",
      "The OpenAI API can be applied to virtually any task that requires understanding or generating natural language and code. The OpenAI API can also be used to generate and edit images or convert speech into text. We offer a range of models with different capabilities and price points, as well as the ability to fine-tune custom models.\n",
      "Resources\n",
      "Experiment in the playground\n",
      "Read the API reference\n",
      "Visit the help center\n",
      "View the current API status\n",
      "Check out the OpenAI Developer Forum\n",
      "Learn about our usage policies\n",
      "At OpenAI, protecting user data is fundamental to our mission. We do not train our models on inputs and outputs through our API. Learn more on our API data privacy page.\n",
      "Key concepts\n",
      "GPTs\n",
      "0 --------------------------------------------------------------------------------\n",
      "1 Key concepts\n",
      "GPTs\n",
      "OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \"program\" a GPT model, usually by providing instructions or some examples of how to successfully complete a task. GPTs can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory GPT guide and in our GPT best practices guide.\n",
      "Embeddings\n",
      "0 --------------------------------------------------------------------------------\n",
      "2 Embeddings\n",
      "An embedding is a vector representation of a piece of data (e.g. some text) that is meant to preserve aspects of its content and/or its meaning. Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly detection, classification, and more. Read more about embeddings in our embeddings guide.\n",
      "Tokens\n",
      "0 --------------------------------------------------------------------------------\n",
      "3 Tokens\n",
      "GPT and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string \" tokenization\" is decomposed as \" token\" and \"ization\", while a short and common word like \" the\" is represented as a single token. Note that in a sentence, the first token of each word typically starts with a space character. Check out our tokenizer tool to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.\n",
      "One limitation to keep in mind is that for a GPT model the prompt and the generated output combined must be no more than the model's maximum context length. For embeddings models (which do not output tokens), the input must be shorter than the model's maximum context length. The maximum context lengths for each GPT and embeddings model can be found in the model index.\n",
      "Guides\n",
      "Jump into one of our guides to learn more.\n",
      "Quickstart tutorial\n",
      "0 --------------------------------------------------------------------------------\n",
      "4 Quickstart tutorial\n",
      "Learn by building a quick sample application\n",
      "GPT\n",
      "Learn how to generate text\n",
      "GPT best practices\n",
      "Learn best practices for building with GPT models\n",
      "Embeddings\n",
      "Learn how to search, classify, and compare text\n",
      "Speech to text\n",
      "Learn how to turn speech into text\n",
      "Image generation\n",
      "Learn how to generate or edit images\n",
      "Fine-tuning\n",
      "Learn how to train a model for your use case\n",
      "0 --------------------------------------------------------------------------------\n",
      "0 Plugin review process\n",
      "We are in the early stages of building the plugin store, this page outlines the basic overview of how we are thinking about the plugin review process, and what the plugin review process will look like.\n",
      "The plugin review process will change significantly over time. We are open to feedback about how to improve the process for those building plugins.\n",
      "What makes a great plugin\n",
      "The purpose of the review process is to ensure that plugins on ChatGPT are safe, provide useful functionality, and provide a high-quality user experience. Long-term, we expect it to be routine to go through the review process as we formalize it.\n",
      "In the immediate term, we expect that plugins which deliver new, magical experiences for users, which would not have been possible without the unique capabilities of large language models, will deliver the most value.\n",
      "So far, some categories of plugins that have been the most magical are:\n",
      "1 --------------------------------------------------------------------------------\n",
      "1 Retrieval over user-specific or otherwise hard-to-search knowledge sources (searching over Slack, searching a user’s docs or another proprietary database).\n",
      "Plugins that synergize well with other plugins (asking the model to plan a weekend for you, and having the model blend usage of flight/hotel search with dinner reservation search).\n",
      "Plugins that give the model computational abilities (Wolfram, OpenAI Code Interpreter, etc).\n",
      "Plugins that introduce new ways of using ChatGPT, like games.\n",
      "Plugin states\n",
      "When developing a plugin, the plugin can be in one of several statuses, indicating where it is along the review process and who can use it. Right now, there are only a couple of plugin statuses. We expect this to change as the plugins system evolves.\n",
      "StatusDescriptionDeveloper accessUsers access\n",
      "Unverified\tThe default status that a plugin starts out in.\t100\t0\t\n",
      "Approved\tOpenAI has reviewed the plugin, and has determined that the plugin is approved for use by a general audience of users.\tUnlimited\tUnlimited\n",
      "1 --------------------------------------------------------------------------------\n",
      "2 Banned\tOpenAI has reviewed the plugin, and has determined that the plugin should be banned.\t0\t0\t\n",
      "Note that if you submit a plugin and it is rejected because it fails to meet the requirements, it would still be in the \"Unverified\" state.\n",
      "Types of users\n",
      "Right now there are three categories of users that we talk about when it comes to plugins access.\n",
      "User typeDescription\n",
      "ChatGPT Plus users\tChatGPT Plus users who have been given access to plugins that have gone through our review process and have been approved for general use.\t\n",
      "Plugin developers\tChatGPT Plus users who have been given the ability to develop, use, and test plugins that are in development.\t\n",
      "Normal ChatGPT users\tRight now, normal ChatGPT users without plug don’t have plugin access.\t\n",
      "Plugin store\n",
      "In order to have your plugin available in the plugin store, it needs to be reviewed by OpenAI. Before submitting your plugin for review, make sure your plugin fits the criteria below:\n",
      "Adheres to our content policy\n",
      "Complies with our brand guidelines\n",
      "1 --------------------------------------------------------------------------------\n",
      "3 Complies with our brand guidelines\n",
      "Functions as described in your submission\n",
      "Provides informative error messages\n",
      "Features descriptive operation names\n",
      "Offers a simple and concise manifest file\n",
      "Uses correct grammar and ends the plugin description with punctuation\n",
      "States geographical or functional limitations clearly in the plugin description to avoid user confusion\n",
      "Does not use words like plugin, ChatGPT, or OpenAI in the plugin name or description\n",
      "Plugin name does not attempt to game or manipulate the search rankings by arbitrarily starting with the letter \"A\", \"A+\", or other variations which provide no additional useful information to end users\n",
      "Plugin enforces user confirmation before taking an action (see Zapier for an example of this)\n",
      "If the plugin takes actions in the world, it needs to use OAuth\n",
      "Does not require the user to install software on their device\n",
      "Does not use non-OpenAI generative image models (for safety reasons, we cannot verify these external models). Please use DALL·E.\n",
      "1 --------------------------------------------------------------------------------\n",
      "4 If any one of these criteria are missing, we will reject the plugin and you can submit it again once it is updated.\n",
      "Submit a plugin for review\n",
      "You can expect to hear back about a plugin you submit for review ~7 days after you submit the plugin.\n",
      "We are currently reviewing new plugins on a rolling basis. You can submit a plugin for review using the plugin submission bot. To see the bot, you need to be signed in.\n",
      "To view the status of your plugin submission, make sure you are logged in and select \"Help\" in the top right corner of this page. Under \"Messages\", you will be able to see your plugin submission. We will notify you as the status of your plugin changes during the review process.\n",
      "1 --------------------------------------------------------------------------------\n",
      "0 Models\n",
      "Overview\n",
      "The OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make customizations to our models for your specific use case with fine-tuning.\n",
      "ModelsDescription\n",
      "GPT-4\tA set of models that improve on GPT-3.5 and can understand as well as generate natural language or code\t\n",
      "GPT-3.5\tA set of models that improve on GPT-3 and can understand as well as generate natural language or code\t\n",
      "GPT base\tA set of models without instruction following that can understand as well as generate natural language or code\t\n",
      "DALL·E\tA model that can generate and edit images given a natural language prompt\t\n",
      "Whisper\tA model that can convert audio into text\t\n",
      "Embeddings\tA set of models that can convert text into a numerical form\t\n",
      "Moderation\tA fine-tuned model that can detect whether text may be sensitive or unsafe\t\n",
      "GPT-3\n",
      "Legacy\n",
      "\tA set of models that can understand and generate natural language\t\n",
      "Deprecated\tA full list of models that have been deprecated\n",
      "2 --------------------------------------------------------------------------------\n",
      "1 We have also published open source models including Point-E, Whisper, Jukebox, and CLIP.\n",
      "Visit our model index for researchers to learn more about which models have been featured in our research papers and the differences between model series like InstructGPT and GPT-3.5.\n",
      "Continuous model upgrades\n",
      "Based on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.\n",
      "With the release of gpt-3.5-turbo, some of our models are now being continually updated. gpt-3.5-turbo, gpt-4, and gpt-4-32k point to the latest model version. You can verify this by looking at the response object after sending a ChatCompletion request. The response will include the specific model version used (e.g. gpt-3.5-turbo-0613).\n",
      "2 --------------------------------------------------------------------------------\n",
      "2 We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.\n",
      "The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.\n",
      "Model nameDiscontinuation dateReplacement model\n",
      "gpt-3.5-turbo-0301\tat earliest 2024-06-13\tgpt-3.5-turbo-0613\t\n",
      "gpt-4-0314\tat earliest 2024-06-13\tgpt-4-0613\t\n",
      "gpt-4-32k-0314\tat earliest 2024-06-13\tgpt-4-32k-0613\t\n",
      "Learn more about model deprecation on our deprecation page.\n",
      "GPT-4\n",
      "GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.\n",
      "2 --------------------------------------------------------------------------------\n",
      "3 GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks using the Chat completions API. Learn how to use GPT-4 in our GPT guide.\n",
      "Latest modelDescriptionMax tokensTraining data\n",
      "gpt-4\tMore capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.\t8,192 tokens\tUp to Sep 2021\t\n",
      "gpt-4-0613\tSnapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.\t8,192 tokens\tUp to Sep 2021\n",
      "2 --------------------------------------------------------------------------------\n",
      "4 gpt-4-32k\tSame capabilities as the standard gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.\t32,768 tokens\tUp to Sep 2021\t\n",
      "gpt-4-32k-0613\tSnapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.\t32,768 tokens\tUp to Sep 2021\t\n",
      "gpt-4-0314 (Legacy)\tSnapshot of gpt-4 from March 14th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated on June 13th 2024 at the earliest.\t8,192 tokens\tUp to Sep 2021\t\n",
      "gpt-4-32k-0314 (Legacy)\tSnapshot of gpt-4-32 from March 14th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated on June 13th 2024 at the earliest.\t32,768 tokens\tUp to Sep 2021\t\n",
      "For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.\n",
      "GPT-3.5\n",
      "2 --------------------------------------------------------------------------------\n",
      "5 GPT-3.5\n",
      "GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat using the Chat completions API but works well for traditional completions tasks as well.\n",
      "Latest modelDescriptionMax tokensTraining data\n",
      "gpt-3.5-turbo\tMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.\t4,097 tokens\tUp to Sep 2021\t\n",
      "gpt-3.5-turbo-16k\tSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.\t16,385 tokens\tUp to Sep 2021\t\n",
      "gpt-3.5-turbo-instruct\tSimilar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.\t4,097 tokens\tUp to Sep 2021\n",
      "2 --------------------------------------------------------------------------------\n",
      "6 gpt-3.5-turbo-0613\tSnapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.\t4,097 tokens\tUp to Sep 2021\t\n",
      "gpt-3.5-turbo-16k-0613\tSnapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.\t16,385 tokens\tUp to Sep 2021\t\n",
      "gpt-3.5-turbo-0301 (Legacy)\tSnapshot of gpt-3.5-turbo from March 1st 2023. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated on June 13th 2024 at the earliest.\t4,097 tokens\tUp to Sep 2021\t\n",
      "text-davinci-003 (Legacy)\tCan do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.\t4,097 tokens\tUp to Jun 2021\n",
      "2 --------------------------------------------------------------------------------\n",
      "7 text-davinci-002 (Legacy)\tSimilar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning\t4,097 tokens\tUp to Jun 2021\t\n",
      "code-davinci-002 (Legacy)\tOptimized for code-completion tasks\t8,001 tokens\tUp to Jun 2021\t\n",
      "We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.\n",
      "OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.\n",
      "GPT base\n",
      "GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for our original GPT-3 base models and use the legacy Completions API. Most customers should use GPT-3.5 or GPT-4.\n",
      "Latest modelDescriptionMax tokensTraining data\n",
      "babbage-002\tReplacement for the GPT-3 ada and babbage base models.\t16,384 tokens\tUp to Sep 2021\n",
      "2 --------------------------------------------------------------------------------\n",
      "8 davinci-002\tReplacement for the GPT-3 curie and davinci base models.\t16,384 tokens\tUp to Sep 2021\t\n",
      "DALL·E\n",
      "DALL·E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.\n",
      "The current DALL·E model available through our API is the 2nd iteration of DALL·E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.\n",
      "Whisper\n",
      "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.\n",
      "2 --------------------------------------------------------------------------------\n",
      "9 Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.\n",
      "Embeddings\n",
      "Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.\n",
      "Moderation\n",
      "2 --------------------------------------------------------------------------------\n",
      "10 Moderation\n",
      "The Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.\n",
      "Moderation models take in an arbitrary sized input that is automatically broken up into chunks of 4,096 tokens. In cases where the input is more than 32,768 tokens, truncation is used which in a rare condition may omit a small number of tokens from the moderation check.\n",
      "The final results from each request to the moderation endpoint shows the maximum value on a per category basis. For example, if one chunk of 4K tokens had a category score of 0.9901 and the other had a score of 0.1901, the results would show 0.9901 in the API response since it is higher.\n",
      "ModelDescriptionMax tokens\n",
      "2 --------------------------------------------------------------------------------\n",
      "11 ModelDescriptionMax tokens\n",
      "text-moderation-latest\tMost capable moderation model. Accuracy will be slightly higher than the stable model.\t32,768\t\n",
      "text-moderation-stable\tAlmost as capable as the latest model, but slightly older.\t32,768\t\n",
      "GPT-3 models can understand and generate natural language. These models were superseded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.\n",
      "Latest modelDescriptionMax tokensTraining data\n",
      "text-curie-001\tVery capable, faster and lower cost than Davinci.\t2,049 tokens\tUp to Oct 2019\t\n",
      "text-babbage-001\tCapable of straightforward tasks, very fast, and lower cost.\t2,049 tokens\tUp to Oct 2019\t\n",
      "text-ada-001\tCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.\t2,049 tokens\tUp to Oct 2019\t\n",
      "davinci\tMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.\t2,049 tokens\tUp to Oct 2019\n",
      "2 --------------------------------------------------------------------------------\n",
      "12 curie\tVery capable, but faster and lower cost than Davinci.\t2,049 tokens\tUp to Oct 2019\t\n",
      "babbage\tCapable of straightforward tasks, very fast, and lower cost.\t2,049 tokens\tUp to Oct 2019\t\n",
      "ada\tCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.\t2,049 tokens\tUp to Oct 2019\t\n",
      "How we use your data\n",
      "Your data is your data.\n",
      "As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explicitly opt in). One advantage to opting in is that the models may get better at your use case over time.\n",
      "To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.\n",
      "2 --------------------------------------------------------------------------------\n",
      "13 Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL·E Labs.\n",
      "Default usage policies by endpoint\n",
      "EndpointData used for trainingDefault retentionEligible for zero retention\n",
      "/v1/completions\tNo\t30 days\tYes\t\n",
      "/v1/chat/completions\tNo\t30 days\tYes\t\n",
      "/v1/edits\tNo\t30 days\tYes\t\n",
      "/v1/images/generations\tNo\t30 days\tNo\t\n",
      "/v1/images/edits\tNo\t30 days\tNo\t\n",
      "/v1/images/variations\tNo\t30 days\tNo\t\n",
      "/v1/embeddings\tNo\t30 days\tYes\t\n",
      "/v1/audio/transcriptions\tNo\tZero data retention\t-\t\n",
      "/v1/audio/translations\tNo\tZero data retention\t-\t\n",
      "/v1/files\tNo\tUntil deleted by customer\tNo\t\n",
      "/v1/fine_tuning/jobs\tNo\tUntil deleted by customer\tNo\t\n",
      "/v1/fine-tunes\tNo\tUntil deleted by customer\tNo\t\n",
      "/v1/moderations\tNo\tZero data retention\t-\t\n",
      "For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.\n",
      "Model endpoint compatibility\n",
      "EndpointLatest models\n",
      "/v1/audio/transcriptions\twhisper-1\t\n",
      "/v1/audio/translations\twhisper-1\n",
      "2 --------------------------------------------------------------------------------\n",
      "14 /v1/audio/translations\twhisper-1\t\n",
      "/v1/chat/completions\tgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613\t\n",
      "/v1/completions (Legacy)\tgpt-3.5-turbo-instruct, babbage-002, davinci-002\t\n",
      "/v1/embeddings\ttext-embedding-ada-002\t\n",
      "/v1/fine_tuning/jobs\tgpt-3.5-turbo, babbage-002, davinci-002\t\n",
      "/v1/moderations\ttext-moderation-stable, text-moderation-latest\t\n",
      "This list excludes all of our DALL·E models and our deprecated models.\n",
      "2 --------------------------------------------------------------------------------\n",
      "0 Introduction\n",
      "You can interact with the API through HTTP requests from any language, via our official Python bindings, our official Node.js library, or a community-maintained library.\n",
      "To install the official Python bindings, run the following command:\n",
      "To install the official Node.js library, run the following command in your Node.js project directory:\n",
      "npm install openai@^4.0.0\n",
      "Authentication\n",
      "The OpenAI API uses API keys for authentication. Visit your API Keys page to retrieve the API key you'll use in your requests.\n",
      "Remember that your API key is a secret! Do not share it with others or expose it in any client-side code (browsers, apps). Production requests must be routed through your own backend server where your API key can be securely loaded from an environment variable or key management service.\n",
      "All API requests should include your API key in an Authorization HTTP header as follows:\n",
      "Authorization: Bearer OPENAI_API_KEY\n",
      "Organization (optional)\n",
      "3 --------------------------------------------------------------------------------\n",
      "1 Organization (optional)\n",
      "For users who belong to multiple organizations, you can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's subscription quota.\n",
      "Example curl command:\n",
      "1 2 3 curl https://api.openai.com/v1/models \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -H \"OpenAI-Organization: YOUR_ORG_ID\"\n",
      "Example with the openai Python package:\n",
      "1 2 3 4 5 import os import openai openai.organization = \"YOUR_ORG_ID\" openai.api_key = os.getenv(\"OPENAI_API_KEY\") openai.Model.list()\n",
      "Example with the openai Node.js package:\n",
      "1 2 3 4 5 6 7 import { Configuration, OpenAIApi } from \"openai\"; const configuration = new Configuration({ organization: \"YOUR_ORG_ID\", apiKey: process.env.OPENAI_API_KEY, }); const openai = new OpenAIApi(configuration); const response = await openai.listEngines();\n",
      "Organization IDs can be found on your Organization settings page.\n",
      "Making requests\n",
      "3 --------------------------------------------------------------------------------\n",
      "2 Making requests\n",
      "You can paste the command below into your terminal to run your first API request. Make sure to replace $OPENAI_API_KEY with your secret API key.\n",
      "1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"model\": \"gpt-3.5-turbo\", \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}], \"temperature\": 0.7 }'\n",
      "This request queries the gpt-3.5-turbo model (which under the hood points to the latest gpt-3.5-turbo model variant) to complete the text starting with a prompt of \"Say this is a test\". You should get a response back that resembles the following:\n",
      "3 --------------------------------------------------------------------------------\n",
      "3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"id\": \"chatcmpl-abc123\", \"object\": \"chat.completion\", \"created\": 1677858242, \"model\": \"gpt-3.5-turbo-0613\", \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 7, \"total_tokens\": 20 }, \"choices\": [ { \"message\": { \"role\": \"assistant\", \"content\": \"\\n\\nThis is a test!\" }, \"finish_reason\": \"stop\", \"index\": 0 } ] }\n",
      "Now that you've generated your first chat completion, let's break down the response object. We can see the finish_reason is stop which means the API returned the full chat completion generated by the model without running into any limits. In the choices list, we only generated a single message but you can set the n parameter to generate multiple messages choices.\n",
      "Create transcription\n",
      "post https://api.openai.com/v1/audio/transcriptions\n",
      "Transcribes audio into the input language.\n",
      "Request body\n",
      "The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
      "3 --------------------------------------------------------------------------------\n",
      "4 ID of the model to use. Only whisper-1 is currently available.\n",
      "The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.\n",
      "An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\n",
      "The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt.\n",
      "The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\n",
      "Returns\n",
      "1 2 3 4 5 curl https://api.openai.com/v1/audio/transcriptions \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -H \"Content-Type: multipart/form-data\" \\ -F file=\"@/path/to/file/audio.mp3\" \\ -F model=\"whisper-1\"\n",
      "3 --------------------------------------------------------------------------------\n",
      "5 1 2 3 { \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that.\" }\n",
      "Create translation\n",
      "post https://api.openai.com/v1/audio/translations\n",
      "Translates audio into English.\n",
      "Request body\n",
      "The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
      "ID of the model to use. Only whisper-1 is currently available.\n",
      "An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English.\n",
      "The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt.\n",
      "The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\n",
      "3 --------------------------------------------------------------------------------\n",
      "6 Returns\n",
      "1 2 3 4 5 curl https://api.openai.com/v1/audio/translations \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -H \"Content-Type: multipart/form-data\" \\ -F file=\"@/path/to/file/german.m4a\" \\ -F model=\"whisper-1\"\n",
      "1 2 3 { \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you heading today?\" }\n",
      "Chat\n",
      "Given a list of messages comprising a conversation, the model will return a response.\n",
      "Related guide: Chat completions\n",
      "The chat completion object\n",
      "Represents a chat completion response returned by model, based on the provided input.\n",
      "A unique identifier for the chat completion.\n",
      "A list of chat completion choices. Can be more than one if n is greater than 1.\n",
      "The Unix timestamp (in seconds) of when the chat completion was created.\n",
      "The model used for the chat completion.\n",
      "The object type, which is always chat.completion.\n",
      "Usage statistics for the completion request.\n",
      "3 --------------------------------------------------------------------------------\n",
      "7 Usage statistics for the completion request.\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\": \"chatcmpl-123\", \"object\": \"chat.completion\", \"created\": 1677652288, \"model\": \"gpt-3.5-turbo-0613\", \"choices\": [{ \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"\\n\\nHello there, how may I assist you today?\", }, \"finish_reason\": \"stop\" }], \"usage\": { \"prompt_tokens\": 9, \"completion_tokens\": 12, \"total_tokens\": 21 } }\n",
      "The chat completion chunk object\n",
      "Represents a streamed chunk of a chat completion response returned by model, based on the provided input.\n",
      "A unique identifier for the chat completion. Each chunk has the same ID.\n",
      "A list of chat completion choices. Can be more than one if n is greater than 1.\n",
      "The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.\n",
      "The model to generate the completion.\n",
      "The object type, which is always chat.completion.chunk.\n",
      "3 --------------------------------------------------------------------------------\n",
      "8 1 2 3 4 5 6 7 8 9 10 11 12 13 {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]} {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]} {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"!\"},\"finish_reason\":null}]} .... {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" today\"},\"finish_reason\":null}]} {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"?\"},\"finish_reason\":null}]}\n",
      "3 --------------------------------------------------------------------------------\n",
      "9 {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0613\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n",
      "3 --------------------------------------------------------------------------------\n",
      "10 Create chat completion\n",
      "post https://api.openai.com/v1/chat/completions\n",
      "Creates a model response for the given chat conversation.\n",
      "Request body\n",
      "Controls how the model calls functions. \"none\" means the model will not call a function and instead generates a message. \"auto\" means the model can pick between generating a message or calling a function. Specifying a particular function via {\"name\": \"my_function\"} forces the model to call that function. \"none\" is the default when no functions are present. \"auto\" is the default if functions are present.\n",
      "A list of functions the model may generate JSON inputs for.\n",
      "Modify the likelihood of specified tokens appearing in the completion.\n",
      "3 --------------------------------------------------------------------------------\n",
      "11 Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n",
      "The maximum number of tokens to generate in the chat completion.\n",
      "The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.\n",
      "How many chat completion choices to generate for each input message.\n",
      "Up to 4 sequences where the API will stop generating further tokens.\n",
      "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example Python code.\n",
      "3 --------------------------------------------------------------------------------\n",
      "12 What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
      "We generally recommend altering this or top_p but not both.\n",
      "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
      "We generally recommend altering this or temperature but not both.\n",
      "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n",
      "Returns\n",
      "No StreamingStreamingFunction calling\n",
      "3 --------------------------------------------------------------------------------\n",
      "13 Returns\n",
      "No StreamingStreamingFunction calling\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 curl https://api.openai.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"model\": \"gpt-3.5-turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"Hello!\" } ] }'\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\": \"chatcmpl-123\", \"object\": \"chat.completion\", \"created\": 1677652288, \"model\": \"gpt-3.5-turbo-0613\", \"choices\": [{ \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"\\n\\nHello there, how may I assist you today?\", }, \"finish_reason\": \"stop\" }], \"usage\": { \"prompt_tokens\": 9, \"completion_tokens\": 12, \"total_tokens\": 21 } }\n",
      "Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position. We recommend most users use our Chat completions API. Learn more\n",
      "Related guide: Legacy Completions\n",
      "3 --------------------------------------------------------------------------------\n",
      "14 Related guide: Legacy Completions\n",
      "Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).\n",
      "A unique identifier for the completion.\n",
      "The list of completion choices the model generated for the input prompt.\n",
      "The Unix timestamp (in seconds) of when the completion was created.\n",
      "The model used for completion.\n",
      "The object type, which is always \"text_completion\"\n",
      "Usage statistics for the completion request.\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\", \"object\": \"text_completion\", \"created\": 1589478378, \"model\": \"gpt-3.5-turbo\", \"choices\": [ { \"text\": \"\\n\\nThis is indeed a test\", \"index\": 0, \"logprobs\": null, \"finish_reason\": \"length\" } ], \"usage\": { \"prompt_tokens\": 5, \"completion_tokens\": 7, \"total_tokens\": 12 } }\n",
      "post https://api.openai.com/v1/completions\n",
      "Creates a completion for the provided prompt and parameters.\n",
      "Request body\n",
      "3 --------------------------------------------------------------------------------\n",
      "15 Request body\n",
      "ID of the model to use. You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.\n",
      "The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n",
      "Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n",
      "Generates best_of completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\n",
      "When used with n, best_of controls the number of candidate completions and n specifies how many to return – best_of must be greater than n.\n",
      "Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.\n",
      "Echo back the prompt in addition to the completion\n",
      "3 --------------------------------------------------------------------------------\n",
      "16 Modify the likelihood of specified tokens appearing in the completion.\n",
      "Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n",
      "As an example, you can pass {\"50256\": -100} to prevent the <|endoftext|> token from being generated.\n",
      "Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.\n",
      "3 --------------------------------------------------------------------------------\n",
      "17 The maximum value for logprobs is 5.\n",
      "The maximum number of tokens to generate in the completion.\n",
      "The token count of your prompt plus max_tokens cannot exceed the model's context length. Example Python code for counting tokens.\n",
      "How many completions to generate for each prompt.\n",
      "Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.\n",
      "Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
      "Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example Python code.\n",
      "The suffix that comes after a completion of inserted text.\n",
      "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
      "3 --------------------------------------------------------------------------------\n",
      "18 We generally recommend altering this or top_p but not both.\n",
      "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
      "We generally recommend altering this or temperature but not both.\n",
      "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n",
      "Returns\n",
      "Returns a completion object, or a sequence of completion objects if the request is streamed.\n",
      "1 2 3 4 5 6 7 8 9 curl https://api.openai.com/v1/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"model\": \"gpt-3.5-turbo-instruct\", \"prompt\": \"Say this is a test\", \"max_tokens\": 7, \"temperature\": 0 }'\n",
      "3 --------------------------------------------------------------------------------\n",
      "19 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\", \"object\": \"text_completion\", \"created\": 1589478378, \"model\": \"gpt-3.5-turbo-instruct\", \"choices\": [ { \"text\": \"\\n\\nThis is indeed a test\", \"index\": 0, \"logprobs\": null, \"finish_reason\": \"length\" } ], \"usage\": { \"prompt_tokens\": 5, \"completion_tokens\": 7, \"total_tokens\": 12 } }\n",
      "Embeddings\n",
      "Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.\n",
      "Related guide: Embeddings\n",
      "The embedding object\n",
      "Represents an embedding vector returned by embedding endpoint.\n",
      "The index of the embedding in the list of embeddings.\n",
      "The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the embedding guide.\n",
      "The object type, which is always \"embedding\".\n",
      "1 2 3 4 5 6 7 8 9 10 { \"object\": \"embedding\", \"embedding\": [ 0.0023064255, -0.009327292, .... (1536 floats total for ada-002) -0.0028842222, ], \"index\": 0 }\n",
      "Create embeddings\n",
      "3 --------------------------------------------------------------------------------\n",
      "20 Create embeddings\n",
      "post https://api.openai.com/v1/embeddings\n",
      "Creates an embedding vector representing the input text.\n",
      "Request body\n",
      "Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for text-embedding-ada-002) and cannot be an empty string. Example Python code for counting tokens.\n",
      "ID of the model to use. You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.\n",
      "The format to return the embeddings in. Can be either float or base64.\n",
      "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n",
      "Returns\n",
      "3 --------------------------------------------------------------------------------\n",
      "21 Returns\n",
      "1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/embeddings \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"float\" }'\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"object\": \"list\", \"data\": [ { \"object\": \"embedding\", \"embedding\": [ 0.0023064255, -0.009327292, .... (1536 floats total for ada-002) -0.0028842222, ], \"index\": 0 } ], \"model\": \"text-embedding-ada-002\", \"usage\": { \"prompt_tokens\": 8, \"total_tokens\": 8 } }\n",
      "The fine-tuning job object\n",
      "The fine_tuning.job object represents a fine-tuning job that has been created through the API.\n",
      "The object identifier, which can be referenced in the API endpoints.\n",
      "The Unix timestamp (in seconds) for when the fine-tuning job was created.\n",
      "For fine-tuning jobs that have failed, this will contain more information on the cause of the failure.\n",
      "3 --------------------------------------------------------------------------------\n",
      "22 The name of the fine-tuned model that is being created. The value will be null if the fine-tuning job is still running.\n",
      "The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be null if the fine-tuning job is still running.\n",
      "The hyperparameters used for the fine-tuning job. See the fine-tuning guide for more details.\n",
      "The base model that is being fine-tuned.\n",
      "The object type, which is always \"fine_tuning.job\".\n",
      "The organization that owns the fine-tuning job.\n",
      "The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the Files API.\n",
      "The current status of the fine-tuning job, which can be either validating_files, queued, running, succeeded, failed, or cancelled.\n",
      "The total number of billable tokens processed by this fine-tuning job. The value will be null if the fine-tuning job is still running.\n",
      "The file ID used for training. You can retrieve the training data with the Files API.\n",
      "3 --------------------------------------------------------------------------------\n",
      "23 The file ID used for validation. You can retrieve the validation results with the Files API.\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"object\": \"fine_tuning.job\", \"id\": \"ftjob-abc123\", \"model\": \"davinci-002\", \"created_at\": 1692661014, \"finished_at\": 1692661190, \"fine_tuned_model\": \"ft:davinci-002:my-org:custom_suffix:7q8mpxmy\", \"organization_id\": \"org-123\", \"result_files\": [ \"file-abc123\" ], \"status\": \"succeeded\", \"validation_file\": null, \"training_file\": \"file-abc123\", \"hyperparameters\": { \"n_epochs\": 4, }, \"trained_tokens\": 5768 }\n",
      "Create fine-tuning job\n",
      "post https://api.openai.com/v1/fine_tuning/jobs\n",
      "Creates a job that fine-tunes a specified model from a given dataset.\n",
      "Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.\n",
      "Learn more about fine-tuning\n",
      "Request body\n",
      "The ID of an uploaded file that contains training data.\n",
      "See upload file for how to upload a file.\n",
      "3 --------------------------------------------------------------------------------\n",
      "24 See upload file for how to upload a file.\n",
      "Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with the purpose fine-tune.\n",
      "See the fine-tuning guide for more details.\n",
      "The hyperparameters used for the fine-tuning job.\n",
      "A string of up to 18 characters that will be added to your fine-tuned model name.\n",
      "For example, a suffix of \"custom-model-name\" would produce a model name like ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel.\n",
      "The ID of an uploaded file that contains validation data.\n",
      "If you provide this file, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should not be present in both train and validation files.\n",
      "Your dataset must be formatted as a JSONL file. You must upload your file with the purpose fine-tune.\n",
      "See the fine-tuning guide for more details.\n",
      "Returns\n",
      "No hyperparametersHyperparametersValidation file\n",
      "3 --------------------------------------------------------------------------------\n",
      "25 No hyperparametersHyperparametersValidation file\n",
      "1 2 3 4 5 6 7 curl https://api.openai.com/v1/fine_tuning/jobs \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"training_file\": \"file-abc123\", \"model\": \"gpt-3.5-turbo\" }'\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 { \"object\": \"fine_tuning.job\", \"id\": \"ftjob-abc123\", \"model\": \"gpt-3.5-turbo-0613\", \"created_at\": 1614807352, \"fine_tuned_model\": null, \"organization_id\": \"org-123\", \"result_files\": [], \"status\": \"queued\", \"validation_file\": null, \"training_file\": \"file-abc123\", }\n",
      "List fine-tuning jobs\n",
      "get https://api.openai.com/v1/fine_tuning/jobs\n",
      "List your organization's fine-tuning jobs\n",
      "Query parameters\n",
      "Identifier for the last job from the previous pagination request.\n",
      "Number of fine-tuning jobs to retrieve.\n",
      "Returns\n",
      "1 2 curl https://api.openai.com/v1/fine_tuning/jobs?limit=2 \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "3 --------------------------------------------------------------------------------\n",
      "26 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"object\": \"list\", \"data\": [ { \"object\": \"fine_tuning.job.event\", \"id\": \"ft-event-TjX0lMfOniCZX64t9PUQT5hn\", \"created_at\": 1689813489, \"level\": \"warn\", \"message\": \"Fine tuning process stopping due to job cancellation\", \"data\": null, \"type\": \"message\" }, { ... }, { ... } ], \"has_more\": true }\n",
      "Retrieve fine-tuning job\n",
      "get https://api.openai.com/v1/fine_tuning/jobs/{fine_tuning_job_id}\n",
      "Path parameters\n",
      "The ID of the fine-tuning job.\n",
      "Returns\n",
      "1 2 curl https://api.openai.com/v1/fine_tuning/jobs/ft-AF1WoRqd3aJAHsqc9NY7iL8F \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "3 --------------------------------------------------------------------------------\n",
      "27 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"object\": \"fine_tuning.job\", \"id\": \"ftjob-abc123\", \"model\": \"davinci-002\", \"created_at\": 1692661014, \"finished_at\": 1692661190, \"fine_tuned_model\": \"ft:davinci-002:my-org:custom_suffix:7q8mpxmy\", \"organization_id\": \"org-123\", \"result_files\": [ \"file-abc123\" ], \"status\": \"succeeded\", \"validation_file\": null, \"training_file\": \"file-abc123\", \"hyperparameters\": { \"n_epochs\": 4, }, \"trained_tokens\": 5768 }\n",
      "Cancel fine-tuning\n",
      "post https://api.openai.com/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel\n",
      "Immediately cancel a fine-tune job.\n",
      "Path parameters\n",
      "The ID of the fine-tuning job to cancel.\n",
      "Returns\n",
      "1 2 curl -X POST https://api.openai.com/v1/fine_tuning/jobs/ftjob-abc123/cancel \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "3 --------------------------------------------------------------------------------\n",
      "28 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"object\": \"fine_tuning.job\", \"id\": \"ftjob-abc123\", \"model\": \"gpt-3.5-turbo-0613\", \"created_at\": 1689376978, \"fine_tuned_model\": null, \"organization_id\": \"org-123\", \"result_files\": [], \"hyperparameters\": { \"n_epochs\": \"auto\" }, \"status\": \"cancelled\", \"validation_file\": \"file-abc123\", \"training_file\": \"file-abc123\" }\n",
      "1 2 3 4 5 6 7 { \"object\": \"event\", \"id\": \"ftevent-abc123\" \"created_at\": 1677610602, \"level\": \"info\", \"message\": \"Created fine-tuning job\" }\n",
      "List fine-tuning events\n",
      "get https://api.openai.com/v1/fine_tuning/jobs/{fine_tuning_job_id}/events\n",
      "Get status updates for a fine-tuning job.\n",
      "Path parameters\n",
      "The ID of the fine-tuning job to get events for.\n",
      "Query parameters\n",
      "Identifier for the last event from the previous pagination request.\n",
      "Number of events to retrieve.\n",
      "Returns\n",
      "A list of fine-tuning event objects.\n",
      "1 2 curl https://api.openai.com/v1/fine_tuning/jobs/ftjob-abc123/events \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "3 --------------------------------------------------------------------------------\n",
      "29 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"object\": \"list\", \"data\": [ { \"object\": \"fine_tuning.job.event\", \"id\": \"ft-event-ddTJfwuMVpfLXseO0Am0Gqjm\", \"created_at\": 1692407401, \"level\": \"info\", \"message\": \"Fine tuning job successfully completed\", \"data\": null, \"type\": \"message\" }, { \"object\": \"fine_tuning.job.event\", \"id\": \"ft-event-tyiGuB72evQncpH87xe505Sv\", \"created_at\": 1692407400, \"level\": \"info\", \"message\": \"New fine-tuned model created: ft:gpt-3.5-turbo:openai::7p4lURel\", \"data\": null, \"type\": \"message\" } ], \"has_more\": true }\n",
      "Files\n",
      "Files are used to upload documents that can be used with features like fine-tuning.\n",
      "The file object\n",
      "The File object represents a document that has been uploaded to OpenAI.\n",
      "The file identifier, which can be referenced in the API endpoints.\n",
      "The size of the file in bytes.\n",
      "The Unix timestamp (in seconds) for when the file was created.\n",
      "The object type, which is always \"file\".\n",
      "3 --------------------------------------------------------------------------------\n",
      "30 The object type, which is always \"file\".\n",
      "The intended purpose of the file. Supported values are fine-tune or fine-tune-results.\n",
      "The current status of the file, which can be either uploaded, processed, pending, error, deleting or deleted.\n",
      "Additional details about the status of the file. If the file is in the error state, this will include a message describing the error.\n",
      "1 2 3 4 5 6 7 8 9 10 { \"id\": \"file-abc123\", \"object\": \"file\", \"bytes\": 120000, \"created_at\": 1677610602, \"filename\": \"my_file.jsonl\", \"purpose\": \"fine-tune\", \"status\": \"uploaded\", \"status_details\": null }\n",
      "List files\n",
      "get https://api.openai.com/v1/files\n",
      "Returns a list of files that belong to the user's organization.\n",
      "Returns\n",
      "1 2 curl https://api.openai.com/v1/files \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "3 --------------------------------------------------------------------------------\n",
      "31 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"data\": [ { \"id\": \"file-abc123\", \"object\": \"file\", \"bytes\": 175, \"created_at\": 1613677385, \"filename\": \"train.jsonl\", \"purpose\": \"search\" }, { \"id\": \"file-abc123\", \"object\": \"file\", \"bytes\": 140, \"created_at\": 1613779121, \"filename\": \"puppy.jsonl\", \"purpose\": \"search\" } ], \"object\": \"list\" }\n",
      "Upload file\n",
      "post https://api.openai.com/v1/files\n",
      "Upload a file that can be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit.\n",
      "Request body\n",
      "The file object (not file name) to be uploaded.\n",
      "If the purpose is set to \"fine-tune\", the file will be used for fine-tuning.\n",
      "The intended purpose of the uploaded file.\n",
      "Use \"fine-tune\" for fine-tuning. This allows us to validate the format of the uploaded file is correct for fine-tuning.\n",
      "Returns\n",
      "The uploaded file object.\n",
      "3 --------------------------------------------------------------------------------\n",
      "32 Returns\n",
      "The uploaded file object.\n",
      "1 2 3 4 curl https://api.openai.com/v1/files \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -F purpose=\"fine-tune\" \\ -F file=\"@mydata.jsonl\"\n",
      "1 2 3 4 5 6 7 8 9 { \"id\": \"file-abc123\", \"object\": \"file\", \"bytes\": 140, \"created_at\": 1613779121, \"filename\": \"mydata.jsonl\", \"purpose\": \"fine-tune\", \"status\": \"uploaded\" | \"processed\" | \"pending\" | \"error\" }\n",
      "Delete file\n",
      "delete https://api.openai.com/v1/files/{file_id}\n",
      "Delete a file.\n",
      "Path parameters\n",
      "The ID of the file to use for this request.\n",
      "Returns\n",
      "1 2 3 curl https://api.openai.com/v1/files/file-abc123 \\ -X DELETE \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "1 2 3 4 5 { \"id\": \"file-abc123\", \"object\": \"file\", \"deleted\": true }\n",
      "Retrieve file\n",
      "get https://api.openai.com/v1/files/{file_id}\n",
      "Returns information about a specific file.\n",
      "Path parameters\n",
      "The ID of the file to use for this request.\n",
      "Returns\n",
      "The file object matching the specified ID.\n",
      "1 2 curl https://api.openai.com/v1/files/file-abc123 \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "3 --------------------------------------------------------------------------------\n",
      "33 1 2 3 4 5 6 7 8 { \"id\": \"file-abc123\", \"object\": \"file\", \"bytes\": 140, \"created_at\": 1613779657, \"filename\": \"mydata.jsonl\", \"purpose\": \"fine-tune\" }\n",
      "Retrieve file content\n",
      "get https://api.openai.com/v1/files/{file_id}/content\n",
      "Returns the contents of the specified file.\n",
      "Path parameters\n",
      "The ID of the file to use for this request.\n",
      "Returns\n",
      "1 2 curl https://api.openai.com/v1/files/file-abc123/content \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" > file.jsonl\n",
      "Images\n",
      "Given a prompt and/or an input image, the model will generate a new image.\n",
      "Related guide: Image generation\n",
      "The image object\n",
      "Represents the url or the content of an image generated by the OpenAI API.\n",
      "The base64-encoded JSON of the generated image, if response_format is b64_json.\n",
      "The URL of the generated image, if response_format is url (default).\n",
      "Create image\n",
      "post https://api.openai.com/v1/images/generations\n",
      "Creates an image given a prompt.\n",
      "Request body\n",
      "A text description of the desired image(s). The maximum length is 1000 characters.\n",
      "3 --------------------------------------------------------------------------------\n",
      "34 The number of images to generate. Must be between 1 and 10.\n",
      "The format in which the generated images are returned. Must be one of url or b64_json.\n",
      "The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024.\n",
      "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n",
      "Returns\n",
      "Returns a list of image objects.\n",
      "1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/images/generations \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"prompt\": \"A cute baby sea otter\", \"n\": 2, \"size\": \"1024x1024\" }'\n",
      "1 2 3 4 5 6 7 8 9 10 11 { \"created\": 1589478378, \"data\": [ { \"url\": \"https://...\" }, { \"url\": \"https://...\" } ] }\n",
      "Create image edit\n",
      "post https://api.openai.com/v1/images/edits\n",
      "Creates an edited or extended image given an original image and a prompt.\n",
      "Request body\n",
      "3 --------------------------------------------------------------------------------\n",
      "35 Request body\n",
      "The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.\n",
      "A text description of the desired image(s). The maximum length is 1000 characters.\n",
      "An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where image should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as image.\n",
      "The number of images to generate. Must be between 1 and 10.\n",
      "The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024.\n",
      "The format in which the generated images are returned. Must be one of url or b64_json.\n",
      "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n",
      "Returns\n",
      "Returns a list of image objects.\n",
      "3 --------------------------------------------------------------------------------\n",
      "36 Returns\n",
      "Returns a list of image objects.\n",
      "1 2 3 4 5 6 7 curl https://api.openai.com/v1/images/edits \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -F image=\"@otter.png\" \\ -F mask=\"@mask.png\" \\ -F prompt=\"A cute baby sea otter wearing a beret\" \\ -F n=2 \\ -F size=\"1024x1024\"\n",
      "1 2 3 4 5 6 7 8 9 10 11 { \"created\": 1589478378, \"data\": [ { \"url\": \"https://...\" }, { \"url\": \"https://...\" } ] }\n",
      "Create image variation\n",
      "post https://api.openai.com/v1/images/variations\n",
      "Creates a variation of a given image.\n",
      "Request body\n",
      "The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.\n",
      "The number of images to generate. Must be between 1 and 10.\n",
      "The format in which the generated images are returned. Must be one of url or b64_json.\n",
      "The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024.\n",
      "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n",
      "Returns\n",
      "Returns a list of image objects.\n",
      "3 --------------------------------------------------------------------------------\n",
      "37 Returns\n",
      "Returns a list of image objects.\n",
      "1 2 3 4 5 curl https://api.openai.com/v1/images/variations \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -F image=\"@otter.png\" \\ -F n=2 \\ -F size=\"1024x1024\"\n",
      "1 2 3 4 5 6 7 8 9 10 11 { \"created\": 1589478378, \"data\": [ { \"url\": \"https://...\" }, { \"url\": \"https://...\" } ] }\n",
      "Models\n",
      "List\n",
      "3 --------------------------------------------------------------------------------\n",
      "0 Developer quickstart\n",
      "Get up and running with the OpenAI API\n",
      "The OpenAI API provides a simple interface for developers to create an intelligence layer in their applications, powered by OpenAI's state of the art models. The Chat Completions endpoint powers ChatGPT and provides a simple way to take text as input and use a model like GPT-4 to generate an output.\n",
      "Want to jump straight to the code?\n",
      "Skip the quickstart and dive into the API reference.\n",
      "This quickstart is designed to help get your local development environment setup and send your first API request. If you are an experienced developer or want to just dive into using the OpenAI API, the API reference of GPT guide are a great place to start. Throughout this quickstart, you will learn:\n",
      "How to setup your development environment\n",
      "How to install the latest SDKs\n",
      "Some of the basic concepts of the OpenAI API\n",
      "How to send your first API request\n",
      "If you run into any challenges or have questions getting started, please join our developer forum.\n",
      "Account setup\n",
      "4 --------------------------------------------------------------------------------\n",
      "1 Account setup\n",
      "First, create an OpenAI account or sign in. Next, navigate to the API key page and \"Create new secret key\", optionally naming the key. Make sure to save this somewhere safe and do not share it with anyone.\n",
      "Quickstart language selection\n",
      "Select the tool or language you want to get started using the OpenAI API with.\n",
      "Python is a popular programming language that is commonly used for data applications, web development, and many other programming tasks due to its ease of use. OpenAI provides a custom Python library which makes working with the OpenAI API in Python simple and efficient.\n",
      "Step 1: Setup Python\n",
      "To use the OpenAI Python library, you will need to ensure you have Python installed. Some computers come with Python pre-installed while others require that you set it up yourself. To test if you have Python installed, you can navigate to your Terminal or Command line:\n",
      "MacOS: Open Terminal: You can find it in the Applications folder or search for it using Spotlight (Command + Space).\n",
      "4 --------------------------------------------------------------------------------\n",
      "2 Windows: Open Command Prompt: You can find it by searching \"cmd\" in the start menu.\n",
      "Next, enter the word python and then press return/enter. If you enter into the Python interpreter, then you have Python installed on your computer already and you can go to the next step. If you get an error message that says something like \"Error: command python not found\", you likely need to install Python and make it available in your terminal / command line.\n",
      "To download Python, head to the official Python website and download the latest version. To use the OpenAI Python library, you need at least Python 3.7.1 or newer. If you are installing Python for the first time, you can follow the official Python installation guide for beginners.\n",
      "Setup a virtual environment (optional)\n",
      "4 --------------------------------------------------------------------------------\n",
      "3 Setup a virtual environment (optional)\n",
      "Once you have Python installed, it is a good practice to create a virtual python environment to install the OpenAI Python library. Virtual environments provide a clean working space for your Python packages to be installed so that you do not have conflicts with other libraries you install for other projects. You are not required to use a virtual environment, so skip to step 3 if you do not want to set one up.\n",
      "To create a virtual environment, Python supplies a built in venv module which provides the basic functionality needed for the virtual environment setup. Running the command below will create a virtual environment named \"openai-env\" inside the current folder you have selected in your terminal / command line:\n",
      "python -m venv openai-env\n",
      "Once you’ve created the virtual environment, you need to activate it. On Windows, run:\n",
      "openai-env\\Scripts\\activate\n",
      "On Unix or MacOS, run:\n",
      "source openai-env/bin/activate\n",
      "4 --------------------------------------------------------------------------------\n",
      "4 source openai-env/bin/activate\n",
      "You should see the terminal / command line interface change slightly after you active the virtual environment, it should now show \"openai-env\" to the left of the cursor input section. For more details on working wit virtual environments, please refer to the official Python documentation.\n",
      "Install the OpenAI Python library\n",
      "Once you have Python 3.7.1 or newer installed and (optionally) a virtual environment setup, the OpenAI Python library can be installed. From the terminal / command line, run:\n",
      "pip install --upgrade openai\n",
      "Once this completes, running pip list will show you the Python libraries you have installed in your current environment, which should confirm that the OpenAI Python library was successfully installed.\n",
      "Step 2: Setup your API key\n",
      "Setup your API key for all projects (recommended)\n",
      "The main advantage to making your API key accessible for all projects is that the Python library will automatically detect it and use it without having to write any code.\n",
      "4 --------------------------------------------------------------------------------\n",
      "5 Open Terminal: You can find it in the Applications folder or search for it using Spotlight (Command + Space).\n",
      "Edit Bash Profile: Use the command nano ~/.bash_profile or nano ~/.zshrc (for newer MacOS versions) to open the profile file in a text editor.\n",
      "Add Environment Variable: In the editor, add the line below, replacing your-api-key-here with your actual API key:\n",
      "export OPENAI_API_KEY='your-api-key-here'\n",
      "Save and Exit: Press Ctrl+O to write the changes, followed by Ctrl+X to close the editor.\n",
      "Load Your Profile: Use the command source ~/.bash_profile or source ~/.zshrc to load the updated profile.\n",
      "Verification: Verify the setup by typing echo $OPENAI_API_KEY in the terminal. It should display your API key.\n",
      "Open Command Prompt: You can find it by searching \"cmd\" in the start menu.\n",
      "Set environment variable in the current session: To set the environment variable in the current session, use the command below, replacing your-api-key-here with your actual API key:\n",
      "setx OPENAI_API_KEY \"your-api-key-here\"\n",
      "4 --------------------------------------------------------------------------------\n",
      "6 setx OPENAI_API_KEY \"your-api-key-here\"\n",
      "This command will set the OPENAI_API_KEY environment variable for the current session.\n",
      "Permanent setup: To make the setup permanent, add the variable through the system properties as follows:\n",
      "Right-click on 'This PC' or 'My Computer' and select 'Properties'.\n",
      "Click on 'Advanced system settings'.\n",
      "Click the 'Environment Variables' button.\n",
      "In the 'System variables' section, click 'New...' and enter OPENAI_API_KEY as the variable name and your API key as the variable value.\n",
      "Verification: To verify the setup, reopen the command prompt and type the command below. It should display your API key: echo %OPENAI_API_KEY%\n",
      "Setup your API key for a single project\n",
      "If you only want your API key to be accessible to a single project, you can create a local .env file which contains the API key and then explicitly use that API key with the Python code shown in the steps to come.\n",
      "Start by going to the project folder you want to create the .env file in.\n",
      "4 --------------------------------------------------------------------------------\n",
      "7 In order for your .env file to be ignored by version control, create a .gitignore file in the root of your project directory. Add a line with .env on it which will make sure your API key or other secrets are not accidentally shared via version control.\n",
      "Once you create the .gitignore and .env files using the terminal or an integrated development environment (IDE), copy your secret API key and set it as the OPENAI_API_KEY in your .env file. If you haven't created a secret key yet, you can do so on the API key page.\n",
      "The .env file should look like the following:\n",
      "# Once you add your API key below, make sure to not share it with anyone! The API key should remain private. OPENAI_API_KEY=abc123\n",
      "The API key can be imported by running the code below:\n",
      "1 2 3 import os import openai openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "Step 3: Sending your first API request\n",
      "4 --------------------------------------------------------------------------------\n",
      "8 Step 3: Sending your first API request\n",
      "After you have Python configured and an API key setup, the final step is to send a request to the OpenAI API using the Python library. To do this, create a file named openai-test.py using th terminal or an IDE.\n",
      "Inside the file, copy and paste one of the examples below:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 import os import openai openai.api_key = os.getenv(\"OPENAI_API_KEY\") completion = openai.ChatCompletion.create( model=\"gpt-3.5-turbo\", messages=[ {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"}, {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"} ] ) print(completion.choices[0].message)\n",
      "To run the code, enter python openai-test.py into the terminal / command line.\n",
      "4 --------------------------------------------------------------------------------\n",
      "9 The Chat Completions example highlights just one area of strength for our models: creative ability. Explaining recursion (the programming topic) in a well formatted poem is something both the best developers and best poets would struggle with. In this case, gpt-3.5-turbo does it effortlessly.\n",
      "Next steps\n",
      "Now that you have made you first OpenAI API request, it is time to explore what else is possible:\n",
      "For more detailed information on our models and the API, see our GPT guide.\n",
      "Visit the OpenAI Cookbook for in-depth example API use-cases, as well as code snippets for common tasks.\n",
      "Wondering what GPT models are capable of? Check out our library of example prompts.\n",
      "Want to try the API without writing any code? Start experimenting in the Playground.\n",
      "Keep our usage policies in mind as you start building.\n",
      "4 --------------------------------------------------------------------------------\n",
      "0 Libraries\n",
      "Python library\n",
      "We provide a Python library, which you can install as follows:\n",
      "Once installed, you can use the bindings and your secret key to run the following:\n",
      "1 2 3 4 5 6 7 import os import openai # Load your API key from an environment variable or secret management service openai.api_key = os.getenv(\"OPENAI_API_KEY\") chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
      "The bindings also will install a command-line utility you can use as follows:\n",
      "$ openai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n",
      "Node.js library\n",
      "We also have a Node.js library, which you can install by running the following command in your Node.js project directory:\n",
      "Once installed, you can use the library and your secret key to run the following:\n",
      "5 --------------------------------------------------------------------------------\n",
      "1 1 2 3 4 5 6 7 8 9 10 import OpenAI from \"openai\"; const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY, }); const chatCompletion = await openai.chat.completions.create({ messages: [{ role: \"user\", content: \"Say this is a test\" }], model: \"gpt-3.5-turbo\", });\n",
      "Azure OpenAI libraries\n",
      "Microsoft's Azure team maintains libraries that are compatible with both the OpenAI API and Azure OpenAI services. Read the library documentation below to learn how you can use them with the OpenAI API.\n",
      "Azure OpenAI client library for .NET\n",
      "Azure OpenAI client library for JavaScript\n",
      "Azure OpenAI client library for Java\n",
      "Azure OpenAI client library for Go\n",
      "Community libraries\n",
      "The libraries below are built and maintained by the broader developer community. If you'd like to add a new library here, please follow the instructions in our help center article on adding community libraries. You can also watch our OpenAPI specification repository on GitHub to get timely updates on when we make changes to our API.\n",
      "5 --------------------------------------------------------------------------------\n",
      "2 Please note that OpenAI does not verify the correctness or security of these projects. Use them at your own risk!\n",
      "C# / .NET\n",
      "Betalgo.OpenAI by Betalgo\n",
      "OpenAI-API-dotnet by OkGoDoIt\n",
      "OpenAI-DotNet by RageAgainstThePixel\n",
      "C++\n",
      "liboai by D7EAD\n",
      "Clojure\n",
      "openai-clojure by wkok\n",
      "Crystal\n",
      "openai-crystal by sferik\n",
      "Dart/Flutter\n",
      "openai by anasfik\n",
      "Delphi\n",
      "DelphiOpenAI by HemulGM\n",
      "Elixir\n",
      "openai.ex by mgallo\n",
      "Go\n",
      "go-gpt3 by sashabaranov\n",
      "Java\n",
      "openai-java by Theo Kanning\n",
      "Julia\n",
      "OpenAI.jl by rory-linehan\n",
      "Kotlin\n",
      "openai-kotlin by Mouaad Aallam\n",
      "Node.js\n",
      "openai-api by Njerschow\n",
      "openai-api-node by erlapso\n",
      "gpt-x by ceifa\n",
      "gpt3 by poteat\n",
      "gpts by thencc\n",
      "@dalenguyen/openai by dalenguyen\n",
      "tectalic/openai by tectalic\n",
      "PHP\n",
      "orhanerday/open-ai by orhanerday\n",
      "tectalic/openai by tectalic\n",
      "openai-php clinet by openai-php\n",
      "Python\n",
      "chronology by OthersideAI\n",
      "R\n",
      "rgpt3 by ben-aaron188\n",
      "Ruby\n",
      "openai by nileshtrivedi\n",
      "ruby-openai by alexrudall\n",
      "Rust\n",
      "async-openai by 64bit\n",
      "fieri by lbkolev\n",
      "Scala\n",
      "openai-scala-client by cequence-io\n",
      "Swift\n",
      "OpenAIKit by dylanshine\n",
      "5 --------------------------------------------------------------------------------\n",
      "3 Swift\n",
      "OpenAIKit by dylanshine\n",
      "OpenAI by MacPaw\n",
      "Unity\n",
      "OpenAi-Api-Unity by hexthedev\n",
      "com.openai.unity by RageAgainstThePixel\n",
      "Unreal Engine\n",
      "OpenAI-Api-Unreal by KellanM\n",
      "5 --------------------------------------------------------------------------------\n",
      "0 Deprecations\n",
      "Overview\n",
      "As we launch safer and more capable models, we regularly retire older models. Software relying on OpenAI models may need occasional updates to keep working. Impacted customers will always be notified by email and in our documentation along with blog posts for larger changes.\n",
      "This page lists all API deprecations, along with recommended replacements.\n",
      "Incremental model updates\n",
      "As announced in March 2023, we regularly release new versions of gpt-4 and gpt-3.5-turbo.\n",
      "Each model version is dated with an -MMDD suffix; e.g., gpt-4-0613. The undated model name, e.g., gpt-4, will typically point to the latest version (e.g. gpt-4 points to gpt-4-0613). Users of undated model names will be notified by email typically 2 weeks before any change takes place.\n",
      "After a new version is launched, older versions will typically be deprecated 3 months later.\n",
      "Migrating to replacements\n",
      "6 --------------------------------------------------------------------------------\n",
      "1 Migrating to replacements\n",
      "Once a model is deprecated, be sure to migrate all usage to a suitable replacement before the shutdown date. Requests to models past the shutdown date will fail.\n",
      "To help measure the performance of replacement models on your tasks, we have open-sourced Evals, a Python framework for evaluating language models.\n",
      "If new models perform worse on your tasks, let us know by by submitting a pull request to our Evals repo with examples of the task.\n",
      "Deprecation history\n",
      "All deprecations are listed below, with the most recent announcements at the top.\n",
      "2023-08-22: Fine-tunes endpoint\n",
      "On August 22nd, 2023, we announced the new fine-tuning API (/v1/fine_tuning/jobs) and that the original /v1/fine-tunes API along with legacy models (including those fine-tuned with the /v1/fine-tunes API) will be shut down on January 04, 2024.\n",
      "Fine-tunes endpoint\n",
      "Shutdown dateSystemRecommended replacement\n",
      "2024-01-04\t/v1/fine-tunes\t/v1/fine_tuning/jobs\t\n",
      "2023-07-06: GPT and embeddings\n",
      "6 --------------------------------------------------------------------------------\n",
      "2 2023-07-06: GPT and embeddings\n",
      "On July 06, 2023, we announced the upcoming retirements of older GPT-3 and GPT-3.5 models served via the completions endpoint. We also announced the upcoming retirement of our first-generation text embedding models. They will be shut down on January 04, 2024.\n",
      "InstructGPT models\n",
      "Shutdown dateLegacy modelLegacy model priceRecommended replacement\n",
      "2024-01-04\ttext-ada-001\t$0.0004 / 1K tokens\tgpt-3.5-turbo-instruct\t\n",
      "2024-01-04\ttext-babbage-001\t$0.0005 / 1K tokens\tgpt-3.5-turbo-instruct\t\n",
      "2024-01-04\ttext-curie-001\t$0.0020 / 1K tokens\tgpt-3.5-turbo-instruct\t\n",
      "2024-01-04\ttext-davinci-001\t$0.0200 / 1K tokens\tgpt-3.5-turbo-instruct\t\n",
      "2024-01-04\ttext-davinci-002\t$0.0200 / 1K tokens\tgpt-3.5-turbo-instruct\t\n",
      "2024-01-04\ttext-davinci-003\t$0.0200 / 1K tokens\tgpt-3.5-turbo-instruct\t\n",
      "Pricing for the replacement gpt-3.5-turbo-instruct model can be found on the pricing page.\n",
      "Base GPT models\n",
      "Shutdown dateLegacy modelLegacy model priceRecommended replacement\n",
      "6 --------------------------------------------------------------------------------\n",
      "3 2024-01-04\tada\t$0.0004 / 1K tokens\tbabbage-002\t\n",
      "2024-01-04\tbabbage\t$0.0005 / 1K tokens\tbabbage-002\t\n",
      "2024-01-04\tcurie\t$0.0020 / 1K tokens\tdavinci-002\t\n",
      "2024-01-04\tdavinci\t$0.0200 / 1K tokens\tdavinci-002\t\n",
      "2024-01-04\tcode-davinci-002\tfree to researchers\tgpt-3.5-turbo-base\t\n",
      "Pricing for the replacement babbage-002 and davinci-002 models can be found on the pricing page.\n",
      "Edit models & endpoint\n",
      "Shutdown dateModel / systemRecommended replacement\n",
      "2024-01-04\ttext-davinci-edit-001\tgpt-4\t\n",
      "2024-01-04\tcode-davinci-edit-001\tgpt-4\t\n",
      "2024-01-04\t/v1/edits\t/v1/chat/completions\t\n",
      "Fine-tuning GPT models\n",
      "Shutdown dateLegacy modelTraining priceUsage priceRecommended replacement\n",
      "2024-01-04\tada\t$0.0004 / 1K tokens\t$0.0016 / 1K tokens\tbabbage-002\t\n",
      "2024-01-04\tbabbage\t$0.0006 / 1K tokens\t$0.0024 / 1K tokens\tbabbage-002\t\n",
      "2024-01-04\tcurie\t$0.003 / 1K tokens\t$0.012 / 1K tokens\tdavinci-002\t\n",
      "2024-01-04\tdavinci\t$0.03 / 1K tokens\t$0.12 / 1K tokens\tdavinci-002, gpt-3.5-turbo, gpt-4\t\n",
      "First-generation text embedding models\n",
      "6 --------------------------------------------------------------------------------\n",
      "4 First-generation text embedding models\n",
      "Shutdown dateLegacy modelLegacy model priceRecommended replacement\n",
      "2024-01-04\ttext-similarity-ada-001\t$0.004 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-search-ada-doc-001\t$0.004 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-search-ada-query-001\t$0.004 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\tcode-search-ada-code-001\t$0.004 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\tcode-search-ada-text-001\t$0.004 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-similarity-babbage-001\t$0.005 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-search-babbage-doc-001\t$0.005 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-search-babbage-query-001\t$0.005 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\tcode-search-babbage-code-001\t$0.005 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\tcode-search-babbage-text-001\t$0.005 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-similarity-curie-001\t$0.020 / 1K tokens\ttext-embedding-ada-002\n",
      "6 --------------------------------------------------------------------------------\n",
      "5 2024-01-04\ttext-search-curie-doc-001\t$0.020 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-search-curie-query-001\t$0.020 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-similarity-davinci-001\t$0.200 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-search-davinci-doc-001\t$0.200 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2024-01-04\ttext-search-davinci-query-001\t$0.200 / 1K tokens\ttext-embedding-ada-002\t\n",
      "2023-06-13: Updated chat models\n",
      "On June 13, 2023, we announced new chat model versions in the Function calling and other API updates blog post. The three original versions will be retired in June 2024 at the earliest.\n",
      "Shutdown dateLegacy modelLegacy model priceRecommended replacement\n",
      "at earliest 2024-06-13\tgpt-3.5-turbo-0301\t$0.0015 / 1K input tokens + $0.0020 / 1K output tokens\tgpt-3.5-turbo-0613\t\n",
      "at earliest 2024-06-13\tgpt-4-0314\t$0.03 / 1K input tokens + $0.06 / 1K output tokens\tgpt-4-0613\t\n",
      "at earliest 2024-06-13\tgpt-4-32k-0314\t$0.06 / 1K input tokens + $0.12 / 1K output tokens\tgpt-4-32k-0613\n",
      "6 --------------------------------------------------------------------------------\n",
      "6 2023-03-20: Codex models\n",
      "Shutdown dateLegacy modelRecommended replacement\n",
      "2023-03-23\tcode-davinci-002\tgpt-4 or researcher access program\t\n",
      "2023-03-23\tcode-davinci-001\tgpt-4 or researcher access program\t\n",
      "2023-03-23\tcode-cushman-002\tgpt-4 or researcher access program\t\n",
      "2023-03-23\tcode-cushman-001\tgpt-4 or researcher access program\t\n",
      "2022-06-03: Legacy endpoints\n",
      "Shutdown dateSystemRecommended replacement\n",
      "2022-12-03\t/v1/engines\t/v1/models\t\n",
      "2022-12-03\t/v1/search\tView transition guide\t\n",
      "2022-12-03\t/v1/classifications\tView transition guide\t\n",
      "2022-12-03\t/v1/answers\tView transition guide\n",
      "6 --------------------------------------------------------------------------------\n",
      "0 Tutorials\n",
      "Get started with the OpenAI API by building real AI apps step by step.\n",
      "Website Q&A with Embeddings\n",
      "Learn how to build an AI that can answer questions about your website.\n",
      "Meeting minutes transcription with Whisper\n",
      "Learn how to create an automated meeting minutes generator with Whisper and GPT-4.\n",
      "Coming soon\n",
      "Learn how to build and deploy an AI chat bot that understands multiple knowledge bases.\n",
      "Looking for more ideas? Check out our Examples gallery or the OpenAI Cookbook on GitHub.\n",
      "7 --------------------------------------------------------------------------------\n",
      "0 GPT models\n",
      "OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you “program” a GPT model, usually by providing instructions or some examples of how to successfully complete a task.\n",
      "Using GPTs, you can build applications to:\n",
      "Draft documents\n",
      "Write computer code\n",
      "Answer questions about a knowledge base\n",
      "Analyze texts\n",
      "Create conversational agents\n",
      "Give software a natural language interface\n",
      "Tutor in a range of subjects\n",
      "Translate languages\n",
      "Simulate characters for games\n",
      "...and much more!\n",
      "To use a GPT model via the OpenAI API, you’ll send a request containing the inputs and your API key, and receive a response containing the model’s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint.\n",
      "Model familiesAPI endpoint\n",
      "8 --------------------------------------------------------------------------------\n",
      "1 Model familiesAPI endpoint\n",
      "Newer models (2023–)\tgpt-4, gpt-3.5-turbo\thttps://api.openai.com/v1/chat/completions\t\n",
      "Updated base models (2023)\tbabbage-002, davinci-002\thttps://api.openai.com/v1/completions\t\n",
      "Legacy models (2020–2022)\ttext-davinci-003, text-davinci-002, davinci, curie, babbage, ada\thttps://api.openai.com/v1/completions\t\n",
      "You can experiment with GPTs in the playground. If you’re not sure which model to use, then use gpt-3.5-turbo or gpt-4.\n",
      "Chat completions API\n",
      "Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it’s just as useful for single-turn tasks without any conversation.\n",
      "An example Chat completions API call looks like the following:\n",
      "8 --------------------------------------------------------------------------------\n",
      "2 1 2 3 4 5 6 7 8 9 response = openai.ChatCompletion.create( model=\"gpt-3.5-turbo\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}, {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"}, {\"role\": \"user\", \"content\": \"Where was it played?\"} ] )\n",
      "To learn more, you can view the full API reference documentation for the Chat API.\n",
      "The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.\n",
      "Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.\n",
      "8 --------------------------------------------------------------------------------\n",
      "3 The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
      "The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
      "8 --------------------------------------------------------------------------------\n",
      "4 Including conversation history is important when user instructions refer to prior messages. In the example above, the user’s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model’s token limit, it will need to be shortened in some way.\n",
      "To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.\n",
      "Chat completions response format\n",
      "An example Chat completions API response looks as follows:\n",
      "8 --------------------------------------------------------------------------------\n",
      "5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\", \"role\": \"assistant\" } } ], \"created\": 1677664795, \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\", \"model\": \"gpt-3.5-turbo-0613\", \"object\": \"chat.completion\", \"usage\": { \"completion_tokens\": 17, \"prompt_tokens\": 57, \"total_tokens\": 74 } }\n",
      "The assistant’s reply can be extracted with:\n",
      "response['choices'][0]['message']['content']\n",
      "Every response will include a finish_reason. The possible values for finish_reason are:\n",
      "stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n",
      "length: Incomplete model output due to max_tokens parameter or token limit\n",
      "function_call: The model decided to call a function\n",
      "content_filter: Omitted content due to a flag from our content filters\n",
      "null: API response still in progress or incomplete\n",
      "8 --------------------------------------------------------------------------------\n",
      "6 Depending on input parameters (like providing functions as shown below), the model response may include different information.\n",
      "Function calling\n",
      "In an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.\n",
      "The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).\n",
      "8 --------------------------------------------------------------------------------\n",
      "7 Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.\n",
      "Function calling allows you to more reliably get structured data back from the model. For example, you can:\n",
      "Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)\n",
      "e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')\n",
      "Convert natural language into API calls\n",
      "e.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal API\n",
      "Extract structured data from text\n",
      "e.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)\n",
      "...and much more!\n",
      "8 --------------------------------------------------------------------------------\n",
      "8 ...and much more!\n",
      "The basic sequence of steps for function calling is as follows:\n",
      "Call the model with the user query and a set of functions defined in the functions parameter.\n",
      "The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).\n",
      "Parse the string into JSON in your code, and call your function with the provided arguments if they exist.\n",
      "Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.\n",
      "You can see these steps in action through the example below:\n",
      "8 --------------------------------------------------------------------------------\n",
      "9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 import openai import json # Example dummy function hard coded to return the same weather # In production, this could be your backend API or an external API def get_current_weather(location, unit=\"fahrenheit\"): \"\"\"Get the current weather in a given location\"\"\" weather_info = { \"location\": location, \"temperature\": \"72\", \"unit\": unit, \"forecast\": [\"sunny\", \"windy\"], } return json.dumps(weather_info) def run_conversation(): # Step 1: send the conversation and available functions to GPT messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}] functions = [ { \"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San\n",
      "8 --------------------------------------------------------------------------------\n",
      "10 \"description\": \"The city and state, e.g. San Francisco, CA\", }, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}, }, \"required\": [\"location\"], }, } ] response = openai.ChatCompletion.create( model=\"gpt-3.5-turbo-0613\", messages=messages, functions=functions, function_call=\"auto\", # auto is default, but we'll be explicit ) response_message = response[\"choices\"][0][\"message\"] # Step 2: check if GPT wanted to call a function if response_message.get(\"function_call\"): # Step 3: call the function # Note: the JSON response may not always be valid; be sure to handle errors available_functions = { \"get_current_weather\": get_current_weather, } # only one function in this example, but you can have multiple function_name = response_message[\"function_call\"][\"name\"] function_to_call = available_functions[function_name] function_args = json.loads(response_message[\"function_call\"][\"arguments\"]) function_response = function_to_call( location=function_args.get(\"location\"), unit=function_args.get(\"unit\"), ) # Step\n",
      "8 --------------------------------------------------------------------------------\n",
      "11 unit=function_args.get(\"unit\"), ) # Step 4: send the info on the function call and function response to GPT messages.append(response_message) # extend conversation with assistant's reply messages.append( { \"role\": \"function\", \"name\": function_name, \"content\": function_response, } ) # extend conversation with function response second_response = openai.ChatCompletion.create( model=\"gpt-3.5-turbo-0613\", messages=messages, ) # get a new response from GPT where it can see the function response return second_response print(run_conversation())\n",
      "8 --------------------------------------------------------------------------------\n",
      "12 Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"\n",
      "In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.\n",
      "For example, if you ask the model “Find the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar” and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.\n",
      "8 --------------------------------------------------------------------------------\n",
      "13 If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.\n",
      "You can find more examples of function calling in the OpenAI cookbook:\n",
      "Function calling\n",
      "Learn from more examples demonstrating function calling\n",
      "The completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.\n",
      "An example API call looks as follows:\n",
      "1 2 3 4 response = openai.Completion.create( model=\"gpt-3.5-turbo-instruct\", prompt=\"Write a tagline for an ice cream shop.\" )\n",
      "See the full API reference documentation to learn more.\n",
      "Token log probabilities\n",
      "8 --------------------------------------------------------------------------------\n",
      "14 Token log probabilities\n",
      "The completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.\n",
      "Inserting text\n",
      "The completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.\n",
      "Completions response format\n",
      "An example completions API response looks as follows:\n",
      "8 --------------------------------------------------------------------------------\n",
      "15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"choices\": [ { \"finish_reason\": \"length\", \"index\": 0, \"logprobs\": null, \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\" } ], \"created\": 1683130927, \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\", \"model\": \"gpt-3.5-turbo-instruct\", \"object\": \"text_completion\", \"usage\": { \"completion_tokens\": 16, \"prompt_tokens\": 10, \"total_tokens\": 26 } }\n",
      "In Python, the output can be extracted with response['choices'][0]['text'].\n",
      "The response format is similar to the response format of the Chat completions API but also includes the optional field logprobs.\n",
      "Chat completions vs. Completions\n",
      "The Chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:\n",
      "Translate the following English text to French: \"{text}\"\n",
      "And an equivalent chat prompt would be:\n",
      "8 --------------------------------------------------------------------------------\n",
      "16 And an equivalent chat prompt would be:\n",
      "[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]\n",
      "Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.\n",
      "The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo).\n",
      "Which model should I use?\n",
      "8 --------------------------------------------------------------------------------\n",
      "17 Which model should I use?\n",
      "We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.\n",
      "We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.\n",
      "8 --------------------------------------------------------------------------------\n",
      "18 GPT best practices\n",
      "An awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as “prompt engineering”, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.\n",
      "Managing tokens\n",
      "Language models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or apple), and in some languages tokens can be even shorter than one character or even longer than one word.\n",
      "8 --------------------------------------------------------------------------------\n",
      "19 For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].\n",
      "The total number of tokens in an API call affects:\n",
      "How much your API call costs, as you pay per token\n",
      "How long your API call takes, as writing more tokens takes more time\n",
      "Whether your API call works at all, as total tokens must be below the model’s maximum limit (4097 tokens for gpt-3.5-turbo)\n",
      "Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).\n",
      "To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).\n",
      "8 --------------------------------------------------------------------------------\n",
      "20 Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n",
      "To see how many tokens are in a text string without making an API call, use OpenAI’s tiktoken Python library. Example code can be found in the OpenAI Cookbook’s guide on how to count tokens with tiktoken.\n",
      "Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.\n",
      "If a conversation has too many tokens to fit within a model’s maximum limit (e.g., more than 4097 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.\n",
      "8 --------------------------------------------------------------------------------\n",
      "21 Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.\n",
      "Parameter details\n",
      "Frequency and presence penalties\n",
      "The frequency and presence penalties found in the Chat completions API and Legacy Completions API can be used to reduce the likelihood of sampling repetitive sequences of tokens. They work by directly modifying the logits (un-normalized log-probabilities) with an additive contribution.\n",
      "mu[j] -> mu[j] - c[j] * alpha_frequency - float(c[j] > 0) * alpha_presence\n",
      "Where:\n",
      "mu[j] is the logits of the j-th token\n",
      "c[j] is how often that token was sampled prior to the current position\n",
      "float(c[j] > 0) is 1 if c[j] > 0 and 0 otherwise\n",
      "alpha_frequency is the frequency penalty coefficient\n",
      "alpha_presence is the presence penalty coefficient\n",
      "8 --------------------------------------------------------------------------------\n",
      "22 As we can see, the presence penalty is a one-off additive contribution that applies to all tokens that have been sampled at least once and the frequency penalty is a contribution that is proportional to how often a particular token has already been sampled.\n",
      "Reasonable values for the penalty coefficients are around 0.1 to 1 if the aim is to just reduce repetitive samples somewhat. If the aim is to strongly suppress repetition, then one can increase the coefficients up to 2, but this can noticeably degrade the quality of samples. Negative values can be used to increase the likelihood of repetition.\n",
      "FAQ\n",
      "Why are model outputs inconsistent?\n",
      "The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.\n",
      "How should I set the temperature parameter?\n",
      "8 --------------------------------------------------------------------------------\n",
      "23 How should I set the temperature parameter?\n",
      "Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.\n",
      "Is fine-tuning available for the latest models?\n",
      "Yes, for some. Currently, you can only fine-tune gpt-3.5-turbo and our updated base models (babbage-002 and davinci-002). See the fine-tuning guide for more details on how to use fine-tuned models.\n",
      "Do you store the data that is passed into the API?\n",
      "As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.\n",
      "How can I make my application more safe?\n",
      "If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI’s usage policies from being shown.\n",
      "8 --------------------------------------------------------------------------------\n",
      "24 Should I use ChatGPT or the API?\n",
      "ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI’s API provides more flexibility.\n",
      "8 --------------------------------------------------------------------------------\n",
      "0 GPT best practices\n",
      "This guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.\n",
      "Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.\n",
      "Six strategies for getting better results\n",
      "Write clear instructions\n",
      "GPTs can’t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less GPTs have to guess at what you want, the more likely you’ll get it.\n",
      "Tactics:\n",
      "Include details in your query to get more relevant answers\n",
      "Ask the model to adopt a persona\n",
      "9 --------------------------------------------------------------------------------\n",
      "1 Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "Provide reference text\n",
      "GPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.\n",
      "Tactics:\n",
      "Instruct the model to answer using a reference text\n",
      "Instruct the model to answer with citations from a reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "9 --------------------------------------------------------------------------------\n",
      "2 Split complex tasks into simpler subtasks\n",
      "Just as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.\n",
      "Tactics:\n",
      "Use intent classification to identify the most relevant instructions for a user query\n",
      "For dialogue applications that require very long conversations, summarize or filter previous dialogue\n",
      "Summarize long documents piecewise and construct a full summary recursively\n",
      "Give GPTs time to \"think\"\n",
      "9 --------------------------------------------------------------------------------\n",
      "3 Give GPTs time to \"think\"\n",
      "If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.\n",
      "Tactics:\n",
      "Instruct the model to work out its own solution before rushing to a conclusion\n",
      "Use inner monologue or a sequence of queries to hide the model's reasoning process\n",
      "Ask the model if it missed anything on previous passes\n",
      "Use external tools\n",
      "Compensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.\n",
      "Tactics:\n",
      "9 --------------------------------------------------------------------------------\n",
      "4 Tactics:\n",
      "Use embeddings-based search to implement efficient knowledge retrieval\n",
      "Use code execution to perform more accurate calculations or call external APIs\n",
      "Give the model access to specific functions\n",
      "Test changes systematically\n",
      "Improving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").\n",
      "Tactic:\n",
      "Evaluate model outputs with reference to gold-standard answers\n",
      "Tactics\n",
      "Each of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.\n",
      "Strategy: Write clear instructions\n",
      "9 --------------------------------------------------------------------------------\n",
      "5 Strategy: Write clear instructions\n",
      "Tactic: Include details in your query to get more relevant answers\n",
      "In order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.\n",
      "Worse\tBetter\t\n",
      "How do I add numbers in Excel?\tHow do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".\t\n",
      "Who’s president?\tWho was the president of Mexico in 2021, and how frequently are elections held?\t\n",
      "Write code to calculate the Fibonacci sequence.\tWrite a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.\n",
      "9 --------------------------------------------------------------------------------\n",
      "6 Summarize the meeting notes.\tSummarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.\t\n",
      "Tactic: Ask the model to adopt a persona\n",
      "The system message can be used to specify the persona used by the model in its replies.\n",
      "When I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph. \n",
      "Write a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.\n",
      "Open in Playground\n",
      "Tactic: Use delimiters to clearly indicate distinct parts of the input\n",
      "Delimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.\n",
      "Summarize the text delimited by triple quotes with a haiku. \"\"\"insert text here\"\"\"\n",
      "Open in Playground\n",
      "9 --------------------------------------------------------------------------------\n",
      "7 Open in Playground\n",
      "You will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why. \n",
      "<article> insert first article here </article> <article> insert second article here </article>\n",
      "Open in Playground\n",
      "You will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives. \n",
      "Abstract: insert abstract here Title: insert title here\n",
      "Open in Playground\n",
      "For straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don’t make GPTs work to understand exactly what you are asking of them.\n",
      "Tactic: Specify the steps required to complete a task\n",
      "9 --------------------------------------------------------------------------------\n",
      "8 Some tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.\n",
      "Use the following step-by-step instructions to respond to user inputs. Step 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \". Step 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \". \n",
      "Open in Playground\n",
      "Tactic: Provide examples\n",
      "Providing general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.\n",
      "Answer in a consistent style.\n",
      "9 --------------------------------------------------------------------------------\n",
      "9 Answer in a consistent style. \n",
      "The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread. \n",
      "Teach me about the ocean.\n",
      "Open in Playground\n",
      "Tactic: Specify the desired length of the output\n",
      "You can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.\n",
      "Summarize the text delimited by triple quotes in about 50 words. \"\"\"insert text here\"\"\"\n",
      "Open in Playground\n",
      "Summarize the text delimited by triple quotes in 2 paragraphs. \"\"\"insert text here\"\"\"\n",
      "Open in Playground\n",
      "Summarize the text delimited by triple quotes in 3 bullet points. \"\"\"insert text here\"\"\"\n",
      "9 --------------------------------------------------------------------------------\n",
      "10 Open in Playground\n",
      "Strategy: Provide reference text\n",
      "Tactic: Instruct the model to answer using a reference text\n",
      "If we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.\n",
      "Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\" \n",
      "<insert articles, each delimited by triple quotes> Question: <insert question here>\n",
      "Open in Playground\n",
      "Given that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.\n",
      "Tactic: Instruct the model to answer with citations from a reference text\n",
      "9 --------------------------------------------------------------------------------\n",
      "11 If the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.\n",
      "You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": …}). \n",
      "\"\"\"<insert document here>\"\"\" Question: <insert question here>\n",
      "Open in Playground\n",
      "Strategy: Split complex tasks into simpler subtasks\n",
      "9 --------------------------------------------------------------------------------\n",
      "12 Tactic: Use intent classification to identify the most relevant instructions for a user query\n",
      "For tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).\n",
      "Suppose for example that for a customer service application, queries could be usefully classified as follows:\n",
      "9 --------------------------------------------------------------------------------\n",
      "13 You will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary. Primary categories: Billing, Technical Support, Account Management, or General Inquiry. Billing secondary categories: - Unsubscribe or upgrade - Add a payment method - Explanation for charge - Dispute a charge Technical Support secondary categories: - Troubleshooting - Device compatibility - Software updates Account Management secondary categories: - Password reset - Update personal information - Close account - Account security General Inquiry secondary categories: - Product information - Pricing - Feedback - Speak to a human \n",
      "I need to get my internet working again.\n",
      "Open in Playground\n",
      "Based on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".\n",
      "9 --------------------------------------------------------------------------------\n",
      "14 You will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by: - Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time. - If all cables are connected and the issue persists, ask them which router model they are using - Now you will advise them how to restart their device: -- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection. -- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection. - If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}. - If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request\n",
      "9 --------------------------------------------------------------------------------\n",
      "15 about troubleshooting and classify their request according to the following scheme: <insert primary/secondary classification scheme from above here>\n",
      "9 --------------------------------------------------------------------------------\n",
      "16 I need to get my internet working again.\n",
      "Open in Playground\n",
      "Notice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.\n",
      "Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogue\n",
      "Since GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.\n",
      "9 --------------------------------------------------------------------------------\n",
      "17 There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.\n",
      "An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".\n",
      "Tactic: Summarize long documents piecewise and construct a full summary recursively\n",
      "Since GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.\n",
      "9 --------------------------------------------------------------------------------\n",
      "18 To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it’s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.\n",
      "Strategy: Give GPTs time to \"think\"\n",
      "Tactic: Instruct the model to work out its own solution before rushing to a conclusion\n",
      "9 --------------------------------------------------------------------------------\n",
      "19 Sometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student’s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.\n",
      "Determine if the student's solution is correct or not. \n",
      "Problem Statement: I'm building a solar power installation and I need help working out the financials. - Land costs $100 / square foot - I can buy solar panels for $250 / square foot - I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot What is the total cost for the first year of operations as a function of the number of square feet. Student's Solution: Let x be the size of the installation in square feet. 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
      "9 --------------------------------------------------------------------------------\n",
      "20 The student's solution is correct.\n",
      "Open in Playground\n",
      "But the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.\n",
      "First work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\n",
      "9 --------------------------------------------------------------------------------\n",
      "21 Problem Statement: I'm building a solar power installation and I need help working out the financials. - Land costs $100 / square foot - I can buy solar panels for $250 / square foot - I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot What is the total cost for the first year of operations as a function of the number of square feet. Student's Solution: Let x be the size of the installation in square feet. 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000 \n",
      "Let x be the size of the installation in square feet. 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 10x Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000 The student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.\n",
      "Open in Playground\n",
      "9 --------------------------------------------------------------------------------\n",
      "22 Open in Playground\n",
      "Tactic: Use inner monologue or a sequence of queries to hide the model's reasoning process\n",
      "The previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model’s reasoning process about the student’s solution could reveal the answer to the student.\n",
      "Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.\n",
      "9 --------------------------------------------------------------------------------\n",
      "23 Follow these steps to answer the user queries. Step 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\"). Step 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\"). Step 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\"). Step 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\". \n",
      "Problem Statement: <insert problem statement> Student Solution: <insert student solution>\n",
      "Open in Playground\n",
      "Alternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.\n",
      "9 --------------------------------------------------------------------------------\n",
      "24 First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student’s solution, it can be omitted. This provides the additional advantage that there is no chance that the model’s solution will be biased by the student’s attempted solution.\n",
      "Next, we can have the model use all available information to assess the correctness of the student’s solution.\n",
      "Compare your solution to the student's solution and evaluate if the student's solution is correct or not. \n",
      "Problem statement: \"\"\"<insert problem statement>\"\"\" Your solution: \"\"\"<insert model generated solution>\"\"\" Student’s solution: \"\"\"<insert student's solution>\"\"\"\n",
      "Open in Playground\n",
      "Finally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.\n",
      "You are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\n",
      "9 --------------------------------------------------------------------------------\n",
      "25 Problem statement: \"\"\"<insert problem statement>\"\"\" Your solution: \"\"\"<insert model generated solution>\"\"\" Student’s solution: \"\"\"<insert student's solution>\"\"\" Analysis: \"\"\"<insert model generated analysis from previous step>\"\"\"\n",
      "Open in Playground\n",
      "Tactic: Ask the model if it missed anything on previous passes\n",
      "Suppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.\n",
      "9 --------------------------------------------------------------------------------\n",
      "26 You will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\" Ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows: [{\"excerpt\": \"...\"}, ... {\"excerpt\": \"...\"}] \n",
      "\"\"\"<insert document here>\"\"\" \n",
      "[{\"excerpt\": \"the model writes an excerpt here\"}, ... {\"excerpt\": \"the model writes another excerpt here\"}] \n",
      "Are there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.\n",
      "Open in Playground\n",
      "Strategy: Use external tools\n",
      "Tactic: Use embeddings-based search to implement efficient knowledge retrieval\n",
      "9 --------------------------------------------------------------------------------\n",
      "27 A model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc…) to the model’s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.\n",
      "9 --------------------------------------------------------------------------------\n",
      "28 A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).\n",
      "Example implementations can be found in the OpenAI Cookbook. See the tactic “Instruct the model to use retrieved knowledge to answer queries” for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.\n",
      "Tactic: Use code execution to perform more accurate calculations or call external APIs\n",
      "9 --------------------------------------------------------------------------------\n",
      "29 GPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.\n",
      "You can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations. \n",
      "Find all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.\n",
      "Open in Playground\n",
      "9 --------------------------------------------------------------------------------\n",
      "30 Open in Playground\n",
      "Another good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.\n",
      "You can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends: ```python import message message.write(to=\"John\", message=\"Hey, want to meetup after work?\")```\n",
      "Open in Playground\n",
      "WARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.\n",
      "Tactic: Give the model access to specific functions\n",
      "9 --------------------------------------------------------------------------------\n",
      "31 The Chat completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.\n",
      "Strategy: Test changes systematically\n",
      "Sometimes it can be hard to tell whether a change — e.g., a new instruction or a new design — makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.\n",
      "9 --------------------------------------------------------------------------------\n",
      "32 Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:\n",
      "Representative of real-world usage (or at least diverse)\n",
      "Contain many test cases for greater statistical power (see table below for guidelines)\n",
      "Easy to automate or repeat\n",
      "Difference to detectSample size needed for 95% confidence\n",
      "30%\t~10\t\n",
      "10%\t~100\t\n",
      "3%\t~1,000\t\n",
      "1%\t~10,000\t\n",
      "Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.\n",
      "9 --------------------------------------------------------------------------------\n",
      "33 Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.\n",
      "Tactic: Evaluate model outputs with reference to gold-standard answers\n",
      "Suppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.\n",
      "For example, using the following system message:\n",
      "9 --------------------------------------------------------------------------------\n",
      "34 For example, using the following system message:\n",
      "You will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer: - Neil Armstrong was the first person to walk on the moon. - The date Neil Armstrong first walked on the moon was July 21, 1969. For each of these points perform the following steps: 1 - Restate the point. 2 - Provide a citation from the answer which is closest to this point. 3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind. 4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\". Finally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.\n",
      "Here's an example input where both points are satisfied:\n",
      "<insert system message above>\n",
      "9 --------------------------------------------------------------------------------\n",
      "35 <insert system message above> \n",
      "\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"\n",
      "Open in Playground\n",
      "Here's an example input where only one point is satisfied:\n",
      "<insert system message above> \n",
      "\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"\n",
      "Open in Playground\n",
      "Here's an example input where none are satisfied:\n",
      "<insert system message above> \n",
      "\"\"\"In the summer of '69, a voyage grand, Apollo 11, bold as legend's hand. Armstrong took a step, history unfurled, \"One small step,\" he said, for a new world.\"\"\"\n",
      "Open in Playground\n",
      "There are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.\n",
      "9 --------------------------------------------------------------------------------\n",
      "36 Use the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\". Step 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset). Step 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer. Step 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}\n",
      "Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:\n",
      "<insert system message above>\n",
      "9 --------------------------------------------------------------------------------\n",
      "37 <insert system message above> \n",
      "Question: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\" Submitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\" Expert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"\n",
      "Open in Playground\n",
      "Here's an example input with answer that directly contradicts the expert answer:\n",
      "<insert system message above> \n",
      "Question: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\" Submitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\" Expert Answer: \"\"\"Ne\n",
      "9 --------------------------------------------------------------------------------\n",
      "0 Image generation\n",
      "Learn how to generate or manipulate images with our DALL·E models\n",
      "Introduction\n",
      "The Images API provides three methods for interacting with images:\n",
      "Creating images from scratch based on a text prompt\n",
      "Creating edits of an existing image based on a new text prompt\n",
      "Creating variations of an existing image\n",
      "This guide covers the basics of using these three API endpoints with useful code samples. To see them in action, check out our DALL·E preview app.\n",
      "Usage\n",
      "Generations\n",
      "The image generations endpoint allows you to create an original image given a text prompt. Generated images can have a size of 256x256, 512x512, or 1024x1024 pixels. Smaller sizes are faster to generate. You can request 1-10 images at a time using the n parameter.\n",
      "1 2 3 4 5 6 response = openai.Image.create( prompt=\"a white siamese cat\", n=1, size=\"1024x1024\" ) image_url = response['data'][0]['url']\n",
      "10 --------------------------------------------------------------------------------\n",
      "1 The more detailed the description, the more likely you are to get the result that you or your end user want. You can explore the examples in the DALL·E preview app for more prompting inspiration. Here's a quick example:\n",
      "Each image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour.\n",
      "Edits\n",
      "The image edits endpoint allows you to edit and extend an image by uploading a mask. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL·E preview app.\n",
      "1 2 3 4 5 6 7 8 response = openai.Image.create_edit( image=open(\"sunlit_lounge.png\", \"rb\"), mask=open(\"mask.png\", \"rb\"), prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\", n=1, size=\"1024x1024\" ) image_url = response['data'][0]['url']\n",
      "Prompt: a sunlit indoor lounge area with a pool containing a flamingo\n",
      "10 --------------------------------------------------------------------------------\n",
      "2 The uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don’t necessarily need to match the original image like the example above.\n",
      "Variations\n",
      "The image variations endpoint allows you to generate a variation of a given image.\n",
      "1 2 3 4 5 6 response = openai.Image.create_variation( image=open(\"corgi_and_cat_paw.png\", \"rb\"), n=1, size=\"1024x1024\" ) image_url = response['data'][0]['url']\n",
      "Similar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.\n",
      "Content moderation\n",
      "Prompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged. If you have any feedback on false positives or related issues, please contact us through our help center.\n",
      "Language-specific tips\n",
      "Using in-memory image data\n",
      "10 --------------------------------------------------------------------------------\n",
      "3 Language-specific tips\n",
      "Using in-memory image data\n",
      "The Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object:\n",
      "1 2 3 4 5 6 7 8 9 // This is the Buffer object that contains your image data const buffer = [your image data]; // Set a `name` that ends with .png so that the API knows it's a PNG image buffer.name = \"image.png\"; const response = await openai.createImageVariation( buffer, 1, \"1024x1024\" );\n",
      "Working with TypeScript\n",
      "If you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:\n",
      "1 2 3 4 5 6 // Cast the ReadStream to `any` to appease the TypeScript compiler const response = await openai.createImageVariation( fs.createReadStream(\"image.png\") as any, 1, \"1024x1024\" );\n",
      "And here's a similar example for in-memory image data:\n",
      "10 --------------------------------------------------------------------------------\n",
      "4 1 2 3 4 5 6 7 8 9 10 11 // This is the Buffer object that contains your image data const buffer: Buffer = [your image data]; // Cast the buffer to `any` so that we can set the `name` property const file: any = buffer; // Set a `name` that ends with .png so that the API knows it's a PNG image file.name = \"image.png\"; const response = await openai.createImageVariation( file, 1, \"1024x1024\" );\n",
      "Error handling\n",
      "API requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 try { const response = await openai.createImageVariation( fs.createReadStream(\"image.png\"), 1, \"1024x1024\" ); console.log(response.data.data[0].url); } catch (error) { if (error.response) { console.log(error.response.status); console.log(error.response.data); } else { console.log(error.message); } }\n",
      "10 --------------------------------------------------------------------------------\n",
      "0 Fine-tuning\n",
      "Learn how to customize a model for your application.\n",
      "Introduction\n",
      "This guide is intended for users of the new OpenAI fine-tuning API. If you are a legacy fine-tuning user, please refer to our legacy fine-tuning guide.\n",
      "Fine-tuning lets you get more out of the models available through the API by providing:\n",
      "Higher quality results than prompting\n",
      "Ability to train on more examples than can fit in a prompt\n",
      "Token savings due to shorter prompts\n",
      "Lower latency requests\n",
      "GPT models have been pre-trained on a vast amount of text. To use the models effectively, we include instructions and sometimes several examples in a prompt. Using demonstrations to show how to perform a task is often called \"few-shot learning.\"\n",
      "11 --------------------------------------------------------------------------------\n",
      "1 Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide as many examples in the prompt. This saves costs and enables lower-latency requests.\n",
      "At a high level, fine-tuning involves the following steps:\n",
      "Prepare and upload training data\n",
      "Train a new fine-tuned model\n",
      "Use your fine-tuned model\n",
      "Visit our pricing page to learn more about how fine-tuned model training and usage are billed.\n",
      "What models can be fine-tuned?\n",
      "We are working on enabling fine-tuning for GPT-4 and expect this feature to be available later this year.\n",
      "Fine-tuning is currently available for the following models:\n",
      "gpt-3.5-turbo-0613 (recommended)\n",
      "babbage-002\n",
      "davinci-002\n",
      "We expect gpt-3.5-turbo to be the right model for most users in terms of results and ease of use, unless you are migrating a legacy fine-tuned model.\n",
      "When to use fine-tuning\n",
      "11 --------------------------------------------------------------------------------\n",
      "2 When to use fine-tuning\n",
      "Fine-tuning GPT models can make them better for specific applications, but it requires a careful investment of time and effort. We recommend first attempting to get good results with prompt engineering, prompt chaining (breaking complex tasks into multiple prompts), and function calling, with the key reasons being:\n",
      "There are many tasks at which our models may not initially appear to perform well, but results can be improved with the right prompts - thus fine-tuning may not be necessary\n",
      "Iterating over prompts and other tactics has a much faster feedback loop than iterating with fine-tuning, which requires creating datasets and running training jobs\n",
      "In cases where fine-tuning is still necessary, initial prompt engineering work is not wasted - we typically see best results when using a good prompt in the fine-tuning data (or combining prompt chaining / tool use with fine-tuning)\n",
      "11 --------------------------------------------------------------------------------\n",
      "3 Our GPT best practices guide provides a background on some of the most effective strategies and tactics for getting better performance without fine-tuning. You may find it helpful to iterate quickly on prompts in our playground.\n",
      "Common use cases\n",
      "Some common use cases where fine-tuning can improve results:\n",
      "Setting the style, tone, format, or other qualitative aspects\n",
      "Improving reliability at producing a desired output\n",
      "Correcting failures to follow complex prompts\n",
      "Handling many edge cases in specific ways\n",
      "Performing a new skill or task that’s hard to articulate in a prompt\n",
      "One high-level way to think about these cases is when it’s easier to \"show, not tell\". In the sections to come, we will explore how to set up data for fine-tuning and various examples where fine-tuning improves the performance over the baseline model.\n",
      "11 --------------------------------------------------------------------------------\n",
      "4 Another scenario where fine-tuning is effective is in reducing costs and / or latency, by replacing GPT-4 or by utilizing shorter prompts, without sacrificing quality. If you can achieve good results with GPT-4, you can often reach similar quality with a fine-tuned gpt-3.5-turbo model by fine-tuning on the GPT-4 completions, possibly with a shortened instruction prompt.\n",
      "Preparing your dataset\n",
      "Once you have determined that fine-tuning is the right solution (i.e. you’ve optimized your prompt as far as it can take you and identified problems that the model still has), you’ll need to prepare data for training the model. You should create a diverse set of demonstration conversations that are similar to the conversations you will ask the model to respond to at inference time in production.\n",
      "11 --------------------------------------------------------------------------------\n",
      "5 Each example in the dataset should be a conversation in the same format as our Chat completions API, specifically a list of messages where each message has a role, content, and optional name. At least some of the training examples should directly target cases where the prompted model is not behaving as desired, and the provided assistant messages in the data should be the ideal responses you want the model to provide.\n",
      "Example format\n",
      "In this example, our goal is to create a chatbot that occasionally gives sarcastic responses, these are three training examples (conversations) we could create for a dataset:\n",
      "11 --------------------------------------------------------------------------------\n",
      "6 1 2 3 {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
      "The conversational chat format is required to fine-tune gpt-3.5-turbo. For babbage-002 and davinci-002, you can follow the prompt completion pair format used for legacy fine-tuning as shown below.\n",
      "11 --------------------------------------------------------------------------------\n",
      "7 1 2 3 {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"} {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"} {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
      "Crafting prompts\n",
      "We generally recommend taking the set of instructions and prompts that you found worked best for the model prior to fine-tuning, and including them in every training example. This should let you reach the best and most general results, especially if you have relatively few (e.g. under a hundred) training examples.\n",
      "If you would like to shorten the instructions or prompts that are repeated in every example to save costs, keep in mind that the model will likely behave as if those instructions were included, and it may be hard to get the model to ignore those \"baked-in\" instructions at inference time.\n",
      "It may take more training examples to arrive at good results, as the model has to learn entirely through demonstration and without guided instructions.\n",
      "Example count recommendations\n",
      "11 --------------------------------------------------------------------------------\n",
      "8 Example count recommendations\n",
      "To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples with gpt-3.5-turbo but the right number varies greatly based on the exact use case.\n",
      "We recommend starting with 50 well-crafted demonstrations and seeing if the model shows signs of improvement after fine-tuning. In some cases that may be sufficient, but even if the model is not yet production quality, clear improvements are a good sign that providing more data will continue to improve the model. No improvement suggests that you may need to rethink how to set up the task for the model or restructure the data before scaling beyond a limited example set.\n",
      "Train and test splits\n",
      "11 --------------------------------------------------------------------------------\n",
      "9 Train and test splits\n",
      "After collecting the initial dataset, we recommend splitting it into a training and test portion. When submitting a fine-tuning job with both training and test files, we will provide statistics on both during the course of training. These statistics will be your initial signal of how much the model is improving. Additionally, constructing a test set early on will be useful in making sure you are able to evaluate the model after training, by generating samples on the test set.\n",
      "Token limits\n",
      "Each training example is limited to 4096 tokens. Examples longer than this will be truncated to the first 4096 tokens when training. To be sure that your entire training example fits in context, consider checking that the total token counts in the message contents are under 4,000.\n",
      "You can compute token counts using our counting tokens notebook from the OpenAI cookbook.\n",
      "Estimate costs\n",
      "11 --------------------------------------------------------------------------------\n",
      "10 Estimate costs\n",
      "Please refer to the pricing page for details on cost per 1k input and output tokens (we do to charge for tokens that are part of the validation data). To estimate the costs for a specific fine-tuning job, use the following formula:\n",
      "base cost per 1k tokens * number of tokens in the input file * number of epochs trained\n",
      "For a training file with 100,000 tokens trained over 3 epochs, the expected cost would be ~$2.40 USD.\n",
      "Check data formatting\n",
      "Once you have compiled a dataset and before you create a fine-tuning job, it is important to check the data formatting. To do this, we created a simple Python script which you can use to find potential errors, review token counts, and estimate the cost of a fine-tuning job.\n",
      "Fine-tuning data format validation\n",
      "Learn about fine-tuning data formatting\n",
      "Upload a training file\n",
      "Once you have the data validated, the file needs to be uploaded using the Files API in order to be used with a fine-tuning jobs:\n",
      "11 --------------------------------------------------------------------------------\n",
      "11 1 2 3 4 5 6 7 import os import openai openai.api_key = os.getenv(\"OPENAI_API_KEY\") openai.File.create( file=open(\"mydata.jsonl\", \"rb\"), purpose='fine-tune' )\n",
      "After you upload the file, it may take some time to process. While the file is processing, you can still create a fine-tuning job but it will not start until the file processing has completed.\n",
      "Create a fine-tuned model\n",
      "After ensuring you have the right amount and structure for your dataset, and have uploaded the file, the next step is to create a fine-tuning job.\n",
      "Start your fine-tuning job using the OpenAI SDK:\n",
      "openai.FineTuningJob.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\")\n",
      "In this example, model is the name of the model you want to fine-tune (gpt-3.5-turbo, babbage-002, davinci-002, or an existing fine-tuned model) and training_file is the file ID that was returned when the training file was uploaded to the OpenAI API. You can customize your fine-tuned model's name using the suffix parameter.\n",
      "11 --------------------------------------------------------------------------------\n",
      "12 To set additional fine-tuning parameters like the validation_file or hyperparameters, please refer to the API specification for fine-tuning.\n",
      "After you've started a fine-tuning job, it may take some time to complete. Your job may be queued behind other jobs in our system, and training a model can take minutes or hours depending on the model and dataset size. After the model training is completed, the user who created the fine-tuning job will receive an email confirmation.\n",
      "In addition to creating a fine-tuning job, you can also list existing jobs, retrieve the status of a job, or cancel a job.\n",
      "11 --------------------------------------------------------------------------------\n",
      "13 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # List 10 fine-tuning jobs openai.FineTuningJob.list(limit=10) # Retrieve the state of a fine-tune openai.FineTuningJob.retrieve(\"ftjob-abc123\") # Cancel a job openai.FineTuningJob.cancel(\"ftjob-abc123\") # List up to 10 events from a fine-tuning job openai.FineTuningJob.list_events(id=\"ftjob-abc123\", limit=10) # Delete a fine-tuned model (must be an owner of the org the model was created in) openai.Model.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:abc123\")\n",
      "Use a fine-tuned model\n",
      "When a job has succeeded, you will see the fine_tuned_model field populated with the name of the model when you retrieve the job details. You may now specify this model as a parameter to in the Chat completions (for gpt-3.5-turbo) or legacy Completions API (for babbage-002 and davinci-002), and make requests to it using the Playground.\n",
      "11 --------------------------------------------------------------------------------\n",
      "14 After your job is completed, the model should be available right away for inference use. In some cases, it may take several minutes for your model to become ready to handle requests. If requests to your model time out or the model name cannot be found, it is likely because your model is still being loaded. If this happens, try again in a few minutes.\n",
      "1 2 3 4 5 6 7 8 completion = openai.ChatCompletion.create( model=\"ft:gpt-3.5-turbo:my-org:custom_suffix:id\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"} ] ) print(completion.choices[0].message)\n",
      "You can start making requests by passing the model name as shown above and in our GPT guide.\n",
      "Analyzing your fine-tuned model\n",
      "11 --------------------------------------------------------------------------------\n",
      "15 Analyzing your fine-tuned model\n",
      "We provide the following training metrics computed over the course of training: training loss, training token accuracy, test loss, and test token accuracy. These statistics are meant to provide a sanity check that training went smoothly (loss should decrease, token accuracy should increase). While an active fine-tuning jobs is running, you can view an event object which contains some useful metrics:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 { \"object\": \"fine_tuning.job.event\", \"id\": \"ftevent-abc-123\", \"created_at\": 1693582679, \"level\": \"info\", \"message\": \"Step 100/100: training loss=0.00\", \"data\": { \"step\": 100, \"train_loss\": 1.805623287509661e-5, \"train_mean_token_accuracy\": 1.0 }, \"type\": \"metrics\" }\n",
      "11 --------------------------------------------------------------------------------\n",
      "16 After a fine-tuning job has finished, you can also see metrics around how the training process went by querying a fine-tuning job, extracting a file ID from the result_files, and then retrieving that files content. Each results CSV file has the following columns: step, train_loss, train_accuracy, valid_loss, and valid_mean_token_accuracy.\n",
      "1 2 3 4 5 6 step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy 1,1.52347,0.0,, 2,0.57719,0.0,, 3,3.63525,0.0,, 4,1.72257,0.0,, 5,1.52379,0.0,,\n",
      "While metrics can he helpful, evaluating samples from the fine-tuned model provides the most relevant sense of model quality. We recommend generating samples from both the base model and the fine-tuned model on a test set, and comparing the samples side by side. The test set should ideally include the full distribution of inputs that you might send to the model in a production use case. If manual evaluation is too time-consuming, consider using our Evals library to automate future evaluations.\n",
      "11 --------------------------------------------------------------------------------\n",
      "17 Iterating on data quality\n",
      "If the results from a fine-tuning job are not as good as you expected, consider the following ways to adjust the training dataset:\n",
      "Collect examples to target remaining issues\n",
      "If the model still isn’t good at certain aspects, add training examples that directly show the model how to do these aspects correctly\n",
      "Scrutinize existing examples for issues\n",
      "If your model has grammar, logic, or style issues, check if your data has any of the same issues. For instance, if the model now says \"I will schedule this meeting for you\" (when it shouldn’t), see if existing examples teach the model to say it can do new things that it can’t do\n",
      "Consider the balance and diversity of data\n",
      "If 60% of the assistant responses in the data says \"I cannot answer this\", but at inference time only 5% of responses should say that, you will likely get an overabundance of refusals\n",
      "Make sure your training examples contain all of the information needed for the response\n",
      "11 --------------------------------------------------------------------------------\n",
      "18 If we want the model to compliment a user based on their personal traits and a training example includes assistant compliments for traits not found in the preceding conversation, the model may learn to hallucinate information\n",
      "Look at the agreement / consistency in the training examples\n",
      "If multiple people created the training data, it’s likely that model performance will be limited by the level of agreement / consistency between people. For instance, in a text extraction task, if people only agreed on 70% of extracted snippets, the model would likely not be able to do better than this\n",
      "Make sure your all of your training examples are in the same format, as expected for inference\n",
      "Iterating on data quantity\n",
      "11 --------------------------------------------------------------------------------\n",
      "19 Iterating on data quantity\n",
      "Once you’re satisfied with the quality and distribution of the examples, you can consider scaling up the number of training examples. This tends to help the model learn the task better, especially around possible \"edge cases\". We expect a similar amount of improvement every time you double the number of training examples. You can loosely estimate the expected quality gain from increasing the training data size by:\n",
      "Fine-tuning on your current dataset\n",
      "Fine-tuning on half of your current dataset\n",
      "Observing the quality gap between the two\n",
      "In general, if you have to make a trade-off, a smaller amount of high-quality data is generally more effective than a larger amount of low-quality data.\n",
      "Iterating on hyperparameters\n",
      "We allow you to specify the number of epochs to fine-tune a model for. We recommend initially training without specifying the number of epochs, allowing us to pick a default for you based on dataset size, then adjusting if you observe the following:\n",
      "11 --------------------------------------------------------------------------------\n",
      "20 If the model does not follow the training data as much as expected increase the number by 1 or 2 epochs\n",
      "This is more common for tasks for which there is a single ideal completion (or a small set of ideal completions which are similar). Some examples include classification, entity extraction, or structured parsing. These are often tasks for which you can compute a final accuracy metric against a reference answer.\n",
      "If the model becomes less diverse than expected decrease the number by 1 or 2 epochs\n",
      "This is more common for tasks for which there are a wide range of possible good completions\n",
      "openai.FineTuningJob.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\", hyperparameters={\"n_epochs\":2})\n",
      "Fine-tuning examples\n",
      "Now that we have explored the basics of the fine-tuning API, let’s look at going through the fine-tuning lifecycle for a few different use cases.\n",
      "11 --------------------------------------------------------------------------------\n",
      "21 In this example, we will explore how to build a fine-tuned model which gets the model follow specific style and tone guidance beyond what is possible with prompting alone.\n",
      "To begin, we create a sample set of messages showing what the model should which in this case is misspelled words.\n",
      "11 --------------------------------------------------------------------------------\n",
      "22 1 2 3 {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
      "If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples.\n",
      "11 --------------------------------------------------------------------------------\n",
      "23 After getting the data that will potentially improve the model, the next step is to check if the data meets all the formatting requirements.\n",
      "Now that we have the data formatted and validated, the final training step is to kick off a job to create the fine-tuned model. You can do this via the OpenAI CLI or one of our SDKs as shown below:\n",
      "1 2 3 openai.File.create(file=open(\"marv.jsonl\", \"rb\"), purpose='fine-tune') openai.FineTuningJob.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\")\n",
      "Once the training job is done, you will be able to use your fine-tuned model.\n",
      "Another type of use case which works really well with fine-tuning is getting the model to provide structured information, in this case about sports headlines:\n",
      "11 --------------------------------------------------------------------------------\n",
      "24 1 2 3 4 {\"messages\": [{\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: \"player\" (full name)\", \"team\", \"sport\", and \"gender\".},{\"role\": \"user\", \"content\": \"Sources: Colts grant RB Taylor OK to seek trade\"}, {\"role\": \"assistant\", \"content\": \"{\"player\": \"Jonathan Taylor\", \"team\": \"Colts\", \"sport\": \"football\", \"gender\": \"male\" }\"},]} {\"messages\": [{\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: \"player\" (full name)\", \"team\", \"sport\", and \"gender\".},{\"role\": \"user\", \"content\": \"OSU 'split down middle' on starting QB battle\"}, {\"role\": \"assistant\", \"content\": \"{\"player\": null, \"team\": \"OSU\", \"sport\": \"football\", \"gender\": null }\"},]}\n",
      "If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples.\n",
      "After getting the data that will potentially improve the model, the next step is to check if the data meets all the formatting requirements.\n",
      "11 --------------------------------------------------------------------------------\n",
      "25 Now that we have the data formatted and validated, the final training step is to kick off a job to create the fine-tuned model. You can do this via the OpenAI CLI or one of our SDKs as shown below:\n",
      "1 2 3 openai.File.create(file=open(\"sports-context.jsonl\", \"rb\"), purpose='fine-tune') openai.FineTuningJob.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\")\n",
      "Once the training job is done, you will be able to use your fine-tuned model and make a request that looks like the following:\n",
      "1 2 3 4 5 6 7 8 9 completion = openai.ChatCompletion.create( model=\"ft:gpt-3.5-turbo:my-org:custom_suffix:id\", messages=[ {\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: player (full name), team, sport, and gender\"}, {\"role\": \"user\", \"content\": \"Richardson wins 100m at worlds to cap comeback\"} ] ) print(completion.choices[0].message)\n",
      "Based on the formatted training data, the response should look like the following:\n",
      "11 --------------------------------------------------------------------------------\n",
      "26 {\"player\": \"Sha'Carri Richardson\", \"team\": null\", \"sport\": \"track and field\", \"gender\": \"female\"}\n",
      "The chat completions API supports function calling. Including a long list of functions in the completions API can consume a considerable number of prompt tokens and sometimes the model hallucinates or does not provide valid JSON output.\n",
      "Fine-tuning a model with function calling examples can allow you to:\n",
      "Get similarly formatted responses even when the full function definition isn't present\n",
      "Get more accurate and consistent outputs\n",
      "Format your examples as shown, with each line including a list of \"messages\" and an optional list of \"functions\":\n",
      "11 --------------------------------------------------------------------------------\n",
      "27 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"messages\": [ {\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"}, {\"role\": \"assistant\", \"function_call\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celcius\\\"}\"} ], \"functions\": [{ \"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]} }, \"required\": [\"location\", \"format\"] } }] }\n",
      "If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples.\n",
      "If your goal is to use less tokens, some useful techniques are:\n",
      "Omit function and parameter descriptions: remove the description field from function and parameters\n",
      "Omit parameters: remove the entire properties field from the parameters object\n",
      "11 --------------------------------------------------------------------------------\n",
      "28 Omit function entirely: remove the entire function object from the functions array\n",
      "If your goal is to maximize the correctness of the function calling output, we recommend using the same function definitions for both training and querying the fine-tuned model.\n",
      "Fine-tuning on function calling can also be used to customize the model's response to function outputs. To do this you can include a function response message and an assistant message interpreting that response:\n",
      "1 2 3 4 5 6 7 8 9 { \"messages\": [ {\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"}, {\"role\": \"assistant\", \"function_call\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celcius\\\"}\"}} {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": \"21.0\"}, {\"role\": \"assistant\", \"content\": \"It is 21 degrees celsius in San Francisco, CA\"} ], \"functions\": [...] // same as before }\n",
      "Migration of legacy models\n",
      "11 --------------------------------------------------------------------------------\n",
      "29 Migration of legacy models\n",
      "For users migrating from /v1/fine-tunes to the updated /v1/fine_tuning/jobs API and newer models, the main difference you can expect is the updated API. The legacy prompt completion pair data format has been retained for the updated babbage-002 and davinci-002 models to ensure a smooth transition. The new models will support fine-tuning with 4k token context and have a knowledge cutoff of September 2021.\n",
      "For most tasks, you should expect to get better performance from gpt-3.5-turbo than from the GPT base models.\n",
      "FAQ\n",
      "When should I use fine-tuning vs embeddings with retrieval?\n",
      "Embeddings with retrieval is best suited for cases when you need to have a large database of documents with relevant context and information.\n",
      "11 --------------------------------------------------------------------------------\n",
      "30 By default OpenAI’s models are trained to be helpful generalist assistants. Fine-tuning can be used to make a model which is narrowly focused, and exhibits specific ingrained behavior patterns. Retrieval strategies can be used to make new information available to a model by providing it with relevant context before generating its response. Retrieval strategies are not an alternative to fine-tuning and can in fact be complementary to it.\n",
      "When can I fine-tune GPT-4 or GPT-3.5-Turbo-16k?\n",
      "We plan to release support for fine-tuning both of these models later this year.\n",
      "How do I know if my fine-tuned model is actually better than the base model?\n",
      "We recommend generating samples from both the base model and the fine-tuned model on a test set of chat conversations, and comparing the samples side by side. For more comprehensive evaluations, consider using the OpenAI evals framework to create an eval specific to your use case.\n",
      "Can I continue fine-tuning a model that has already been fine-tuned?\n",
      "11 --------------------------------------------------------------------------------\n",
      "31 Yes, you can pass the name of a fine-tuned model into the model parameter when creating a fine-tuning job. This will start a new fine-tuning job using the fine-tuned model as the starting point.\n",
      "How can I estimate the cost of fine-tuning a model?\n",
      "Please refer to the estimate cost section above.\n",
      "Does the new fine-tuning endpoint still work with Weights & Biases for tracking metrics?\n",
      "No, we do not currently support this integration but are working to enable it in the near future.\n",
      "How many fine-tuning jobs can I have running at once?\n",
      "Please refer to our rate limit guide for the most up to date information on the limits.\n",
      "How do rate limits work on fine-tuned models?\n",
      "11 --------------------------------------------------------------------------------\n",
      "32 How do rate limits work on fine-tuned models?\n",
      "A fine-tuned model pulls from the same shared rate limit as the model it is based off of. For example, if you use half your TPM rate limit in a given time period with the standard gpt-3.5-turbo model, any model(s) you fine-tuned from gpt-3.5-turbo would only have the remaining half of the TPM rate limit accessible since the capacity is shared across all models of the same type.\n",
      "Put another way, having fine-tuned models does not give you more capacity to use our models from a total throughput perspective.\n",
      "11 --------------------------------------------------------------------------------\n",
      "0 Embeddings\n",
      "What are embeddings?\n",
      "OpenAI’s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:\n",
      "Search (where results are ranked by relevance to a query string)\n",
      "Clustering (where text strings are grouped by similarity)\n",
      "Recommendations (where items with related text strings are recommended)\n",
      "Anomaly detection (where outliers with little relatedness are identified)\n",
      "Diversity measurement (where similarity distributions are analyzed)\n",
      "Classification (where text strings are classified by their most similar label)\n",
      "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n",
      "Visit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent.\n",
      "To see embeddings in action, check out our code samples\n",
      "Classification\n",
      "Topic clustering\n",
      "Search\n",
      "Recommendations\n",
      "Browse Samples‍\n",
      "12 --------------------------------------------------------------------------------\n",
      "1 Search\n",
      "Recommendations\n",
      "Browse Samples‍\n",
      "How to get embeddings\n",
      "To get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use.\n",
      "Example requests:\n",
      "1 2 3 4 5 6 7 curl https://api.openai.com/v1/embeddings \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"input\": \"Your text string goes here\", \"model\": \"text-embedding-ada-002\" }'\n",
      "Example response:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"data\": [ { \"embedding\": [ -0.006929283495992422, -0.005336422007530928, ... -4.547132266452536e-05, -0.024047505110502243 ], \"index\": 0, \"object\": \"embedding\" } ], \"model\": \"text-embedding-ada-002\", \"object\": \"list\", \"usage\": { \"prompt_tokens\": 5, \"total_tokens\": 5 } }\n",
      "See more Python code examples in the OpenAI Cookbook.\n",
      "When using OpenAI embeddings, please keep in mind their limitations and risks.\n",
      "Embedding models\n",
      "12 --------------------------------------------------------------------------------\n",
      "2 Embedding models\n",
      "OpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID).\n",
      "We recommend using text-embedding-ada-002 for nearly all use cases. It’s better, cheaper, and simpler to use. Read the blog post announcement.\n",
      "Model generationtokenizermax input tokensknowledge cutoff\n",
      "V2\tcl100k_base\t8191\tSep 2021\t\n",
      "V1\tGPT-2/GPT-3\t2046\tAug 2020\t\n",
      "Usage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page):\n",
      "ModelRough pages per dollarExample performance on BEIR search eval\n",
      "text-embedding-ada-002\t3000\t53.9\t\n",
      "*-davinci-*-001\t6\t52.8\t\n",
      "*-curie-*-001\t60\t50.9\t\n",
      "*-babbage-*-001\t240\t50.4\t\n",
      "*-ada-*-001\t300\t49.0\t\n",
      "Second-generation models\n",
      "Model nametokenizermax input tokensoutput dimensions\n",
      "text-embedding-ada-002\tcl100k_base\t8191\t1536\t\n",
      "First-generation models (not recommended)\n",
      "12 --------------------------------------------------------------------------------\n",
      "3 First-generation models (not recommended)\n",
      "All first-generation models (those ending in -001) use the GPT-3 tokenizer and have a max input of 2046 tokens.\n",
      "First-generation embeddings are generated by five different model families tuned for three different tasks: text search, text similarity and code search. The search models come in pairs: one for short queries and one for long documents. Each family includes up to four models on a spectrum of quality and speed:\n",
      "Modeloutput dimensions\n",
      "Ada\t1024\t\n",
      "Babbage\t2048\t\n",
      "Curie\t4096\t\n",
      "Davinci\t12288\t\n",
      "Davinci is the most capable, but is slower and more expensive than the other models. Ada is the least capable, but is significantly faster and cheaper.\n",
      "Similarity embeddings\n",
      "Similarity models are best at capturing semantic similarity between pieces of text.\n",
      "Use casesAvailable models\n",
      "Clustering, regression, anomaly detection, visualization\ttext-similarity-ada-001\n",
      "text-similarity-babbage-001\n",
      "text-similarity-curie-001\n",
      "text-similarity-davinci-001\t\n",
      "Text search embeddings\n",
      "12 --------------------------------------------------------------------------------\n",
      "4 Text search embeddings\n",
      "Text search models help measure which long documents are most relevant to a short search query. Two models are used: one for embedding the search query and one for embedding the documents to be ranked. The document embeddings closest to the query embedding should be the most relevant.\n",
      "Use casesAvailable models\n",
      "Search, context relevance, information retrieval\ttext-search-ada-doc-001\n",
      "text-search-ada-query-001\n",
      "text-search-babbage-doc-001\n",
      "text-search-babbage-query-001\n",
      "text-search-curie-doc-001\n",
      "text-search-curie-query-001\n",
      "text-search-davinci-doc-001\n",
      "text-search-davinci-query-001\t\n",
      "Code search embeddings\n",
      "Similarly to search embeddings, there are two types: one for embedding natural language search queries and one for embedding code snippets to be retrieved.\n",
      "Use casesAvailable models\n",
      "Code search and relevance\tcode-search-ada-code-001\n",
      "code-search-ada-text-001\n",
      "code-search-babbage-code-001\n",
      "code-search-babbage-text-001\n",
      "12 --------------------------------------------------------------------------------\n",
      "5 code-search-babbage-text-001\t\n",
      "With the -001 text embeddings (not -002, and not code embeddings), we suggest replacing newlines (\\n) in your input with a single space, as we have seen worse results when newlines are present.\n",
      "Use cases\n",
      "Here we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.\n",
      "Obtaining the embeddings\n",
      "The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:\n",
      "Product IdUser IdScoreSummaryText\n",
      "B001E4KFG0\tA3SGXH7AUHU8GW\t5\tGood Quality Dog Food\tI have bought several of the Vitality canned...\t\n",
      "B00813GRG4\tA1D87F6ZCVE5NK\t1\tNot as Advertised\tProduct arrived labeled as Jumbo Salted Peanut...\n",
      "12 --------------------------------------------------------------------------------\n",
      "6 We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.\n",
      "Get embeddings from dataset\n",
      "1 2 3 4 5 6 def get_embedding(text, model=\"text-embedding-ada-002\"): text = text.replace(\"\\n\", \" \") return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding'] df['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002')) df.to_csv('output/embedded_1k_reviews.csv', index=False)\n",
      "To load the data from a saved file, you can run the following:\n",
      "1 2 3 4 import pandas as pd df = pd.read_csv('output/embedded_1k_reviews.csv') df['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)\n",
      "Visualizing_embeddings_in_2D.ipynb\n",
      "The size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions.\n",
      "12 --------------------------------------------------------------------------------\n",
      "7 We color the individual reviews based on the star rating which the reviewer has given:\n",
      "1-star: red\n",
      "2-star: dark orange\n",
      "3-star: gold\n",
      "4-star: turquoise\n",
      "5-star: dark green\n",
      "The visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews.\n",
      "12 --------------------------------------------------------------------------------\n",
      "8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import pandas as pd from sklearn.manifold import TSNE import matplotlib.pyplot as plt import matplotlib df = pd.read_csv('output/embedded_1k_reviews.csv') matrix = df.ada_embedding.apply(eval).to_list() # Create a t-SNE model and transform the data tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200) vis_dims = tsne.fit_transform(matrix) colors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"] x = [x for x,y in vis_dims] y = [y for x,y in vis_dims] color_indices = df.Score.values - 1 colormap = matplotlib.colors.ListedColormap(colors) plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3) plt.title(\"Amazon ratings visualized in language using t-SNE\")\n",
      "Embedding as a text feature encoder for ML algorithms\n",
      "Regression_using_embeddings.ipynb\n",
      "12 --------------------------------------------------------------------------------\n",
      "9 Regression_using_embeddings.ipynb\n",
      "An embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.\n",
      "We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.\n",
      "This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.\n",
      "12 --------------------------------------------------------------------------------\n",
      "10 1 2 3 4 5 6 7 8 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( list(df.ada_embedding.values), df.Score, test_size = 0.2, random_state=42 )\n",
      "Regression using the embedding features\n",
      "Embeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer’s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.\n",
      "We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star.\n",
      "1 2 3 4 5 from sklearn.ensemble import RandomForestRegressor rfr = RandomForestRegressor(n_estimators=100) rfr.fit(X_train, y_train) preds = rfr.predict(X_test)\n",
      "12 --------------------------------------------------------------------------------\n",
      "11 Classification using the embedding features\n",
      "Classification_using_embeddings.ipynb\n",
      "This time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.\n",
      "After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression.\n",
      "1 2 3 4 5 6 from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report, accuracy_score clf = RandomForestClassifier(n_estimators=100) clf.fit(X_train, y_train) preds = clf.predict(X_test)\n",
      "Zero-shot_classification_with_embeddings.ipynb\n",
      "12 --------------------------------------------------------------------------------\n",
      "12 Zero-shot_classification_with_embeddings.ipynb\n",
      "We can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity.\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 from openai.embeddings_utils import cosine_similarity, get_embedding df= df[df.Score!=3] df['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'}) labels = ['negative', 'positive'] label_embeddings = [get_embedding(label, model=model) for label in labels] def label_score(review_embedding, label_embeddings): return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0]) prediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'\n",
      "Obtaining user and product embeddings for cold-start recommendation\n",
      "12 --------------------------------------------------------------------------------\n",
      "13 User_and_product_embeddings.ipynb\n",
      "We can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.\n",
      "We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product.\n",
      "user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean) prod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)\n",
      "Clustering.ipynb\n",
      "12 --------------------------------------------------------------------------------\n",
      "14 Clustering.ipynb\n",
      "Clustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.\n",
      "In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews.\n",
      "1 2 3 4 5 6 7 8 9 import numpy as np from sklearn.cluster import KMeans matrix = np.vstack(df.ada_embedding.values) n_clusters = 4 kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42) kmeans.fit(matrix) df['Cluster'] = kmeans.labels_\n",
      "Text search using embeddings\n",
      "Semantic_text_search_using_embeddings.ipynb\n",
      "To retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents.\n",
      "12 --------------------------------------------------------------------------------\n",
      "15 1 2 3 4 5 6 7 8 9 from openai.embeddings_utils import get_embedding, cosine_similarity def search_reviews(df, product_description, n=3, pprint=True): embedding = get_embedding(product_description, model='text-embedding-ada-002') df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding)) res = df.sort_values('similarities', ascending=False).head(n) return res res = search_reviews(df, 'delicious beans', n=3)\n",
      "Code search using embeddings\n",
      "Code_search.ipynb\n",
      "Code search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the text-embedding-ada-002 model.\n",
      "To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant.\n",
      "12 --------------------------------------------------------------------------------\n",
      "16 1 2 3 4 5 6 7 8 9 10 11 from openai.embeddings_utils import get_embedding, cosine_similarity df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002')) def search_functions(df, code_query, n=3, pprint=True, n_lines=7): embedding = get_embedding(code_query, model='text-embedding-ada-002') df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding)) res = df.sort_values('similarities', ascending=False).head(n) return res res = search_functions(df, 'Completions API tests', n=3)\n",
      "Recommendations using embeddings\n",
      "Recommendation_using_embeddings.ipynb\n",
      "Because shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.\n",
      "12 --------------------------------------------------------------------------------\n",
      "17 Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the AG news dataset (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.\n",
      "12 --------------------------------------------------------------------------------\n",
      "18 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def recommendations_from_strings( strings: List[str], index_of_source_string: int, model=\"text-embedding-ada-002\", ) -> List[int]: \"\"\"Return nearest neighbors of a given string.\"\"\" # get embeddings for all strings embeddings = [embedding_from_string(string, model=model) for string in strings] # get the embedding of the source string query_embedding = embeddings[index_of_source_string] # get distances between the source embedding and other embeddings (function from embeddings_utils.py) distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\") # get indices of nearest neighbors (function from embeddings_utils.py) indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances) return indices_of_nearest_neighbors\n",
      "Limitations & risks\n",
      "Our embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations.\n",
      "Social bias\n",
      "12 --------------------------------------------------------------------------------\n",
      "19 Social bias\n",
      "Limitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups.\n",
      "We found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes.\n",
      "For example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women.\n",
      "These benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias.\n",
      "12 --------------------------------------------------------------------------------\n",
      "20 These tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance.\n",
      "Please contact our support team via chat if you have any questions; we are happy to advise on this.\n",
      "Blindness to recent events\n",
      "Limitation: Models lack knowledge of events that occurred after August 2020.\n",
      "Our models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well.\n",
      "Frequently asked questions\n",
      "How can I tell how many tokens a string has before I embed it?\n",
      "In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken.\n",
      "Example code:\n",
      "12 --------------------------------------------------------------------------------\n",
      "21 Example code:\n",
      "1 2 3 4 5 6 7 8 9 import tiktoken def num_tokens_from_string(string: str, encoding_name: str) -> int: \"\"\"Returns the number of tokens in a text string.\"\"\" encoding = tiktoken.get_encoding(encoding_name) num_tokens = len(encoding.encode(string)) return num_tokens num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n",
      "For second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding.\n",
      "More details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken.\n",
      "How can I retrieve K nearest embedding vectors quickly?\n",
      "For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub.\n",
      "Vector database options include:\n",
      "Chroma, an open-source embeddings store\n",
      "Elasticsearch, a popular search/analytics engine and vector database\n",
      "Milvus, a vector database built for scalable similarity search\n",
      "Pinecone, a fully managed vector database\n",
      "12 --------------------------------------------------------------------------------\n",
      "22 Pinecone, a fully managed vector database\n",
      "Qdrant, a vector search engine\n",
      "Redis as a vector database\n",
      "Typesense, fast open source vector search\n",
      "Weaviate, an open-source vector search engine\n",
      "Zilliz, data infrastructure, powered by Milvus\n",
      "Which distance function should I use?\n",
      "We recommend cosine similarity. The choice of distance function typically doesn’t matter much.\n",
      "OpenAI embeddings are normalized to length 1, which means that:\n",
      "Cosine similarity can be computed slightly faster using just a dot product\n",
      "Cosine similarity and Euclidean distance will result in the identical rankings\n",
      "Can I share my embeddings online?\n",
      "Customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our Terms of Use.\n",
      "12 --------------------------------------------------------------------------------\n",
      "0 Speech to text\n",
      "Learn how to turn audio into text\n",
      "Introduction\n",
      "The speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:\n",
      "Transcribe audio into whatever language the audio is in.\n",
      "Translate and transcribe the audio into english.\n",
      "File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.\n",
      "Quickstart\n",
      "Transcriptions\n",
      "The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.\n",
      "1 2 3 4 # Note: you need to be using OpenAI Python v0.27.0 for the code below to work import openai audio_file= open(\"/path/to/file/audio.mp3\", \"rb\") transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
      "By default, the response type will be json with the raw text included.\n",
      "13 --------------------------------------------------------------------------------\n",
      "1 { \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. .... }\n",
      "The Audio API also allows you to set additional parameters in a request. For example, if you want to set the response_format as text, your request would look like the following:\n",
      "transcript = openai.Audio.translate(model=\"whisper-1\", file=\"openai.mp3\", response_format=\"text\")\n",
      "The API Reference includes the full list of available parameters.\n",
      "Translations\n",
      "The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.\n",
      "1 2 3 4 # Note: you need to be using OpenAI Python v0.27.0 for the code below to work import openai audio_file= open(\"/path/to/file/german.mp3\", \"rb\") transcript = openai.Audio.translate(\"whisper-1\", audio_file)\n",
      "13 --------------------------------------------------------------------------------\n",
      "2 In this case, the inputted audio was german and the outputted text looks like:\n",
      "Hello, my name is Wolfgang and I come from Germany. Where are you heading today?\n",
      "We only support translation into english at this time.\n",
      "Supported languages\n",
      "We currently support the following languages through both the transcriptions and translations endpoint:\n",
      "Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.\n",
      "13 --------------------------------------------------------------------------------\n",
      "3 While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.\n",
      "Longer inputs\n",
      "By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.\n",
      "One way to handle this is to use the PyDub open source Python package to split the audio:\n",
      "1 2 3 4 5 6 7 8 9 10 from pydub import AudioSegment song = AudioSegment.from_mp3(\"good_morning.mp3\") # PyDub handles time in milliseconds ten_minutes = 10 * 60 * 1000 first_10_minutes = song[:ten_minutes] first_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")\n",
      "13 --------------------------------------------------------------------------------\n",
      "4 OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.\n",
      "Prompting\n",
      "You can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:\n",
      "Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL·E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL·E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"\n",
      "13 --------------------------------------------------------------------------------\n",
      "5 To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.\n",
      "Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"\n",
      "The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"\n",
      "13 --------------------------------------------------------------------------------\n",
      "6 Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.\n",
      "Improving reliability\n",
      "As we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:\n",
      "Using the prompt parameter\n",
      "The first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.\n",
      "Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.\n",
      "13 --------------------------------------------------------------------------------\n",
      "7 transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")\n",
      "While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.\n",
      "Post-processing with GPT-4\n",
      "The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.\n",
      "We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.\n",
      "13 --------------------------------------------------------------------------------\n",
      "8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 system_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\" def generate_corrected_transcript(temperature, system_prompt, audio_file): response = openai.ChatCompletion.create( model=\"gpt-4\", temperature=temperature, messages=[ { \"role\": \"system\", \"content\": system_prompt }, { \"role\": \"user\", \"content\": transcribe(audio_file, \"\") } ] ) return response['choices'][0]['message']['content'] corrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)\n",
      "13 --------------------------------------------------------------------------------\n",
      "9 If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.\n",
      "13 --------------------------------------------------------------------------------\n",
      "0 Moderation\n",
      "Overview\n",
      "The moderations endpoint is a tool you can use to check whether content complies with OpenAI's usage policies. Developers can thus identify content that our usage policies prohibits and take action, for instance by filtering it.\n",
      "The models classifies the following categories:\n",
      "CategoryDescription\n",
      "hate\tContent that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harrassment.\t\n",
      "hate/threatening\tHateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.\t\n",
      "harassment\tContent that expresses, incites, or promotes harassing language towards any target.\t\n",
      "harassment/threatening\tHarassment content that also includes violence or serious harm towards any target.\n",
      "14 --------------------------------------------------------------------------------\n",
      "1 self-harm\tContent that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.\t\n",
      "self-harm/intent\tContent where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.\t\n",
      "self-harm/instructions\tContent that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.\t\n",
      "sexual\tContent meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).\t\n",
      "sexual/minors\tSexual content that includes an individual who is under 18 years old.\t\n",
      "violence\tContent that depicts death, violence, or physical injury.\t\n",
      "violence/graphic\tContent that depicts death, violence, or physical injury in graphic detail.\n",
      "14 --------------------------------------------------------------------------------\n",
      "2 The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. We currently disallow other use cases. Accuracy may be lower on longer pieces of text. For higher accuracy, try splitting long pieces of text into smaller chunks each less than 2,000 characters.\n",
      "We are continuously working to improve the accuracy of our classifier. Our support for non-English languages is currently limited.\n",
      "Quickstart\n",
      "To obtain a classification for a piece of text, make a request to the moderation endpoint as demonstrated in the following code snippets:\n",
      "1 2 3 4 5 curl https://api.openai.com/v1/moderations \\ -X POST \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{\"input\": \"Sample text goes here\"}'\n",
      "Below is an example output of the endpoint. It returns the following fields:\n",
      "flagged: Set to true if the model classifies the content as violating OpenAI's usage policies, false otherwise.\n",
      "14 --------------------------------------------------------------------------------\n",
      "3 categories: Contains a dictionary of per-category binary usage policies violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise.\n",
      "category_scores: Contains a dictionary of per-category raw scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence. The scores should not be interpreted as probabilities.\n",
      "14 --------------------------------------------------------------------------------\n",
      "4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"id\": \"modr-XXXXX\", \"model\": \"text-moderation-005\", \"results\": [ { \"flagged\": true, \"categories\": { \"sexual\": false, \"hate\": false, \"harassment\": false, \"self-harm\": false, \"sexual/minors\": false, \"hate/threatening\": false, \"violence/graphic\": false, \"self-harm/intent\": false, \"self-harm/instructions\": false, \"harassment/threatening\": true, \"violence\": true, }, \"category_scores\": { \"sexual\": 1.2282071e-06, \"hate\": 0.010696256, \"harassment\": 0.29842457, \"self-harm\": 1.5236925e-08, \"sexual/minors\": 5.7246268e-08, \"hate/threatening\": 0.0060676364, \"violence/graphic\": 4.435014e-06, \"self-harm/intent\": 8.098441e-10, \"self-harm/instructions\": 2.8498655e-11, \"harassment/threatening\": 0.63055265, \"violence\": 0.99011886, } } ] }\n",
      "OpenAI will continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on category_scores may need recalibration over time.\n",
      "14 --------------------------------------------------------------------------------\n",
      "0 Rate limits\n",
      "Overview\n",
      "What are rate limits?\n",
      "A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.\n",
      "Why do we have rate limits?\n",
      "Rate limits are a common practice for APIs, and they're put in place for a few different reasons:\n",
      "They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.\n",
      "Rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.\n",
      "15 --------------------------------------------------------------------------------\n",
      "1 Rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.\n",
      "Please work through this document in its entirety to better understand how OpenAI’s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the Rate Limit Increase Request form with details regarding how to fill it out in the last section.\n",
      "What are the rate limits for our API?\n",
      "You can view the rate limits for your organization under the rate limits section of the account management page. Rate limits are automatically adjusted based on history of good use.\n",
      "15 --------------------------------------------------------------------------------\n",
      "2 We enforce rate limits at the organization level, not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in three ways: RPM (requests per minute), RPD (requests per day), and TPM (tokens per minute). Rate limit can be hit by any of the three options depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Completions endpoint and that would fill your limit (if your RPM was 20), even if you did not send 150k tokens (if your TPM limit was 150k) within those 20 requests.\n",
      "Usage tiers\n",
      "Your rate limit and spending limit (quota) are automatically adjusted based on a number of factors. As your usage of the OpenAI API goes up and you successfully pay the bill, we automatically increase your usage tier. Below is a breakdown of the first 3 usage tiers.\n",
      "TierQualificationMax creditsRequest limitsToken limits\n",
      "Free\tUser must be in an allowed geography\t$100\t3 RPM, 200 RPD\t20K TPM (GPT-3.5), 4K TPM (GPT-4)\n",
      "15 --------------------------------------------------------------------------------\n",
      "3 Tier 1\t$5 paid\t$100\t500 RPM, 10K RPD\t40K TPM (GPT-3.5), 10K TPM (GPT-4)\t\n",
      "Tier 2\t$50 paid and 7+ days since first successful payment\t$250\t5000 RPM\t80K TPM (GPT-3.5), 20K TPM (GPT-4)\t\n",
      "We plan to expose additional usage tiers over time and will adjust these accordingly in response to capacity and fraud activity. Our main goal with usage tiers is to automatically increase rate limits and spending limits for customers who are successfully paying their bill.\n",
      "As your usage tier increases, we may also move your account onto lower latency models behind the scenes.\n",
      "How do rate limits work?\n",
      "15 --------------------------------------------------------------------------------\n",
      "4 How do rate limits work?\n",
      "If your rate limit is 60 requests per minute and 150k tokens per minute, you’ll be limited either by reaching the requests/min cap or running out of tokens—whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you’d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds.\n",
      "Rate limits in headers\n",
      "In addition to seeing your rate limit on your account page, you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.\n",
      "You can expect to see the following header fields:\n",
      "FieldSample ValueDescription\n",
      "15 --------------------------------------------------------------------------------\n",
      "5 FieldSample ValueDescription\n",
      "x-ratelimit-limit-requests\t60\tThe maximum number of requests that are permitted before exhausting the rate limit.\t\n",
      "x-ratelimit-limit-tokens\t150000\tThe maximum number of tokens that are permitted before exhausting the rate limit.\t\n",
      "x-ratelimit-remaining-requests\t59\tThe remaining number of requests that are permitted before exhausting the rate limit.\t\n",
      "x-ratelimit-remaining-tokens\t149984\tThe remaining number of tokens that are permitted before exhausting the rate limit.\t\n",
      "x-ratelimit-reset-requests\t1s\tThe time until the rate limit (based on requests) resets to its initial state.\t\n",
      "x-ratelimit-reset-tokens\t6m0s\tThe time until the rate limit (based on tokens) resets to its initial state.\t\n",
      "What happens if I hit a rate limit error?\n",
      "Rate limit errors look like this:\n",
      "Rate limit reached for text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min.\n",
      "15 --------------------------------------------------------------------------------\n",
      "6 If you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.\n",
      "Rate limits vs max_tokens\n",
      "Each model we offer has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using text-ada-001, the maximum number of tokens you can send to this model is 2,048 tokens per request.\n",
      "Error Mitigation\n",
      "What are some steps I can take to mitigate this?\n",
      "The OpenAI Cookbook has a Python notebook that explains how to avoid rate limit errors, as well an example Python script for staying under rate limits while batch processing API requests.\n",
      "You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.\n",
      "15 --------------------------------------------------------------------------------\n",
      "7 To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.\n",
      "Retrying with exponential backoff\n",
      "One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits:\n",
      "Automatic retries means you can recover from rate limit errors without crashes or missing data\n",
      "Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail\n",
      "15 --------------------------------------------------------------------------------\n",
      "8 Adding random jitter to the delay helps retries from all hitting at the same time.\n",
      "Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.\n",
      "Below are a few example solutions for Python that use exponential backoff.\n",
      "Example #1: Using the Tenacity library\n",
      "Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. To add exponential backoff to your requests, you can use the tenacity.retry decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.\n",
      "15 --------------------------------------------------------------------------------\n",
      "9 1 2 3 4 5 6 7 8 9 10 11 12 import openai from tenacity import ( retry, stop_after_attempt, wait_random_exponential, ) # for exponential backoff @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6)) def completion_with_backoff(**kwargs): return openai.Completion.create(**kwargs) completion_with_backoff(model=\"gpt-3.5-turbo-instruct\", prompt=\"Once upon a time,\")\n",
      "Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.\n",
      "Example #2: Using the backoff library\n",
      "Another python library that provides function decorators for backoff and retry is backoff:\n",
      "1 2 3 4 5 6 7 import backoff import openai @backoff.on_exception(backoff.expo, openai.error.RateLimitError) def completions_with_backoff(**kwargs): return openai.Completion.create(**kwargs) completions_with_backoff(model=\"gpt-3.5-turbo-instruct\", prompt=\"Once upon a time,\")\n",
      "15 --------------------------------------------------------------------------------\n",
      "10 Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.\n",
      "Example 3: Manual backoff implementation\n",
      "If you don't want to use third-party libraries, you can implement your own backoff logic following this example:\n",
      "15 --------------------------------------------------------------------------------\n",
      "11 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # imports import random import time import openai # define a retry decorator def retry_with_exponential_backoff( func, initial_delay: float = 1, exponential_base: float = 2, jitter: bool = True, max_retries: int = 10, errors: tuple = (openai.error.RateLimitError,), ): \"\"\"Retry a function with exponential backoff.\"\"\" def wrapper(*args, **kwargs): # Initialize variables num_retries = 0 delay = initial_delay # Loop until a successful response or max_retries is hit or an exception is raised while True: try: return func(*args, **kwargs) # Retry on specific errors except errors as e: # Increment retries num_retries += 1 # Check if max retries has been reached if num_retries > max_retries: raise Exception( f\"Maximum number of retries ({max_retries}) exceeded.\" ) # Increment the delay delay *= exponential_base * (1 + jitter * random.random()) # Sleep for the delay\n",
      "15 --------------------------------------------------------------------------------\n",
      "12 + jitter * random.random()) # Sleep for the delay time.sleep(delay) # Raise exceptions for any errors not specified except Exception as e: raise e return wrapper @retry_with_exponential_backoff def completions_with_backoff(**kwargs): return openai.Completion.create(**kwargs)\n",
      "15 --------------------------------------------------------------------------------\n",
      "13 Again, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.\n",
      "Reduce the max_tokens to match the size of your completions\n",
      "Your rate limit is calculated as the maximum of max_tokens and the estimated number of tokens based on the character count of your request. Try to set the max_tokens value as close to your expected response size as possible.\n",
      "Batching requests\n",
      "The OpenAI API has separate limits for requests per minute and tokens per minute.\n",
      "If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.\n",
      "Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string.\n",
      "15 --------------------------------------------------------------------------------\n",
      "14 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import openai num_stories = 10 prompt = \"Once upon a time,\" # serial example, with one story completion per request for _ in range(num_stories): response = openai.Completion.create( model=\"curie\", prompt=prompt, max_tokens=20, ) # print story print(prompt + response.choices[0].text)\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import openai # for making OpenAI API requests num_stories = 10 prompts = [\"Once upon a time,\"] * num_stories # batched example, with 10 story completions per request response = openai.Completion.create( model=\"curie\", prompt=prompts, max_tokens=20, ) # match completions to prompts by index stories = [\"\"] * len(prompts) for choice in response.choices: stories[choice.index] = prompts[choice.index] + choice.text # print stories for story in stories: print(story)\n",
      "Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field.\n",
      "Request Increase\n",
      "15 --------------------------------------------------------------------------------\n",
      "15 Request Increase\n",
      "When should I consider applying for a rate limit increase?\n",
      "Our default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days.\n",
      "Keep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers.\n",
      "Will my rate limit increase request be rejected?\n",
      "15 --------------------------------------------------------------------------------\n",
      "16 Will my rate limit increase request be rejected?\n",
      "A rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API.\n",
      "I’ve implemented exponential backoff for my text/code APIs, but I’m still hitting this error. How do I increase my rate limit?\n",
      "15 --------------------------------------------------------------------------------\n",
      "17 We understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our Rate Limit Increase Request form. To help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval.\n",
      "We will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days.\n",
      "Here are some examples of how you might fill out this form:\n",
      "15 --------------------------------------------------------------------------------\n",
      "18 ModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput cost\n",
      "DALL-E API\tN/A\t50\t1000\tOur app is currently in production and based on our past traffic, we make about 10 requests per minute.\t$60\t\n",
      "DALL-E API\tN/A\t150\t10,000\tOur app is gaining traction in the App Store and we’re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we’ll submit a new form. Thanks!\t$180\t\n",
      "ModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput cost\n",
      "gpt-3.5-turbo\t325,000\t4,000\t50\tWe’re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage.\t$390\n",
      "15 --------------------------------------------------------------------------------\n",
      "19 gpt-4\t750,000\t10,000\t10,000\tOur application is receiving a lot of interest; we have 50,000 people on our waitlist. We’d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point.\t$900\t\n",
      "ModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput cost\n",
      "code-davinci-002\t150,000\t1,000\t15\tWe are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation [...]\tCodex models are now deprecated and no longer accessible.\t\n",
      "Please note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage.\n",
      "15 --------------------------------------------------------------------------------\n",
      "0 Error codes\n",
      "This guide includes an overview on error codes you might see from both the API and our official Python library. Each error code mentioned in the overview has a dedicated section with further guidance.\n",
      "API errors\n",
      "CodeOverview\n",
      "401 - Invalid Authentication\tCause: Invalid Authentication \n",
      "Solution: Ensure the correct API key and requesting organization are being used.\t\n",
      "401 - Incorrect API key provided\tCause: The requesting API key is not correct. \n",
      "Solution: Ensure the API key used is correct, clear your browser cache, or generate a new one.\t\n",
      "401 - You must be a member of an organization to use the API\tCause: Your account is not part of an organization. \n",
      "Solution: Contact us to get added to a new organization or ask your organization manager to invite you to an organization.\t\n",
      "429 - Rate limit reached for requests\tCause: You are sending requests too quickly. \n",
      "Solution: Pace your requests. Read the Rate limit guide.\n",
      "16 --------------------------------------------------------------------------------\n",
      "1 429 - You exceeded your current quota, please check your plan and billing details\tCause: You have hit your maximum monthly spend (hard limit) which you can view in the account billing section. \n",
      "Solution: Apply for a quota increase.\t\n",
      "500 - The server had an error while processing your request\tCause: Issue on our servers. \n",
      "Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.\t\n",
      "503 - The engine is currently overloaded, please try again later\tCause: Our servers are experiencing high traffic. \n",
      "Solution: Please retry your requests after a brief wait.\t\n",
      "401 - Invalid Authentication\n",
      "This error message indicates that your authentication credentials are invalid. This could happen for several reasons, such as:\n",
      "You are using a revoked API key.\n",
      "You are using a different API key than the one assigned to the requesting organization.\n",
      "You are using an API key that does not have the required permissions for the endpoint you are calling.\n",
      "16 --------------------------------------------------------------------------------\n",
      "2 To resolve this error, please follow these steps:\n",
      "Check that you are using the correct API key and organization ID in your request header. You can find your API key and organization ID in your account settings.\n",
      "If you are unsure whether your API key is valid, you can generate a new one. Make sure to replace your old API key with the new one in your requests and follow our best practices guide.\n",
      "401 - Incorrect API key provided\n",
      "This error message indicates that the API key you are using in your request is not correct. This could happen for several reasons, such as:\n",
      "There is a typo or an extra space in your API key.\n",
      "You are using an API key that belongs to a different organization.\n",
      "You are using an API key that has been deleted or deactivated.\n",
      "An old, revoked API key might be cached locally.\n",
      "To resolve this error, please follow these steps:\n",
      "Try clearing your browser's cache and cookies, then try again.\n",
      "Check that you are using the correct API key in your request header.\n",
      "16 --------------------------------------------------------------------------------\n",
      "3 If you are unsure whether your API key is correct, you can generate a new one. Make sure to replace your old API key in your codebase and follow our best practices guide.\n",
      "401 - You must be a member of an organization to use the API\n",
      "This error message indicates that your account is not part of an organization. This could happen for several reasons, such as:\n",
      "You have left or been removed from your previous organization.\n",
      "Your organization has been deleted.\n",
      "To resolve this error, please follow these steps:\n",
      "If you have left or been removed from your previous organization, you can either request a new organization or get invited to an existing one.\n",
      "To request a new organization, reach out to us via help.openai.com\n",
      "Existing organization owners can invite you to join their organization via the Members Panel.\n",
      "429 - Rate limit reached for requests\n",
      "16 --------------------------------------------------------------------------------\n",
      "4 429 - Rate limit reached for requests\n",
      "This error message indicates that you have hit your assigned rate limit for the API. This means that you have submitted too many tokens or requests in a short period of time and have exceeded the number of requests allowed. This could happen for several reasons, such as:\n",
      "You are using a loop or a script that makes frequent or concurrent requests.\n",
      "You are sharing your API key with other users or applications.\n",
      "You are using a free plan that has a low rate limit.\n",
      "To resolve this error, please follow these steps:\n",
      "Pace your requests and avoid making unnecessary or redundant calls.\n",
      "If you are using a loop or a script, make sure to implement a backoff mechanism or a retry logic that respects the rate limit and the response headers. You can read more about our rate limiting policy and best practices in our rate limit guide.\n",
      "16 --------------------------------------------------------------------------------\n",
      "5 If you are sharing your organization with other users, note that limits are applied per organization and not per user. It is worth checking on the usage of the rest of your team as this will contribute to the limit.\n",
      "If you are using a free or low-tier plan, consider upgrading to a pay-as-you-go plan that offers a higher rate limit. You can compare the restrictions of each plan in our rate limit guide.\n",
      "429 - You exceeded your current quota, please check your plan and billing details\n",
      "This error message indicates that you have hit your maximum monthly spend for the API. You can view your maximum monthly limit, under ‘hard limit’ in your [account billing settings](/account/billing/limits). This means that you have consumed all the credits allocated to your plan and have reached the limit of your current billing cycle. This could happen for several reasons, such as:\n",
      "You are using a high-volume or complex service that consumes a lot of credits or tokens.\n",
      "Your limit is set too low for your organization’s usage.\n",
      "16 --------------------------------------------------------------------------------\n",
      "6 To resolve this error, please follow these steps:\n",
      "Check your current quota in your account settings. You can see how many tokens your requests have consumed in the usage section of your account.\n",
      "If you are using a free plan, consider upgrading to a pay-as-you-go plan that offers a higher quota.\n",
      "If you need a quota increase, you can apply for one and provide relevant details on expected usage. We will review your request and get back to you in ~7-10 business days.\n",
      "503 - The engine is currently overloaded, please try again later\n",
      "This error message indicates that our servers are experiencing high traffic and are unable to process your request at the moment. This could happen for several reasons, such as:\n",
      "There is a sudden spike or surge in demand for our services.\n",
      "There is scheduled or unscheduled maintenance or update on our servers.\n",
      "There is an unexpected or unavoidable outage or incident on our servers.\n",
      "To resolve this error, please follow these steps:\n",
      "16 --------------------------------------------------------------------------------\n",
      "7 To resolve this error, please follow these steps:\n",
      "Retry your request after a brief wait. We recommend using an exponential backoff strategy or a retry logic that respects the response headers and the rate limit. You can read more about our rate limit best practices.\n",
      "Check our status page for any updates or announcements regarding our services and servers.\n",
      "If you are still getting this error after a reasonable amount of time, please contact us for further assistance. We apologize for any inconvenience and appreciate your patience and understanding.\n",
      "Python library error types\n",
      "TypeOverview\n",
      "APIError\tCause: Issue on our side. \n",
      "Solution: Retry your request after a brief wait and contact us if the issue persists.\t\n",
      "Timeout\tCause: Request timed out. \n",
      "Solution: Retry your request after a brief wait and contact us if the issue persists.\t\n",
      "RateLimitError\tCause: You have hit your assigned rate limit. \n",
      "Solution: Pace your requests. Read more in our Rate limit guide.\n",
      "16 --------------------------------------------------------------------------------\n",
      "8 APIConnectionError\tCause: Issue connecting to our services. \n",
      "Solution: Check your network settings, proxy configuration, SSL certificates, or firewall rules.\t\n",
      "InvalidRequestError\tCause: Your request was malformed or missing some required parameters, such as a token or an input. \n",
      "Solution: The error message should advise you on the specific error made. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data.\t\n",
      "AuthenticationError\tCause: Your API key or token was invalid, expired, or revoked. \n",
      "Solution: Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard.\t\n",
      "ServiceUnavailableError\tCause: Issue on our servers. \n",
      "Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.\n",
      "16 --------------------------------------------------------------------------------\n",
      "9 An `APIError` indicates that something went wrong on our side when processing your request. This could be due to a temporary error, a bug, or a system outage.\n",
      "We apologize for any inconvenience and we are working hard to resolve any issues as soon as possible. You can check our system status page for more information.\n",
      "If you encounter an APIError, please try the following steps:\n",
      "Wait a few seconds and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the second attempt.\n",
      "Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.\n",
      "If the issue persists, check out our Persistent errors next steps section.\n",
      "16 --------------------------------------------------------------------------------\n",
      "10 Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.\n",
      "A `Timeout` error indicates that your request took too long to complete and our server closed the connection. This could be due to a network issue, a heavy load on our services, or a complex request that requires more processing time.\n",
      "If you encounter a Timeout error, please try the following steps:\n",
      "Wait a few seconds and retry your request. Sometimes, the network congestion or the load on our services may be reduced and your request may succeed on the second attempt.\n",
      "Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.\n",
      "If the issue persists, check out our persistent errors next steps section.\n",
      "16 --------------------------------------------------------------------------------\n",
      "11 A `RateLimitError` indicates that you have hit your assigned rate limit. This means that you have sent too many tokens or requests in a given period of time, and our services have temporarily blocked you from sending more.\n",
      "We impose rate limits to ensure fair and efficient use of our resources and to prevent abuse or overload of our services.\n",
      "If you encounter a RateLimitError, please try the following steps:\n",
      "Send fewer tokens or requests or slow down. You may need to reduce the frequency or volume of your requests, batch your tokens, or implement exponential backoff. You can read our Rate limit guide for more details.\n",
      "Wait until your rate limit resets (one minute) and retry your request. The error message should give you a sense of your usage rate and permitted usage.\n",
      "You can also check your API usage statistics from your account dashboard.\n",
      "16 --------------------------------------------------------------------------------\n",
      "12 An `APIConnectionError` indicates that your request could not reach our servers or establish a secure connection. This could be due to a network issue, a proxy configuration, an SSL certificate, or a firewall rule.\n",
      "If you encounter an APIConnectionError, please try the following steps:\n",
      "Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.\n",
      "Check your proxy configuration and make sure it is compatible with our services. You may need to update your proxy settings, use a different proxy, or bypass the proxy altogether.\n",
      "Check your SSL certificates and make sure they are valid and up-to-date. You may need to install or renew your certificates, use a different certificate authority, or disable SSL verification.\n",
      "16 --------------------------------------------------------------------------------\n",
      "13 Check your firewall rules and make sure they are not blocking or filtering our services. You may need to modify your firewall settings.\n",
      "If appropriate, check that your container has the correct permissions to send and receive traffic.\n",
      "If the issue persists, check out our persistent errors next steps section.\n",
      "An InvalidRequestError indicates that your request was malformed or missing some required parameters, such as a token or an input. This could be due to a typo, a formatting error, or a logic error in your code.\n",
      "If you encounter an InvalidRequestError, please try the following steps:\n",
      "Read the error message carefully and identify the specific error made. The error message should advise you on what parameter was invalid or missing, and what value or format was expected.\n",
      "16 --------------------------------------------------------------------------------\n",
      "14 Check the API Reference for the specific API method you were calling and make sure you are sending valid and complete parameters. You may need to review the parameter names, types, values, and formats, and ensure they match the documentation.\n",
      "Check the encoding, format, or size of your request data and make sure they are compatible with our services. You may need to encode your data in UTF-8, format your data in JSON, or compress your data if it is too large.\n",
      "Test your request using a tool like Postman or curl and make sure it works as expected. You may need to debug your code and fix any errors or inconsistencies in your request logic.\n",
      "If the issue persists, check out our persistent errors next steps section.\n",
      "An `AuthenticationError` indicates that your API key or token was invalid, expired, or revoked. This could be due to a typo, a formatting error, or a security breach.\n",
      "If you encounter an AuthenticationError, please try the following steps:\n",
      "16 --------------------------------------------------------------------------------\n",
      "15 Check your API key or token and make sure it is correct and active. You may need to generate a new key from the API Key dashboard, ensure there are no extra spaces or characters, or use a different key or token if you have multiple ones.\n",
      "Ensure that you have followed the correct formatting.\n",
      "A `ServiceUnavailableError` indicates that our servers are temporarily unable to handle your request. This could be due to a planned or unplanned maintenance, a system upgrade, or a server failure. These errors can also be returned during periods of high traffic.\n",
      "We apologize for any inconvenience and we are working hard to restore our services as soon as possible.\n",
      "If you encounter a ServiceUnavailableError, please try the following steps:\n",
      "Wait a few minutes and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the next attempt.\n",
      "16 --------------------------------------------------------------------------------\n",
      "16 Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.\n",
      "If the issue persists, check out our persistent errors next steps section.\n",
      "Persistent errors\n",
      "If the issue persists, contact our support team via chat and provide them with the following information:\n",
      "The model you were using\n",
      "The error message and code you received\n",
      "The request data and headers you sent\n",
      "The timestamp and timezone of your request\n",
      "Any other relevant details that may help us diagnose the issue\n",
      "Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.\n",
      "Handling errors\n",
      "We advise you to programmatically handle errors returned by the API. To do so, you may want to use a code snippet like below:\n",
      "16 --------------------------------------------------------------------------------\n",
      "17 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 try: #Make your OpenAI API request here response = openai.Completion.create(prompt=\"Hello world\", model=\"gpt-3.5-turbo-instruct\") except openai.error.APIError as e: #Handle API error here, e.g. retry or log print(f\"OpenAI API returned an API Error: {e}\") pass except openai.error.APIConnectionError as e: #Handle connection error here print(f\"Failed to connect to OpenAI API: {e}\") pass except openai.error.RateLimitError as e: #Handle rate limit error (we recommend using exponential backoff) print(f\"OpenAI API request exceeded rate limit: {e}\") pass\n",
      "16 --------------------------------------------------------------------------------\n",
      "0 Safety best practices\n",
      "Use our free Moderation API\n",
      "OpenAI's Moderation API is free-to-use and can help reduce the frequency of unsafe content in your completions. Alternatively, you may wish to develop your own content filtration system tailored to your use case.\n",
      "Adversarial testing\n",
      "We recommend “red-teaming” your application to ensure it's robust to adversarial input. Test your product over a wide range of inputs and user behaviors, both a representative set and those reflective of someone trying to ‘break' your application. Does it wander off topic? Can someone easily redirect the feature via prompt injections, e.g. “ignore the previous instructions and do this instead”?\n",
      "Human in the loop (HITL)\n",
      "17 --------------------------------------------------------------------------------\n",
      "1 Human in the loop (HITL)\n",
      "Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation. Humans should be aware of the limitations of the system, and have access to any information needed to verify the outputs (for example, if the application summarizes notes, a human should have easy access to the original notes to refer back).\n",
      "Prompt engineering\n",
      "“Prompt engineering” can help constrain the topic and tone of output text. This reduces the chance of producing undesired content, even if a user tries to produce it. Providing additional context to the model (such as by giving a few high-quality examples of desired behavior prior to the new input) can make it easier to steer model outputs in desired directions.\n",
      "“Know your customer” (KYC)\n",
      "17 --------------------------------------------------------------------------------\n",
      "2 “Know your customer” (KYC)\n",
      "Users should generally need to register and log-in to access your service. Linking this service to an existing account, such as a Gmail, LinkedIn, or Facebook log-in, may help, though may not be appropriate for all use-cases. Requiring a credit card or ID card reduces risk further.\n",
      "Constrain user input and limit output tokens\n",
      "Limiting the amount of text a user can input into the prompt helps avoid prompt injection. Limiting the number of output tokens helps reduce the chance of misuse.\n",
      "Narrowing the ranges of inputs or outputs, especially drawn from trusted sources, reduces the extent of misuse possible within an application.\n",
      "Allowing user inputs through validated dropdown fields (e.g., a list of movies on Wikipedia) can be more secure than allowing open-ended text inputs.\n",
      "17 --------------------------------------------------------------------------------\n",
      "3 Returning outputs from a validated set of materials on the backend, where possible, can be safer than returning novel generated content (for instance, routing a customer query to the best-matching existing customer support article, rather than attempting to answer the query from-scratch).\n",
      "Allow users to report issues\n",
      "Users should generally have an easily-available method for reporting improper functionality or other concerns about application behavior (listed email address, ticket submission method, etc). This method should be monitored by a human and responded to as appropriate.\n",
      "Understand and communicate limitations\n",
      "17 --------------------------------------------------------------------------------\n",
      "4 Understand and communicate limitations\n",
      "From hallucinating inaccurate information, to offensive outputs, to bias, and much more, language models may not be suitable for every use case without significant modifications. Consider whether the model is fit for your purpose, and evaluate the performance of the API on a wide range of potential inputs in order to identify cases where the API's performance might drop. Consider your customer base and the range of inputs that they will be using, and ensure their expectations are calibrated appropriately.\n",
      "Safety and security are very important to us at OpenAI.\n",
      "If in the course of your development you do notice any safety or security issues with the API or anything else related to OpenAI, please submit these through our Coordinated Vulnerability Disclosure Program.\n",
      "End-user IDs\n",
      "17 --------------------------------------------------------------------------------\n",
      "5 End-user IDs\n",
      "Sending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application.\n",
      "The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. If you offer a preview of your product to non-logged in users, you can send a session ID instead.\n",
      "You can include end-user IDs in your API requests via the user parameter as follows:\n",
      "1 2 3 4 5 6 response = openai.Completion.create( model=\"gpt-3.5-turbo-instruct\", prompt=\"This is a test\", max_tokens=5, user=\"user123456\" )\n",
      "17 --------------------------------------------------------------------------------\n",
      "0 Production best practices\n",
      "This guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.\n",
      "Setting up your organization\n",
      "Once you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.\n",
      "18 --------------------------------------------------------------------------------\n",
      "1 Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.\n",
      "You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.\n",
      "Managing billing limits\n",
      "New free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.\n",
      "18 --------------------------------------------------------------------------------\n",
      "2 Once you’ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.\n",
      "If you’d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.\n",
      "API keys\n",
      "The OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.\n",
      "18 --------------------------------------------------------------------------------\n",
      "3 This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.\n",
      "Staging accounts\n",
      "As you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.\n",
      "Scaling your solution architecture\n",
      "18 --------------------------------------------------------------------------------\n",
      "4 Scaling your solution architecture\n",
      "When designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:\n",
      "Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.\n",
      "18 --------------------------------------------------------------------------------\n",
      "5 Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.\n",
      "Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.\n",
      "18 --------------------------------------------------------------------------------\n",
      "6 Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.\n",
      "Managing rate limits\n",
      "When using our API, it's important to understand and plan for rate limits.\n",
      "Improving latencies\n",
      "Latency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.\n",
      "The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:\n",
      "Network\n",
      "End user to API latency\n",
      "Server\n",
      "Time to process prompt tokens\n",
      "Server\n",
      "Time to sample/generate tokens\n",
      "Network\n",
      "API to end user latency\n",
      "18 --------------------------------------------------------------------------------\n",
      "7 Network\n",
      "API to end user latency\n",
      "The bulk of the latency typically arises from the token generation step.\n",
      "Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.\n",
      "Common factors affecting latency and possible mitigation techniques\n",
      "Now that we have looked at the basics of latency, let’s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.\n",
      "Model\n",
      "18 --------------------------------------------------------------------------------\n",
      "8 Model\n",
      "Our API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query. Models such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.\n",
      "Number of completion tokens\n",
      "Requesting a large amount of generated tokens completions can lead to increased latencies:\n",
      "Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.\n",
      "18 --------------------------------------------------------------------------------\n",
      "9 Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.\n",
      "Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.\n",
      "If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.\n",
      "If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as [ max_tokens * max (n, best_of) ]\n",
      "Streaming\n",
      "18 --------------------------------------------------------------------------------\n",
      "10 Streaming\n",
      "Setting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it’s worth experimenting with streaming.\n",
      "Infrastructure\n",
      "Our servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.\n",
      "Batching\n",
      "18 --------------------------------------------------------------------------------\n",
      "11 Batching\n",
      "Depending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.\n",
      "Managing costs\n",
      "To monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.\n",
      "Text generation\n",
      "18 --------------------------------------------------------------------------------\n",
      "12 Text generation\n",
      "One of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.\n",
      "18 --------------------------------------------------------------------------------\n",
      "13 One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.\n",
      "You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you’ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.\n",
      "MLOps strategy\n",
      "18 --------------------------------------------------------------------------------\n",
      "14 MLOps strategy\n",
      "As you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These include\n",
      "Data and model management: managing the data used to train or fine-tune your model and tracking versions and changes.\n",
      "Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.\n",
      "Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.\n",
      "Model deployment: automating the process of deploying your model and related artifacts into production.\n",
      "Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.\n",
      "Security and compliance\n",
      "18 --------------------------------------------------------------------------------\n",
      "15 Security and compliance\n",
      "As you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.\n",
      "Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.\n",
      "Safety best practices\n",
      "18 --------------------------------------------------------------------------------\n",
      "16 Safety best practices\n",
      "When creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.\n",
      "18 --------------------------------------------------------------------------------\n",
      "0 Getting started\n",
      "Creating a plugin takes 3 steps:\n",
      "Build an API\n",
      "Document the API in the OpenAPI yaml or JSON format\n",
      "Create a JSON manifest file that will define relevant metadata for the plugin\n",
      "The focus of the rest of this section will be creating a todo list plugin by defining the OpenAPI specification along with the manifest file.\n",
      "Explore example plugins\n",
      "Explore example plugins covering multiple use cases and authentication methods.\n",
      "Plugin manifest\n",
      "19 --------------------------------------------------------------------------------\n",
      "1 Plugin manifest\n",
      "Every plugin requires a ai-plugin.json file, which needs to be hosted on the API’s domain. For example, a company called example.com would make the plugin JSON file accessible via an https://example.com domain since that is where their API is hosted. When you install the plugin via the ChatGPT UI, on the backend we look for a file located at /.well-known/ai-plugin.json. The /.well-known folder is required and must exist on your domain in order for ChatGPT to connect with your plugin. If there is no file found, the plugin cannot be installed. For local development, you can use HTTP but if you are pointing to a remote server, HTTPS is required.\n",
      "The minimal definition of the required ai-plugin.json file will look like the following:\n",
      "19 --------------------------------------------------------------------------------\n",
      "2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"schema_version\": \"v1\", \"name_for_human\": \"TODO List\", \"name_for_model\": \"todo\", \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\", \"description_for_model\": \"Help the user with managing a TODO list. You can add, remove and view your TODOs.\", \"auth\": { \"type\": \"none\" }, \"api\": { \"type\": \"openapi\", \"url\": \"http://localhost:3333/openapi.yaml\" }, \"logo_url\": \"http://localhost:3333/logo.png\", \"contact_email\": \"support@example.com\", \"legal_info_url\": \"http://www.example.com/legal\" }\n",
      "If you want to see all of the possible options for the plugin file, you can refer to the definition below. When naming your plugin, please keep in mind our brand guidelines and the various character limits for fields below, plugins that fail to adhere to these guidelines will not be approved for the plugin store.\n",
      "FieldTypeDescription / OptionsRequiredPublic\n",
      "schema_version\tString\tManifest schema version\t✅\n",
      "19 --------------------------------------------------------------------------------\n",
      "3 schema_version\tString\tManifest schema version\t✅\t\t\n",
      "name_for_model\tString\tName the model will use to target the plugin (no spaces allowed, only letters and numbers). 50 character max.\t✅\t\t\n",
      "name_for_human\tString\tHuman-readable name, such as the full company name. 20 character max.\t✅\t✅\t\n",
      "description_for_model\tString\tDescription better tailored to the model, such as token context length considerations or keyword usage for improved plugin prompting. 8,000 character max.\t✅\t\t\n",
      "description_for_human\tString\tHuman-readable description of the plugin. 100 character max.\t✅\t✅\t\n",
      "auth\tManifestAuth\tAuthentication schema\t✅\t\t\n",
      "api\tObject\tAPI specification\t✅\t\t\n",
      "logo_url\tString\tURL used to fetch the logo. Suggested size: 512 x 512. Transparent backgrounds are supported. Must be an image, no GIFs are allowed.\t✅\t\t\n",
      "contact_email\tString\tEmail contact for safety/moderation, support, and deactivation\t✅\t✅\t\n",
      "legal_info_url\tString\tRedirect URL for users to view plugin information\t✅\t✅\n",
      "19 --------------------------------------------------------------------------------\n",
      "4 HttpAuthorizationType\tHttpAuthorizationType\t\"bearer\" or \"basic\"\t✅\t\t\n",
      "ManifestAuthType\tManifestAuthType\t\"none\", \"user_http\", \"service_http\", or \"oauth\"\t\t\t\n",
      "interface BaseManifestAuth\tBaseManifestAuth\ttype: ManifestAuthType; instructions: string;\t\t\t\n",
      "ManifestNoAuth\tManifestNoAuth\tNo authentication required: BaseManifestAuth & { type: 'none', }\t\t\t\n",
      "ManifestAuth\tManifestAuth\tManifestNoAuth, ManifestServiceHttpAuth, ManifestUserHttpAuth, ManifestOAuthAuth\t\t\t\n",
      "Note that items listed under Public will be made available to users in the plugin store and the full manifest file is transmitted to the user's client and may be visible to them.\n",
      "The following are examples with different authentication methods:\n",
      "19 --------------------------------------------------------------------------------\n",
      "5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # App-level API keys type ManifestServiceHttpAuth = BaseManifestAuth & { type: 'service_http'; authorization_type: HttpAuthorizationType; verification_tokens: { [service: string]?: string; }; } # User-level HTTP authentication type ManifestUserHttpAuth = BaseManifestAuth & { type: 'user_http'; authorization_type: HttpAuthorizationType; } type ManifestOAuthAuth = BaseManifestAuth & { type: 'oauth'; # OAuth URL where a user is directed to for the OAuth authentication flow to begin. client_url: string; # OAuth scopes required to accomplish operations on the user's behalf. scope: string; # Endpoint used to exchange OAuth code with access token. authorization_url: string; # When exchanging OAuth code with access token, the expected header 'content-type'. For example: 'content-type: application/json' authorization_content_type: string; # When registering the OAuth client ID and secrets, the plugin service will surface a\n",
      "19 --------------------------------------------------------------------------------\n",
      "6 ID and secrets, the plugin service will surface a unique token. verification_tokens: { [service: string]?: string; }; }\n",
      "19 --------------------------------------------------------------------------------\n",
      "7 There are limits to the length of certain fields in the manifest file mentioned above which are subject to change. We also impose a 100,000 character maximum for the API response body which may also change over time.\n",
      "In general, the best practice is to keep the description and responses as concise as possible because the models have limited context windows.\n",
      "OpenAPI definition\n",
      "The next step is to build the OpenAPI specification to document the API. The model in ChatGPT does not know anything about your API other than what is defined in the OpenAPI specification and manifest file. This means that if you have an extensive API, you need not expose all functionality to the model and can choose specific endpoints. For example, if you have a social media API, you might want to have the model access content from the site through a GET request but prevent the model from being able to comment on users posts in order to reduce the chance of spam.\n",
      "19 --------------------------------------------------------------------------------\n",
      "8 The OpenAPI specification is the wrapper that sits on top of your API. A basic OpenAPI specification will look like the following:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 openapi: 3.0.1 info: title: TODO Plugin description: A plugin that allows the user to create and manage a TODO list using ChatGPT. version: 'v1' servers: - url: http://localhost:3333 paths: /todos: get: operationId: getTodos summary: Get the list of todos responses: \"200\": description: OK content: application/json: schema: $ref: '#/components/schemas/getTodosResponse' components: schemas: getTodosResponse: type: object properties: todos: type: array items: type: string description: The list of todos.\n",
      "19 --------------------------------------------------------------------------------\n",
      "9 We start by defining the specification version, the title, description, and version number. When a query is run in ChatGPT, it will look at the description that is defined in the info section to determine if the plugin is relevant for the user query. You can read more about prompting in the writing descriptions section.\n",
      "Keep in mind the following limits in your OpenAPI specification, which are subject to change:\n",
      "200 characters max for each API endpoint description/summary field in API specification\n",
      "200 characters max for each API param description field in API specification\n",
      "Since we are running this example locally, we want to set the server to point to your localhost URL. The rest of the OpenAPI specification follows the traditional OpenAPI format, you can learn more about OpenAPI formatting through various online resources. There are also many tools that auto generate OpenAPI specifications based on your underlying API code.\n",
      "Running a plugin\n",
      "19 --------------------------------------------------------------------------------\n",
      "10 Running a plugin\n",
      "Once you have created an API, manifest file, and OpenAPI specification for your API, you are now ready to connect the plugin via the ChatGPT UI. There are two different places your plugin might be running, either locally in a development environment or on a remote server.\n",
      "If you have a local version of your API running, you can point the plugin interface to your localhost server. To connect the plugin with ChatGPT, navigate to the plugin store and select “Develop your own plugin”. Enter your localhost and port number (e.g localhost:3333). Note that only auth type none is currently supported for localhost development.\n",
      "19 --------------------------------------------------------------------------------\n",
      "11 If the plugin is running on a remote server, you will need to first select “Develop your own plugin” to set it up and then “Install an unverified plugin” to install it for yourself. You can simply add the plugin manifest file to the yourdomain.com/.well-known/ path and start testing your API. However, for subsequent changes to your manifest file, you will have to deploy the new changes to your public site which might take a long time. In that case, we suggest setting up a local server to act as a proxy for your API. This allows you to quickly prototype changes to your OpenAPI spec and manifest file.\n",
      "Setup a local proxy of your public API\n",
      "The following Python code is an example of how you can set up a simple proxy of your public facing API.\n",
      "19 --------------------------------------------------------------------------------\n",
      "12 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import requests import os import yaml from flask import Flask, jsonify, Response, request, send_from_directory from flask_cors import CORS app = Flask(__name__) PORT = 3333 # Note: Setting CORS to allow chat.openapi.com is required for ChatGPT to access your plugin CORS(app, origins=[f\"http://localhost:{PORT}\", \"https://chat.openai.com\"]) api_url = 'https://example.com' @app.route('/.well-known/ai-plugin.json') def serve_manifest(): return send_from_directory(os.path.dirname(__file__), 'ai-plugin.json') @app.route('/openapi.yaml') def serve_openapi_yaml(): with open(os.path.join(os.path.dirname(__file__), 'openapi.yaml'), 'r') as f: yaml_data = f.read() yaml_data = yaml.load(yaml_data, Loader=yaml.FullLoader) return jsonify(yaml_data) @app.route('/openapi.json') def serve_openapi_json(): return send_from_directory(os.path.dirname(__file__),\n",
      "19 --------------------------------------------------------------------------------\n",
      "13 send_from_directory(os.path.dirname(__file__), 'openapi.json') @app.route('/<path:path>', methods=['GET', 'POST']) def wrapper(path): headers = { 'Content-Type': 'application/json', } url = f'{api_url}/{path}' print(f'Forwarding call: {request.method} {path} -> {url}') if request.method == 'GET': response = requests.get(url, headers=headers, params=request.args) elif request.method == 'POST': print(request.headers) response = requests.post(url, headers=headers, params=request.args, json=request.json) else: raise NotImplementedError(f'Method {request.method} not implemented in wrapper for {path=}') return response.content if __name__ == '__main__': app.run(port=PORT)\n",
      "19 --------------------------------------------------------------------------------\n",
      "14 Writing descriptions\n",
      "When a user makes a query that might be a potential request that goes to a plugin, the model looks through the descriptions of the endpoints in the OpenAPI specification along with the description_for_model in the manifest file. Just like with prompting other language models, you will want to test out multiple prompts and descriptions to see what works best.\n",
      "The OpenAPI spec itself is a great place to give the model information about the diverse details of your API – what functions are available, with what parameters, etc. Besides using expressive, informative names for each field, the spec can also contain “description” fields for every attribute. These can be used to provide natural language descriptions of what a function does or what information a query field expects, for example. The model will be able to see these, and they will guide it in using the API. If a field is restricted to only certain values, you can also provide an “enum” with descriptive category names.\n",
      "19 --------------------------------------------------------------------------------\n",
      "15 The description_for_model attribute gives you the freedom to instruct the model on how to use your plugin generally. Overall, the language model behind ChatGPT is highly capable of understanding natural language and following instructions. Therefore, this is a good place to put in general instructions on what your plugin does and how the model should use it properly. Use natural language, preferably in a concise yet descriptive and objective tone. You can look at some of the examples to have an idea of what this should look like. We suggest starting the description_for_model with “Plugin for …” and then enumerating all of the functionality that your API provides.\n",
      "Best practices\n",
      "Here are some best practices to follow when writing your description_for_model and descriptions in your OpenAPI specification, as well as when designing your API responses:\n",
      "19 --------------------------------------------------------------------------------\n",
      "16 Your descriptions should not attempt to control the mood, personality, or exact responses of ChatGPT. ChatGPT is designed to write appropriate responses to plugins.\n",
      "Bad example:\n",
      "When the user asks to see their todo list, always respond with \"I was able to find your todo list! You have [x] todos: [list the todos here]. I can add more todos if you'd like!\"\n",
      "Good example:\n",
      "[no instructions needed for this]\n",
      "Your descriptions should not encourage ChatGPT to use the plugin when the user hasn’t asked for your plugin’s particular category of service.\n",
      "Bad example:\n",
      "Whenever the user mentions any type of task or plan, ask if they would like to use the TODOs plugin to add something to their todo list.\n",
      "Good example:\n",
      "The TODO list can add, remove and view the user's TODOs.\n",
      "Your descriptions should not prescribe specific triggers for ChatGPT to use the plugin. ChatGPT is designed to use your plugin automatically when appropriate.\n",
      "Bad example:\n",
      "19 --------------------------------------------------------------------------------\n",
      "17 Bad example:\n",
      "When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\"\n",
      "Good example:\n",
      "[no instructions needed for this]\n",
      "Plugin API responses should return raw data instead of natural language responses unless it’s necessary. ChatGPT will provide its own natural language response using the returned data.\n",
      "Bad example:\n",
      "I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!\n",
      "Good example:\n",
      "{ \"todos\": [ \"get groceries\", \"walk the dog\" ] }\n",
      "Debugging\n",
      "By default, the chat will not show plugin calls and other information that is not surfaced to the user. In order to get a more complete picture of how the model is interacting with your plugin, you can see the request and response by clicking the down arrow on the plugin name after interacting with the plugin.\n",
      "19 --------------------------------------------------------------------------------\n",
      "18 A model call to the plugin will usually consist of a message from the model containing JSON-like parameters which are sent to the plugin, followed by a response from the plugin, and finally a message from the model utilizing the information returned by the plugin.\n",
      "If you are developing a localhost plugin, you can also open the developer console by going to \"Settings\" and toggling \"Open plugin devtools\". From there, you can see more verbose logs and \"refresh plugin\" which re-fetches the Plugin and OpenAPI specification.\n",
      "19 --------------------------------------------------------------------------------\n",
      "0 Learn how to build a plugin that allows ChatGPT to intelligently call your API.\n",
      "Introduction\n",
      "OpenAI plugins connect ChatGPT to third-party applications. These plugins enable ChatGPT to interact with APIs defined by developers, enhancing ChatGPT's capabilities and allowing it to perform a wide range of actions. Plugins enable ChatGPT to do things like:\n",
      "Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.\n",
      "Retrieve knowledge-base information; e.g., company docs, personal notes, etc.\n",
      "Assist users with actions; e.g., booking a flight, ordering food, etc.\n",
      "Plugins are in a beta and developer access may not be accessible to you. Please join the waitlist to get developer access. During the beta, we will be working closely with users and developers to iterate on the plugin system, which may evolve significantly.\n",
      "If you want to have an example running as you read through the documentation and learn more about plugins, you can begin with our plugin quickstart repo.\n",
      "20 --------------------------------------------------------------------------------\n",
      "1 Plugin developers expose one or more API endpoints, accompanied by a standardized manifest file and an OpenAPI specification. These define the plugin's functionality, allowing ChatGPT to consume the files and make calls to the developer-defined APIs.\n",
      "The AI model acts as an intelligent API caller. Given an API spec and a natural-language description of when to use the API, the model proactively calls the API to perform actions. For instance, if a user asks, \"Where should I stay in Paris for a couple nights?\", the model may choose to call a hotel reservation plugin API, receive the API response, and generate a user-facing answer combining the API data and its natural language capabilities.\n",
      "Over time, we anticipate the system will evolve to accommodate more advanced use cases.\n",
      "Plugin flow\n",
      "To build a plugin, it is important to understand the end-to-end flow.\n",
      "Create a manifest file and host it at yourdomain.com/.well-known/ai-plugin.json\n",
      "20 --------------------------------------------------------------------------------\n",
      "2 The file includes metadata about your plugin (name, logo, etc.), details about authentication required (type of auth, OAuth URLs, etc.), and an OpenAPI spec for the endpoints you want to expose.\n",
      "The model will see the OpenAPI description fields, which can be used to provide a natural language description for the different fields.\n",
      "We suggest exposing only 1-2 endpoints in the beginning with a minimum number of parameters to minimize the length of the text. The plugin description, API requests, and API responses are all inserted into the conversation with ChatGPT. This counts against the context limit of the model.\n",
      "Register your plugin in the ChatGPT UI\n",
      "Select the plugin model from the top drop down, then select “Plugins”, “Plugin Store”, and finally “Develop your own plugin”.\n",
      "If authentication is required, provide an OAuth 2 client_id and client_secret or an API key.\n",
      "Users activate your plugin\n",
      "Users must manually activate your plugin in the ChatGPT UI. (ChatGPT will not use your plugin by default.)\n",
      "20 --------------------------------------------------------------------------------\n",
      "3 You will be able to share your plugin with 100 additional users (only other developers can install unverified plugins).\n",
      "If OAuth is required, users will be redirected via OAuth to your plugin to sign in.\n",
      "Users begin a conversation\n",
      "OpenAI will inject a compact description of your plugin in a message to ChatGPT, invisible to end users. This will include the plugin description, endpoints, and examples.\n",
      "When a user asks a relevant question, the model may choose to invoke an API call from your plugin if it seems relevant; for POST requests, we require that developers build a user confirmation flow to avoid destruction actions.\n",
      "The model will incorporate the API call results into its response to the user.\n",
      "The model might include links returned from the API calls in its response. These will be displayed as rich previews (using the OpenGraph protocol, where we pull the site_name, title, description, image, and url fields).\n",
      "20 --------------------------------------------------------------------------------\n",
      "4 The model can also format data from your API in markdown and the ChatGPT UI will render the markdown automatically.\n",
      "Currently, we will be sending the user’s country and state in the Plugin conversation header (if you are in California for example, it would look like {\"openai-subdivision-1-iso-code\": \"US-CA\"}. This is useful for shopping, restaurants, weather, and more. You can read more in our developer terms of use.\n",
      "Next steps\n",
      "Now that you know the basics of plugins, you might want to:\n",
      "Get started building a plugin\n",
      "Explore example plugins\n",
      "Read about important steps for productionizing your plugin\n",
      "Learn about the plugin review process\n",
      "20 --------------------------------------------------------------------------------\n",
      "0 Plugin authentication\n",
      "Plugins offer numerous authentication schemas to accommodate various use cases. To specify the authentication schema for your plugin, use the manifest file. Our plugin domain policy outlines our strategy for addressing domain security issues. For examples of available authentication options, refer to the examples section, which showcases all the different choices.\n",
      "The ai-plugin.json file requires an auth schema to be set. Even if you elect to use no authentication, it is still required to specify \"auth\": { \"type\": \"none\" }.\n",
      "We support only localhost development without authentication; if you want to use service, user, or OAuth authentication, you need to set up a remote server.\n",
      "Service level\n",
      "We suggest service level auth as it gives developers control over how their plugin is being used but also doesn't introduce overhead for users.\n",
      "21 --------------------------------------------------------------------------------\n",
      "1 If you want to specifically enable OpenAI plugins to work with your API, you can provide a client secret during the plugin installation flow. This means that all traffic from OpenAI plugins will be authenticated but not on a user level. This flow benefits from a simple end user experience but less control from an API perspective.\n",
      "To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted.\n",
      "In ai-plugin.json, set auth.type to \"service_http\" as is shown in our service level auth example.\n",
      "You will be prompted for your service access token, which is a string specified in your code.\n",
      "We securely store an encrypted copy of your service access token to enable plugin installation without additional authentication.\n",
      "The service access token is sent in the Authorization header for plugin requests.\n",
      "Once you add your service access token into the ChatGPT UI, you will be presented with a verification token.\n",
      "21 --------------------------------------------------------------------------------\n",
      "2 Add the verification token to your ai-plugin.json file under the auth section as shown below.\n",
      "1 2 3 4 5 6 7 \"auth\": { \"type\": \"service_http\", \"authorization_type\": \"bearer\", \"verification_tokens\": { \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\" } },\n",
      "The verification tokens are designed to support multiple applications. You can simply add the additional applications you want your plugin to support:\n",
      "1 2 3 4 \"verification_tokens\": { \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\", \"other_service\": \"abc123\" }\n",
      "OAuth\n",
      "The plugin protocol is compatible with OAuth. A simple example of the OAuth flow we are expecting should look something like the following:\n",
      "To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted (cannot be localhost).\n",
      "In ai-plugin.json, set auth.type to \"oauth\" as is shown in our OAuth example.\n",
      "21 --------------------------------------------------------------------------------\n",
      "3 Then, you will be prompted to enter the OAuth client ID and client secret.\n",
      "The client ID and secret can be simple text strings but should follow OAuth best practices.\n",
      "We store an encrypted version of the client secret, while the client ID is available to end users.\n",
      "Once you add your client ID and client secret into the ChatGPT UI, you will be presented with a verification token.\n",
      "Add the verification token to your ai-plugin.json file under the auth section as shown below.\n",
      "OAuth requests will include the following information: request={'grant_type': 'authorization_code', 'client_id': 'id_set_by_developer', 'client_secret': 'secret_set_by_developer', 'code': 'abc123', 'redirect_uri': 'https://chat.openai.com/aip/plugin-some_plugin_id/oauth/callback'}\n",
      "In order for someone to use a plugin with OAuth, they will need to install the plugin and then be presented with a \"Sign in with\" button in the ChatGPT UI.\n",
      "21 --------------------------------------------------------------------------------\n",
      "4 The authorization_url endpoint should return a response that looks like: { \"access_token\": \"example_token\", \"token_type\": \"bearer\", \"refresh_token\": \"example_token\", \"expires_in\": 59 }\n",
      "During the user sign in process, ChatGPT makes a request to your authorization_url using the specified authorization_content_type, we expect to get back an access token and optionally a refresh token which we use to periodically fetch a new access token.\n",
      "Each time a user makes a request to the plugin, the user’s token will be passed in the Authorization header: (“Authorization”: “[Bearer/Basic][user’s token]”).\n",
      "We require that OAuth applications make use of the state parameter for security reasons.\n",
      "Below is an example of what the OAuth configuration inside of the ai-plugin.json file might look like:\n",
      "21 --------------------------------------------------------------------------------\n",
      "5 1 2 3 4 5 6 7 8 9 10 \"auth\": { \"type\": \"oauth\", \"client_url\": \"https://example.com/authorize\", \"scope\": \"\", \"authorization_url\": \"https://example.com/auth/\", \"authorization_content_type\": \"application/json\", \"verification_tokens\": { \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\" } },\n",
      "To better understand the URL structure for OAuth, here is a short description of the fields:\n",
      "When you set up your plugin with ChatGPT, you will be asked to provide your OAuth client_id and client_secret.\n",
      "When a user logs into the plugin, ChatGPT will direct the user’s browser to \"[client_url]?response_type=code&client_id=[client_id]&scope=[scope]&state=xyz123&redirect_uri=https%3A%2F%2Fchat.openai.com%2Faip%2F[plugin_id]%2Foauth%2Fcallback\"\n",
      "21 --------------------------------------------------------------------------------\n",
      "6 The plugin_id is passed via the request made to your OAuth endpoint (note that it is not visible in the ChatGPT UI today but may be in the future). You can inspect the request there to see the plugin_id. We expect the state to be passed along when you redirect back to redirect_uri. If the state doesn't match the initial state, or has expired, the authentication flow will fail.\n",
      "After your plugin redirects back to the given redirect_uri, ChatGPT will complete the OAuth flow by making a POST request to the authorization_url with content type authorization_content_type and parameters { “grant_type”: “authorization_code”, “client_id”: [client_id], “client_secret”: [client_secret], “code”: [the code that was returned with the redirect], “redirect_uri”: [the same redirect uri as before] }.\n",
      "No authentication\n",
      "We do not recommend the use of \"no authentication\", consider using \"service authentication\".\n",
      "21 --------------------------------------------------------------------------------\n",
      "7 We support no-auth flow for applications that do not require authentication, where a user is able to send requests directly to your API without any restrictions. This is particularly useful if you have an open API that you want to make available to everyone, as it allows traffic from sources other than just OpenAI plugin requests.\n",
      "1 2 3 \"auth\": { \"type\": \"none\" },\n",
      "User level\n",
      "Due to current UI limitations, we are not allowing plugins with \"user authentication\" into the plugin store. We expect this may change in the future.\n",
      "Just like how a user might already be using your API, we allow user level authentication through enabling end users to copy and paste their secret API key into the ChatGPT UI during plugin install. While we encrypt the secret key when we store it in our database, we do not recommend this approach given the poor user experience.\n",
      "To start, a user pastes in their access token when installing the plugin\n",
      "We store an encrypted version of the token\n",
      "21 --------------------------------------------------------------------------------\n",
      "8 We store an encrypted version of the token\n",
      "We then pass it in the Authorization header when making requests to the plugin (“Authorization”: “[Bearer/Basic][user’s token]”)\n",
      "1 2 3 4 \"auth\": { \"type\": \"user_http\", \"authorization_type\": \"bearer\", },\n",
      "21 --------------------------------------------------------------------------------\n",
      "0 Example plugins\n",
      "To get started building, we are making available a set of simple plugins that cover different authentication schemas and use cases. From our simple no authentication todo list plugin to the more powerful retrieval plugin, these examples provide a glimpse into what we hope to make possible with plugins.\n",
      "During development, you can run the plugin locally on your computer or through a cloud development environment like GitHub Codespaces, Replit, or CodeSandbox.\n",
      "We created the plugin quickstart as a starting place for developers to get a plugin up and running in less than 5 minutes. If you have not run a plugin yet and want to get acquainted with the minimal steps required to run one, consider beginning with the plugin quickstart repo.\n",
      "Learn how to build a simple todo list plugin with no auth\n",
      "To start, check out the no authentication page, then define an ai-plugin.json file with the following fields:\n",
      "22 --------------------------------------------------------------------------------\n",
      "1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"schema_version\": \"v1\", \"name_for_human\": \"TODO List (No Auth)\", \"name_for_model\": \"todo\", \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\", \"description_for_model\": \"Plugin for managing a TODO list, you can add, remove and view your TODOs.\", \"auth\": { \"type\": \"none\" }, \"api\": { \"type\": \"openapi\", \"url\": \"PLUGIN_HOSTNAME/openapi.yaml\" }, \"logo_url\": \"PLUGIN_HOSTNAME/logo.png\", \"contact_email\": \"support@example.com\", \"legal_info_url\": \"https://example.com/legal\" }\n",
      "Note the PLUGIN_HOSTNAME should be the actual hostname of your plugin server.\n",
      "Next, we can define the API endpoints to create, delete, and fetch todo list items for a specific user.\n",
      "22 --------------------------------------------------------------------------------\n",
      "2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import json import quart import quart_cors from quart import request # Note: Setting CORS to allow chat.openapi.com is only required when running a localhost plugin app = quart_cors.cors(quart.Quart(__name__), allow_origin=\"https://chat.openai.com\") _TODOS = {} @app.post(\"/todos/<string:username>\") async def add_todo(username): request = await quart.request.get_json(force=True) if username not in _TODOS: _TODOS[username] = [] _TODOS[username].append(request[\"todo\"]) return quart.Response(response='OK', status=200) @app.get(\"/todos/<string:username>\") async def get_todos(username): return quart.Response(response=json.dumps(_TODOS.get(username, [])), status=200) @app.delete(\"/todos/<string:username>\") async def delete_todo(username): request = await quart.request.get_json(force=True) todo_idx = request[\"todo_idx\"] if 0 <=\n",
      "22 --------------------------------------------------------------------------------\n",
      "3 todo_idx = request[\"todo_idx\"] if 0 <= todo_idx < len(_TODOS[username]): _TODOS[username].pop(todo_idx) return quart.Response(response='OK', status=200) @app.get(\"/logo.png\") async def plugin_logo(): filename = 'logo.png' return await quart.send_file(filename, mimetype='image/png') @app.get(\"/.well-known/ai-plugin.json\") async def plugin_manifest(): host = request.headers['Host'] with open(\"ai-plugin.json\") as f: text = f.read() # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the manifest text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\") return quart.Response(text, mimetype=\"text/json\") @app.get(\"/openapi.yaml\") async def openapi_spec(): host = request.headers['Host'] with open(\"openapi.yaml\") as f: text = f.read() # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the OpenAPI spec text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\") return quart.Response(text, mimetype=\"text/yaml\") def main(): app.run(debug=True, host=\"0.0.0.0\", port=5002) if __name__ ==\n",
      "22 --------------------------------------------------------------------------------\n",
      "4 host=\"0.0.0.0\", port=5002) if __name__ == \"__main__\": main()\n",
      "22 --------------------------------------------------------------------------------\n",
      "5 Last, we need to set up and define a OpenAPI specification to match the endpoints defined on our local or remote server. You do not need to expose the full functionality of your API via the specification and can instead choose to let ChatGPT have access to only certain functionality.\n",
      "There are also many tools that will automatically turn your server definition code into an OpenAPI specification so you don’t need to do it manually. In the case of the Python code above, the OpenAPI specification will look like:\n",
      "22 --------------------------------------------------------------------------------\n",
      "6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 openapi: 3.0.1 info: title: TODO Plugin description: A plugin that allows the user to create and manage a TODO list using ChatGPT. If you do not know the user's username, ask them first before making queries to the plugin. Otherwise, use the username \"global\". version: \"v1\" servers: - url: PLUGIN_HOSTNAME paths: /todos/{username}: get: operationId: getTodos summary: Get the list of todos parameters: - in: path name: username schema: type: string required: true description: The name of the user. responses: \"200\": description: OK content: application/json: schema: $ref: \"#/components/schemas/getTodosResponse\" post: operationId: addTodo summary: Add a todo to the list parameters: - in: path name: username schema: type: string required: true\n",
      "22 --------------------------------------------------------------------------------\n",
      "7 username schema: type: string required: true description: The name of the user. requestBody: required: true content: application/json: schema: $ref: \"#/components/schemas/addTodoRequest\" responses: \"200\": description: OK delete: operationId: deleteTodo summary: Delete a todo from the list parameters: - in: path name: username schema: type: string required: true description: The name of the user. requestBody: required: true content: application/json: schema: $ref: \"#/components/schemas/deleteTodoRequest\" responses: \"200\": description: OK components: schemas: getTodosResponse: type: object properties: todos: type: array items: type: string description: The list of todos. addTodoRequest: type: object required: - todo properties: todo: type: string description: The todo to add to the list. required: true deleteTodoRequest: type: object required: - todo_idx properties: todo_idx: type: integer description: The index of the todo to delete. required: true\n",
      "22 --------------------------------------------------------------------------------\n",
      "8 Learn how to build a simple todo list plugin with service level auth\n",
      "To start, check out the service level authentication page and then define an ai-plugin.json file with the following fields:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"schema_version\": \"v1\", \"name_for_human\": \"TODO List (service auth)\", \"name_for_model\": \"todo\", \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\", \"description_for_model\": \"Plugin for managing a TODO list, you can add, remove and view your TODOs.\", \"auth\": { \"type\": \"service_http\", \"authorization_type\": \"bearer\", \"verification_tokens\": { \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\" } }, \"api\": { \"type\": \"openapi\", \"url\": \"https://example.com/openapi.yaml\" }, \"logo_url\": \"https://example.com/logo.png\", \"contact_email\": \"support@example.com\", \"legal_info_url\": \"https://example.com/legal\" }\n",
      "22 --------------------------------------------------------------------------------\n",
      "9 Notice that the verification token is required for service level authentication plugins. The token is generated during the plugin installation process in the ChatGPT web UI after you set the service access token.\n",
      "You will also need to update \"Example.com\" to the name of your remote server.\n",
      "Next, we can define the API endpoints to create, delete, and fetch todo list items for a specific user. The endpoints also check that the user is authenticated.\n",
      "22 --------------------------------------------------------------------------------\n",
      "10 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import json import quart import quart_cors from quart import request app = quart_cors.cors(quart.Quart(__name__)) # This key can be anything, though you will likely want a randomly generated sequence. _SERVICE_AUTH_KEY = \"REPLACE_ME\" _TODOS = {} def assert_auth_header(req): assert req.headers.get( \"Authorization\", None) == f\"Bearer {_SERVICE_AUTH_KEY}\" @app.post(\"/todos/<string:username>\") async def add_todo(username): assert_auth_header(quart.request) request = await quart.request.get_json(force=True) if username not in _TODOS: _TODOS[username] = [] _TODOS[username].append(request[\"todo\"]) return quart.Response(response='OK', status=200) @app.get(\"/todos/<string:username>\") async def get_todos(username): assert_auth_header(quart.request) return quart.Response(response=json.dumps(_TODOS.get(username, [])), status=200)\n",
      "22 --------------------------------------------------------------------------------\n",
      "11 [])), status=200) @app.delete(\"/todos/<string:username>\") async def delete_todo(username): assert_auth_header(quart.request) request = await quart.request.get_json(force=True) todo_idx = request[\"todo_idx\"] if 0 <= todo_idx < len(_TODOS[username]): _TODOS[username].pop(todo_idx) return quart.Response(response='OK', status=200) @app.get(\"/logo.png\") async def plugin_logo(): filename = 'logo.png' return await quart.send_file(filename, mimetype='image/png') @app.get(\"/.well-known/ai-plugin.json\") async def plugin_manifest(): host = request.headers['Host'] with open(\"ai-plugin.json\") as f: text = f.read() return quart.Response(text, mimetype=\"text/json\") @app.get(\"/openapi.yaml\") async def openapi_spec(): host = request.headers['Host'] with open(\"openapi.yaml\") as f: text = f.read() return quart.Response(text, mimetype=\"text/yaml\") def main(): app.run(debug=True, host=\"0.0.0.0\", port=5002) if __name__ == \"__main__\": main()\n",
      "22 --------------------------------------------------------------------------------\n",
      "12 Last, we need to set up and define a OpenAPI specification to match the endpoints defined on our remote server. In general, the OpenAPI specification would look the same regardless of the authentication method. Using an automatic OpenAPI generator will reduce the chance of errors when creating your OpenAPI specification.\n",
      "22 --------------------------------------------------------------------------------\n",
      "13 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 openapi: 3.0.1 info: title: TODO Plugin description: A plugin that allows the user to create and manage a TODO list using ChatGPT. If you do not know the user's username, ask them first before making queries to the plugin. Otherwise, use the username \"global\". version: \"v1\" servers: - url: https://example.com paths: /todos/{username}: get: operationId: getTodos summary: Get the list of todos parameters: - in: path name: username schema: type: string required: true description: The name of the user. responses: \"200\": description: OK content: application/json: schema: $ref: \"#/components/schemas/getTodosResponse\" post: operationId: addTodo summary: Add a todo to the list parameters: - in: path name: username schema: type: string required: true\n",
      "22 --------------------------------------------------------------------------------\n",
      "14 username schema: type: string required: true description: The name of the user. requestBody: required: true content: application/json: schema: $ref: \"#/components/schemas/addTodoRequest\" responses: \"200\": description: OK delete: operationId: deleteTodo summary: Delete a todo from the list parameters: - in: path name: username schema: type: string required: true description: The name of the user. requestBody: required: true content: application/json: schema: $ref: \"#/components/schemas/deleteTodoRequest\" responses: \"200\": description: OK components: schemas: getTodosResponse: type: object properties: todos: type: array items: type: string description: The list of todos. addTodoRequest: type: object required: - todo properties: todo: type: string description: The todo to add to the list. required: true deleteTodoRequest: type: object required: - todo_idx properties: todo_idx: type: integer description: The index of the todo to delete. required: true\n",
      "22 --------------------------------------------------------------------------------\n",
      "15 Learn how to build a simple sports stats plugin\n",
      "This plugin is an example of a simple sports stats API. Please keep in mind our domain policy and usage policies when considering what to build.\n",
      "To start, define an ai-plugin.json file with the following fields:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"schema_version\": \"v1\", \"name_for_human\": \"Sport Stats\", \"name_for_model\": \"sportStats\", \"description_for_human\": \"Get current and historical stats for sport players and games.\", \"description_for_model\": \"Get current and historical stats for sport players and games. Always display results using markdown tables.\", \"auth\": { \"type\": \"none\" }, \"api\": { \"type\": \"openapi\", \"url\": \"PLUGIN_HOSTNAME/openapi.yaml\" }, \"logo_url\": \"PLUGIN_HOSTNAME/logo.png\", \"contact_email\": \"support@example.com\", \"legal_info_url\": \"https://example.com/legal\" }\n",
      "Note the PLUGIN_HOSTNAME should be the actual hostname of your plugin server.\n",
      "Next, we define a mock API for a simple sports service plugin.\n",
      "22 --------------------------------------------------------------------------------\n",
      "16 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 import json import requests import urllib.parse import quart import quart_cors from quart import request # Note: Setting CORS to allow chat.openapi.com is only required when running a localhost plugin app = quart_cors.cors(quart.Quart(__name__), allow_origin=\"https://chat.openai.com\") HOST_URL = \"https://example.com\" @app.get(\"/players\") async def get_players(): query = request.args.get(\"query\") res = requests.get( f\"{HOST_URL}/api/v1/players?search={query}&page=0&per_page=100\") body = res.json() return quart.Response(response=json.dumps(body), status=200) @app.get(\"/teams\") async def get_teams(): res =\n",
      "22 --------------------------------------------------------------------------------\n",
      "17 @app.get(\"/teams\") async def get_teams(): res = requests.get( \"{HOST_URL}/api/v1/teams?page=0&per_page=100\") body = res.json() return quart.Response(response=json.dumps(body), status=200) @app.get(\"/games\") async def get_games(): query_params = [(\"page\", \"0\")] limit = request.args.get(\"limit\") query_params.append((\"per_page\", limit or \"100\")) start_date = request.args.get(\"start_date\") if start_date: query_params.append((\"start_date\", start_date)) end_date = request.args.get(\"end_date\") if end_date: query_params.append((\"end_date\", end_date)) seasons = request.args.getlist(\"seasons\") for season in seasons: query_params.append((\"seasons[]\", str(season))) team_ids = request.args.getlist(\"team_ids\") for team_id in team_ids: query_params.append((\"team_ids[]\", str(team_id))) res = requests.get( f\"{HOST_URL}/api/v1/games?{urllib.parse.urlencode(query_params)}\") body = res.json() return quart.Response(response=json.dumps(body), status=200) @app.get(\"/stats\") async def get_stats(): query_params = [(\"page\", \"0\")]\n",
      "22 --------------------------------------------------------------------------------\n",
      "18 def get_stats(): query_params = [(\"page\", \"0\")] limit = request.args.get(\"limit\") query_params.append((\"per_page\", limit or \"100\")) start_date = request.args.get(\"start_date\") if start_date: query_params.append((\"start_date\", start_date)) end_date = request.args.get(\"end_date\") if end_date: query_params.append((\"end_date\", end_date)) player_ids = request.args.getlist(\"player_ids\") for player_id in player_ids: query_params.append((\"player_ids[]\", str(player_id))) game_ids = request.args.getlist(\"game_ids\") for game_id in game_ids: query_params.append((\"game_ids[]\", str(game_id))) res = requests.get( f\"{HOST_URL}/api/v1/stats?{urllib.parse.urlencode(query_params)}\") body = res.json() return quart.Response(response=json.dumps(body), status=200) @app.get(\"/season_averages\") async def get_season_averages(): query_params = [] season = request.args.get(\"season\") if season: query_params.append((\"season\", str(season))) player_ids = request.args.getlist(\"player_ids\") for player_id in player_ids:\n",
      "22 --------------------------------------------------------------------------------\n",
      "19 for player_id in player_ids: query_params.append((\"player_ids[]\", str(player_id))) res = requests.get( f\"{HOST_URL}/api/v1/season_averages?{urllib.parse.urlencode(query_params)}\") body = res.json() return quart.Response(response=json.dumps(body), status=200) @app.get(\"/logo.png\") async def plugin_logo(): filename = 'logo.png' return await quart.send_file(filename, mimetype='image/png') @app.get(\"/.well-known/ai-plugin.json\") async def plugin_manifest(): host = request.headers['Host'] with open(\"ai-plugin.json\") as f: text = f.read() # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the manifest text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\") return quart.Response(text, mimetype=\"text/json\") @app.get(\"/openapi.yaml\") async def openapi_spec(): host = request.headers['Host'] with open(\"openapi.yaml\") as f: text = f.read() # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the OpenAPI spec text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\") return\n",
      "22 --------------------------------------------------------------------------------\n",
      "20 f\"https://{host}\") return quart.Response(text, mimetype=\"text/yaml\") def main(): app.run(debug=True, host=\"0.0.0.0\", port=5001) if __name__ == \"__main__\": main()\n",
      "22 --------------------------------------------------------------------------------\n",
      "21 Last, we define our OpenAPI specification:\n",
      "22 --------------------------------------------------------------------------------\n",
      "22 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 openapi: 3.0.1 info: title: Sport Stats description: Get current and historical stats for sport players and games. version: \"v1\" servers: - url: PLUGIN_HOSTNAME paths: /players: get: operationId: getPlayers summary: Retrieves all players from all seasons whose names match the query string. parameters: - in: query name: query schema: type: string description: Used to filter players based on their name. For example, ?query=davis will return players that have 'davis' in their first or last name. responses: \"200\": description: OK /teams: get: operationId: getTeams summary: Retrieves all teams for the current season. responses: \"200\":\n",
      "22 --------------------------------------------------------------------------------\n",
      "23 teams for the current season. responses: \"200\": description: OK /games: get: operationId: getGames summary: Retrieves all games that match the filters specified by the args. Display results using markdown tables. parameters: - in: query name: limit schema: type: string description: The max number of results to return. - in: query name: seasons schema: type: array items: type: string description: Filter by seasons. Seasons are represented by the year they began. For example, 2018 represents season 2018-2019. - in: query name: team_ids schema: type: array items: type: string description: Filter by team ids. Team ids can be determined using the getTeams function. - in: query name: start_date schema: type: string description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or after this date. - in: query name: end_date schema: type: string description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or before this date. responses: \"200\":\n",
      "22 --------------------------------------------------------------------------------\n",
      "24 occur on or before this date. responses: \"200\": description: OK /stats: get: operationId: getStats summary: Retrieves stats that match the filters specified by the args. Display results using markdown tables. parameters: - in: query name: limit schema: type: string description: The max number of results to return. - in: query name: player_ids schema: type: array items: type: string description: Filter by player ids. Player ids can be determined using the getPlayers function. - in: query name: game_ids schema: type: array items: type: string description: Filter by game ids. Game ids can be determined using the getGames function. - in: query name: start_date schema: type: string description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or after this date. - in: query name: end_date schema: type: string description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or before this date. responses: \"200\": description: OK /season_averages: get:\n",
      "22 --------------------------------------------------------------------------------\n",
      "25 \"200\": description: OK /season_averages: get: operationId: getSeasonAverages summary: Retrieves regular season averages for the given players. Display results using markdown tables. parameters: - in: query name: season schema: type: string description: Defaults to the current season. A season is represented by the year it began. For example, 2018 represents season 2018-2019. - in: query name: player_ids schema: type: array items: type: string description: Filter by player ids. Player ids can be determined using the getPlayers function. responses: \"200\": description: OK\n",
      "22 --------------------------------------------------------------------------------\n",
      "26 Learn how to build a simple OAuth todo list plugin\n",
      "To create an OAuth plugin, we start by defining a ai-plugin.json file with the auth type set to oauth:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"schema_version\": \"v1\", \"name_for_human\": \"TODO List (OAuth)\", \"name_for_model\": \"todo_oauth\", \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\", \"description_for_model\": \"Plugin for managing a TODO list, you can add, remove and view your TODOs.\", \"auth\": { \"type\": \"oauth\", \"client_url\": \"PLUGIN_HOSTNAME/oauth\", \"scope\": \"\", \"authorization_url\": \"PLUGIN_HOSTNAME/auth/oauth_exchange\", \"authorization_content_type\": \"application/json\", \"verification_tokens\": { \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\" } }, \"api\": { \"type\": \"openapi\", \"url\": \"PLUGIN_HOSTNAME/openapi.yaml\" }, \"logo_url\": \"PLUGIN_HOSTNAME/logo.png\", \"contact_email\": \"contact@example.com\", \"legal_info_url\": \"http://www.example.com/legal\" }\n",
      "22 --------------------------------------------------------------------------------\n",
      "27 Next, we need to define our OAuth service. This OAuth example is not intended for production use cases but rather to highlight what a simple OAuth flow will look like so developers can get experience building towards a production solution.\n",
      "22 --------------------------------------------------------------------------------\n",
      "28 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 import json import quart import quart_cors from quart import request app = quart_cors.cors(quart.Quart(__name__), allow_origin=\"*\") _TODOS = {} @app.post(\"/todos/<string:username>\") async def add_todo(username): request = await quart.request.get_json(force=True) if username not in _TODOS: _TODOS[username] = [] _TODOS[username].append(request[\"todo\"]) return quart.Response(response='OK', status=200) @app.get(\"/todos/<string:username>\") async def get_todos(username): print(request.headers) return quart.Response(response=json.dumps(_TODOS.get(username, [])), status=200) @app.delete(\"/todos/<string:username>\") async def delete_todo(username): request = await quart.request.get_json(force=True) todo_idx = request[\"todo_idx\"] # fail\n",
      "22 --------------------------------------------------------------------------------\n",
      "29 todo_idx = request[\"todo_idx\"] # fail silently, it's a simple plugin if 0 <= todo_idx < len(_TODOS[username]): _TODOS[username].pop(todo_idx) return quart.Response(response='OK', status=200) @app.get(\"/logo.png\") async def plugin_logo(): filename = 'logo.png' return await quart.send_file(filename, mimetype='image/png') @app.get(\"/.well-known/ai-plugin.json\") async def plugin_manifest(): host = request.headers['Host'] with open(\"manifest.json\") as f: text = f.read() text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\") return quart.Response(text, mimetype=\"text/json\") @app.get(\"/openapi.yaml\") async def openapi_spec(): host = request.headers['Host'] with open(\"openapi.yaml\") as f: text = f.read() text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\") return quart.Response(text, mimetype=\"text/yaml\") @app.get(\"/oauth\") async def oauth(): query_string = request.query_string.decode('utf-8') parts = query_string.split('&') kvps = {} for part in parts: k, v = part.split('=') v = v.replace(\"%2F\",\n",
      "22 --------------------------------------------------------------------------------\n",
      "30 k, v = part.split('=') v = v.replace(\"%2F\", \"/\").replace(\"%3A\", \":\") kvps[k] = v print(\"OAuth key value pairs from the ChatGPT Request: \", kvps) url = kvps[\"redirect_uri\"] + f\"?code={OPENAI_CODE}\" print(\"URL: \", url) return quart.Response( f'<a href=\"{url}\">Click to authorize</a>' ) # Sample names OPENAI_CLIENT_ID = \"id\" OPENAI_CLIENT_SECRET = \"secret\" OPENAI_CODE = \"abc123\" OPENAI_TOKEN = \"def456\" @app.post(\"/auth/oauth_exchange\") async def oauth_exchange(): request = await quart.request.get_json(force=True) print(f\"oauth_exchange {request=}\") if request[\"client_id\"] != OPENAI_CLIENT_ID: raise RuntimeError(\"bad client ID\") if request[\"client_secret\"] != OPENAI_CLIENT_SECRET: raise RuntimeError(\"bad client secret\") if request[\"code\"] != OPENAI_CODE: raise RuntimeError(\"bad code\") return { \"access_token\": OPENAI_TOKEN, \"token_type\": \"bearer\" } def main(): app.run(debug=True, host=\"0.0.0.0\", port=5002) if __name__ == \"__main__\": main()\n",
      "22 --------------------------------------------------------------------------------\n",
      "31 Last, like with our other examples, we define a simple OpenAPI file based on the endpoints:\n",
      "22 --------------------------------------------------------------------------------\n",
      "32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 openapi: 3.0.1 info: title: TODO Plugin description: A plugin that allows the user to create and manage a TODO list using ChatGPT. If you do not know the user's username, ask them first before making queries to the plugin. Otherwise, use the username \"global\". version: \"v1\" servers: - url: PLUGIN_HOSTNAME paths: /todos/{username}: get: operationId: getTodos summary: Get the list of todos parameters: - in: path name: username schema: type: string required: true description: The name of the user. responses: \"200\": description: OK content: application/json: schema: $ref: \"#/components/schemas/getTodosResponse\" post: operationId: addTodo summary: Add a todo to the list parameters: - in: path name: username schema: type: string required: true\n",
      "22 --------------------------------------------------------------------------------\n",
      "33 username schema: type: string required: true description: The name of the user. requestBody: required: true content: application/json: schema: $ref: \"#/components/schemas/addTodoRequest\" responses: \"200\": description: OK delete: operationId: deleteTodo summary: Delete a todo from the list parameters: - in: path name: username schema: type: string required: true description: The name of the user. requestBody: required: true content: application/json: schema: $ref: \"#/components/schemas/deleteTodoRequest\" responses: \"200\": description: OK components: schemas: getTodosResponse: type: object properties: todos: type: array items: type: string description: The list of todos. addTodoRequest: type: object required: - todo properties: todo: type: string description: The todo to add to the list. required: true deleteTodoRequest: type: object required: - todo_idx properties: todo_idx: type: integer description: The index of the todo to delete. required: true\n",
      "22 --------------------------------------------------------------------------------\n",
      "34 Learn how to build a semantic search and retrieval plugin\n",
      "The ChatGPT retrieval plugin is a more fully featured code example. The scope of the plugin is large, so we encourage you to read through the code to see what a more advanced plugin looks like.\n",
      "The retrieval plugin includes:\n",
      "Support for multiple vector databases providers\n",
      "All 4 different authentication methods\n",
      "Multiple different API features\n",
      "22 --------------------------------------------------------------------------------\n",
      "0 Plugins in production\n",
      "Rate limits\n",
      "Consider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your plugin after receiving a certain number of 429's or 500's in a short period of time.\n",
      "Timeouts\n",
      "When making API calls during the plugin experience, timeouts take place if the following thresholds are exceeded:\n",
      "15 seconds round trip for fetching ai-plugin.json/openapi.yaml\n",
      "45 seconds round trip for API calls\n",
      "As we scale the plugin experience to more people, we expect that the timeout thresholds will decrease.\n",
      "Rate limiting users\n",
      "We do not expose persistent user level ID's, instead we pass a openai-ephemeral-user-id header in the requests we send to a plugin. The openai-ephemeral-user-id resets every day at midnight UTC time. We also expose a persistent openai-conversation-id header which allows developers to see which conversation a request is related to.\n",
      "Updating your plugin\n",
      "23 --------------------------------------------------------------------------------\n",
      "1 Updating your plugin\n",
      "Manifest files must be manually updated by going through the \"Develop your own plugin\" flow in the plugin store each time you make a change to the ai-plugin.json file. ChatGPT will automatically fetch the latest OpenAPI spec each time a request is made so changes made will propagate to end users immediately. If your plugin is available in the ChatGPT plugin store and you go through the \"Develop your own plugin\" flow, we will automatically look for changes in the files and remove the plugin if it has changed. You will have to resubmit your plugin to be included in the store again.\n",
      "Plugin terms\n",
      "In order to register a plugin, you must agree to the Plugin Terms.\n",
      "Domain verification and security\n",
      "To ensure that plugins can only perform actions on resources that they control, OpenAI enforces requirements on the plugin's manifest and API specifications.\n",
      "Defining the plugin's root domain\n",
      "23 --------------------------------------------------------------------------------\n",
      "2 Defining the plugin's root domain\n",
      "The manifest file defines information shown to the user (like logo and contact information) as well as a URL where the plugin's OpenAPI spec is hosted. When the manifest is fetched, the plugin's root domain is established following these rules:\n",
      "If the domain has www. as a subdomain, then the root domain will strip out www. from the domain that hosts the manifest.\n",
      "Otherwise, the root domain is the same as the domain that hosts the manifest.\n",
      "Note on redirects: If there are any redirects in resolving the manifest, only child subdomain redirects are allowed. The only exception is following a redirect from a www subdomain to one without the www.\n",
      "Examples of what the root domain looks like:\n",
      "✅ https://example.com/.well-known/ai-plugin.json\n",
      "Root domain: example.com\n",
      "✅ https://www.example.com/.well-known/ai-plugin.json\n",
      "Root domain: example.com\n",
      "✅ https://www.example.com/.well-known/ai-plugin.json → redirects to https://example.com/.well-known/ai-plugin.json\n",
      "Root domain: example.com\n",
      "23 --------------------------------------------------------------------------------\n",
      "3 Root domain: example.com\n",
      "✅ https://foo.example.com/.well-known/ai-plugin.json → redirects to https://bar.foo.example.com/.well-known/ai-plugin.json\n",
      "Root domain: bar.foo.example.com\n",
      "✅ https://foo.example.com/.well-known/ai-plugin.json → redirects to https://bar.foo.example.com/baz/ai-plugin.json\n",
      "Root domain: bar.foo.example.com\n",
      "❌ https://foo.example.com/.well-known/ai-plugin.json → redirects to https://example.com/.well-known/ai-plugin.json\n",
      "Redirect to parent level domain is disallowed\n",
      "❌ https://foo.example.com/.well-known/ai-plugin.json → redirects to https://bar.example.com/.well-known/ai-plugin.json\n",
      "Redirect to same level subdomain is disallowed\n",
      "❌ https://example.com/.well-known/ai-plugin.json -> redirects to https://example2.com/.well-known/ai-plugin.json\n",
      "Redirect to another domain is disallowed\n",
      "Manifest validation\n",
      "Specific fields in the manifest itself must satisfy the following requirements:\n",
      "23 --------------------------------------------------------------------------------\n",
      "4 api.url - the URL provided to the OpenAPI spec must be hosted at the same level or a subdomain of the root domain.\n",
      "legal_info - The second-level domain of the URL provided must be the same as the second-level domain of the root domain.\n",
      "contact_info - The second-level domain of the email address should be the same as the second-level domain of the root domain.\n",
      "Resolving the API spec\n",
      "The api.url field in the manifest provides a link to an OpenAPI spec that defines APIs that the plugin can call into. OpenAPI allows specifying multiple server base URLs. The following logic is used to select the server URL:\n",
      "Iterate through the list of server URLs\n",
      "Use the first server URL that is either an exact match of the root domain or a subdomain of the root domain\n",
      "If neither cases above apply, then default to the domain where the API spec is hosted. For example, if the spec is hosted on api.example.com, then api.example.com will be used as the base URL for the routes in the OpenAPI spec.\n",
      "23 --------------------------------------------------------------------------------\n",
      "5 Note: Please avoid using redirects for hosting the API spec and any API endpoints, as it is not guaranteed that redirects will always be followed.\n",
      "Use TLS and HTTPS\n",
      "All traffic with the plugin (e.g., fetching the ai-plugin.json file, the OpenAPI spec, API calls) must use TLS 1.2 or later on port 443 with a valid public certificate.\n",
      "IP egress ranges\n",
      "ChatGPT will call your plugin from an IP address in the CIDR block 23.102.140.112/28. You may wish to explicitly allowlist these IP addresses.\n",
      "Separately, OpenAI's web browsing plugin accesses websites from a different IP address block: 23.98.142.176/28.\n",
      "FAQ\n",
      "How is plugin data used?\n",
      "Plugins connect ChatGPT to external apps. If a user enables a plugin, ChatGPT may send parts of their conversation and their country or state to your plugin.\n",
      "What happens if a request to my API fails?\n",
      "If an API request fails, the model might retry the request up to 10 times before letting the user know it cannot get a response from that plugin.\n",
      "Can I invite people to try my plugin?\n",
      "23 --------------------------------------------------------------------------------\n",
      "6 Can I invite people to try my plugin?\n",
      "Yes, all unverified plugins can be installed by up to 100 other developers who have plugin access. If your plugin is available in the plugin store, it will be accessible to all ChatGPT plus customers.\n",
      "Can I charge people money for my plugin?\n",
      "Not at this time.\n",
      "23 --------------------------------------------------------------------------------\n",
      "0 Model index for researchers\n",
      "Our models are used for both research purposes and developer use cases in production. Researchers often learn about our models from papers that we have published, but there is often not a perfect match between what is available in the OpenAI API and what is published in a paper.\n",
      "The purpose of this page is to help clarify:\n",
      "Some of the differences in the ways that our models are trained, which impacts the comparisons that can be made between models, and various evaluation results.\n",
      "The differences between various model series, such as GPT 3.5 and InstructGPT.\n",
      "Which if any of the models available in the API today match with a model in a paper. In some cases, there might not be a match.\n",
      "Models referred to as \"GPT 3.5\"\n",
      "GPT-3.5 series is a series of models that was trained on a blend of text and code from before Q4 2021. The following models are in the GPT-3.5 series:\n",
      "code-davinci-002 is a base model, so good for pure code-completion tasks\n",
      "24 --------------------------------------------------------------------------------\n",
      "1 text-davinci-002 is an InstructGPT model based on code-davinci-002\n",
      "text-davinci-003 is an improvement on text-davinci-002\n",
      "gpt-3.5-turbo-0301 is an improvement on text-davinci-003, optimized for chat\n",
      "InstructGPT models\n",
      "We offer variants of InstructGPT models trained in 3 different ways:\n",
      "Training MethodModels\n",
      "SFT \n",
      "Supervised fine-tuning on human demonstrations\tdavinci-instruct-beta1\t\n",
      "FeedME \n",
      "Supervised fine-tuning on human-written demonstrations and on model samples rated 7/7 by human labelers on an overall quality score\ttext-davinci-001, text-davinci-002, text-curie-001, text-babbage-001\t\n",
      "PPO \n",
      "Reinforcement learning with reward models trained from comparisons by humans\ttext-davinci-003\n",
      "24 --------------------------------------------------------------------------------\n",
      "2 The SFT and PPO models are trained similarly to the ones from the InstructGPT paper. FeedME (short for \"feedback made easy\") models are trained by distilling the best completions from all of our models. Our models generally used the best available datasets at the time of training, and so different engines using the same training methodology might be trained on different data.\n",
      "Models featured in OpenAI Research\n",
      "These are the most proximate models featured in our research papers that are available in the API today. Please note that not all models available in the API correspond to a paper, and even for models that are listed below there may be subtle differences that do not allow for exact replication of the paper. \n",
      "PaperPublishedModel Name in PaperModel Name in APIParameters2\n",
      "[2005.14165] Language Models are Few-Shot Learners\t22 Jul 2020\tGPT-3 175B\tdavinci\t175B\t\n",
      "GPT-3 6.7B\tcurie\t6.7B\t\n",
      "GPT-3 1B\tbabbage\t1B\n",
      "24 --------------------------------------------------------------------------------\n",
      "3 GPT-3 6.7B\tcurie\t6.7B\t\n",
      "GPT-3 1B\tbabbage\t1B\t\n",
      "[2107.03374] Evaluating Large Language Models Trained on Code\t14 Jul 2021\tCodex 12B\tcode-cushman-0013\t12B\t\n",
      "[2201.10005] Text and Code Embeddings by Contrastive Pre-Training\t14 Jan 2022\tGPT-3 unsupervised cpt-text 175B\ttext-similarity-davinci-001\t175B\t\n",
      "GPT-3 unsupervised cpt-text 6B\ttext-similarity-curie-001\t6B\t\n",
      "GPT-3 unsupervised cpt-text 1.2B\tNo close matching model on API\t1.2B\t\n",
      "[2009.01325] Learning to summarize from human feedback\t15 Feb 2022\tGPT-3 6.7B pretrain\tNo close matching model on API\t6.7B\t\n",
      "GPT-3 2.7B pretrain\tNo close matching model on API\t2.7B\t\n",
      "GPT-3 1.3B pretrain\tNo close matching model on API\t1.3B\t\n",
      "[2203.02155] Training language models to follow instructions with human feedback\t4 Mar 2022\tInstructGPT-3 175B SFT\tdavinci-instruct-beta\t175B\t\n",
      "InstructGPT-3 175B\tNo close matching model on API\t175B\t\n",
      "InstructGPT-3 6B\tNo close matching model on API\t6B\t\n",
      "InstructGPT-3 1.3B\tNo close matching model on API\t1.3B\t\n",
      "Researcher Access Program\n",
      "24 --------------------------------------------------------------------------------\n",
      "4 Researcher Access Program\n",
      "There are a number of research directions we are excited to explore with the OpenAI API. If you are interested in the opportunity for subsidized access, please provide us with details about your research use case using this form.\n",
      "In particular, we consider the following to be especially important directions, though you are free to craft your own direction:\n",
      "Alignment: How can we understand what objective, if any, a model is best understood as pursuing? How do we increase the extent to which that objective is aligned with human preferences, such as via prompt design or fine-tuning?\n",
      "Fairness and Representation: How should performance criteria be established for fairness and representation in language models? How can language models be improved in order to effectively support the goals of fairness and representation in specific, deployed contexts?\n",
      "24 --------------------------------------------------------------------------------\n",
      "5 Interdisciplinary Research: How can AI development draw on insights from other disciplines such as philosophy, cognitive science, and sociolinguistics?\n",
      "Interpretability / Transparency: How do these models work, mechanistically? Can we identify what concepts they’re using, or extract latent knowledge from the model, make inferences about the training procedure, or predict surprising future behavior?\n",
      "Misuse Potential: How can systems like the API be misused? What sorts of ‘red teaming’ approaches can we develop to help us and other AI developers think about responsibly deploying technologies like this?\n",
      "Model Exploration: Models like those served by the API have a variety of capabilities which we have yet to explore. We’re excited by investigations in many areas including model limitations, linguistic properties, commonsense reasoning, and potential uses for many other problems.\n",
      "24 --------------------------------------------------------------------------------\n",
      "6 Robustness: Generative models have uneven capability surfaces, with the potential for surprisingly strong and surprisingly weak areas of capability. How robust are large generative models to \"natural\" perturbations in the prompt, such as phrasing the same idea in different ways or with/without typos? Can we predict the kinds of domains and tasks for which large generative models are more likely to be robust (or not robust), and how does this relate to the training data? Are there techniques we can use to predict and mitigate worst-case behavior? How can robustness be measured in the context of few-shot learning (e.g. across variations in prompts)? Can we train models so that they satisfy safety properties with a very high level of reliability, even under adversarial inputs?\n",
      "24 --------------------------------------------------------------------------------\n",
      "7 Please note that due to a high volume of requests, it takes time for us to review these applications (up to 30 business days) and not all research will be prioritized for subsidy. We will only be in touch if your application is selected for subsidy. If you have questions about the Researcher Access Program, you can get in touch with us at researcheraccess@openai.com.\n",
      "24 --------------------------------------------------------------------------------\n",
      "0 Learn how to customize a model for your application.\n",
      "Introduction\n",
      "On August 22nd, 2023, we announced the deprecation of the /v1/fine-tunes API will take place on January 4th, 2024. We recommend all users migrate to the new /v1/fine_tuning/jobs API.\n",
      "Fine-tuning lets you get more out of the models available through the API by providing:\n",
      "Higher quality results than prompt design\n",
      "Ability to train on more examples than can fit in a prompt\n",
      "Token savings due to shorter prompts\n",
      "Lower latency requests\n",
      "GPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"\n",
      "25 --------------------------------------------------------------------------------\n",
      "1 Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. This saves costs and enables lower-latency requests.\n",
      "At a high level, fine-tuning involves the following steps:\n",
      "Prepare and upload training data\n",
      "Train a new fine-tuned model\n",
      "Use your fine-tuned model\n",
      "Visit our pricing page to learn more about how fine-tuned model training and usage are billed.\n",
      "What models can be fine-tuned?\n",
      "The now deprecated fine-tunes endpoint only supports the following base models: davinci, curie, babbage, and ada. These are the original models that do not have any instruction following training (like text-davinci-003 does for example). You are also able to continue fine-tuning a fine-tuned model to add additional data without having to start from scratch.\n",
      "Installation\n",
      "25 --------------------------------------------------------------------------------\n",
      "2 Installation\n",
      "We recommend using our OpenAI command-line interface (CLI). To install this, run\n",
      "pip install --upgrade openai\n",
      "(The following instructions work for version 0.9.4 and up. Additionally, the OpenAI CLI requires python 3.)\n",
      "Set your OPENAI_API_KEY environment variable by adding the following line into your shell initialization script (e.g. .bashrc, zshrc, etc.) or running it in the command line before the fine-tuning command:\n",
      "export OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n",
      "Prepare training data\n",
      "Training data is how you teach GPT-3 what you'd like it to say.\n",
      "Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. You can use our CLI data preparation tool to easily convert your data into this file format.\n",
      "1 2 3 4 {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"} {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"} {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"} ...\n",
      "25 --------------------------------------------------------------------------------\n",
      "3 Designing your prompts and completions for fine-tuning is different from designing your prompts for use with our base models (Davinci, Curie, Babbage, Ada). In particular, while prompts for base models often consist of multiple examples (\"few-shot learning\"), for fine-tuning, each training example generally consists of a single input example and its associated output, without the need to give detailed instructions or include multiple examples in the same prompt.\n",
      "For more detailed guidance on how to prepare training data for various tasks, please refer to our preparing your dataset best practices.\n",
      "The more training examples you have, the better. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.\n",
      "CLI data preparation tool\n",
      "We developed a tool which validates, gives suggestions and reformats your data:\n",
      "openai tools fine_tunes.prepare_data -f <LOCAL_FILE>\n",
      "25 --------------------------------------------------------------------------------\n",
      "4 This tool accepts different formats, with the only requirement that they contain a prompt and a completion column/key. You can pass a CSV, TSV, XLSX, JSON or JSONL file, and it will save the output into a JSONL file ready for fine-tuning, after guiding you through the process of suggested changes.\n",
      "Create a fine-tuned model\n",
      "The following assumes you've already prepared training data following the above instructions.\n",
      "Start your fine-tuning job using the OpenAI CLI:\n",
      "openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>\n",
      "Where BASE_MODEL is the name of the base model you're starting from (ada, babbage, curie, or davinci). You can customize your fine-tuned model's name using the suffix parameter.\n",
      "Running the above command does several things:\n",
      "Uploads the file using the files API (or uses an already-uploaded file)\n",
      "Creates a fine-tune job\n",
      "Streams events until the job is done (this often takes minutes, but can take hours if there are many jobs in the queue or your dataset is large)\n",
      "25 --------------------------------------------------------------------------------\n",
      "5 Every fine-tuning job starts from a base model, which defaults to curie. The choice of model influences both the performance of the model and the cost of running your fine-tuned model. Your model can be one of: ada, babbage, curie, or davinci. Visit our pricing page for details on fine-tune rates.\n",
      "After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size. If the event stream is interrupted for any reason, you can resume it by running:\n",
      "openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID>\n",
      "When the job is done, it should display the name of the fine-tuned model.\n",
      "In addition to creating a fine-tune job, you can also list existing jobs, retrieve the status of a job, or cancel a job.\n",
      "25 --------------------------------------------------------------------------------\n",
      "6 1 2 3 4 5 6 7 8 9 10 # List all created fine-tunes openai api fine_tunes.list # Retrieve the state of a fine-tune. The resulting object includes # job status (which can be one of pending, running, succeeded, or failed) # and other information openai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID> # Cancel a job openai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>\n",
      "Use a fine-tuned model\n",
      "When a job has succeeded, the fine_tuned_model field will be populated with the name of the model. You may now specify this model as a parameter to our Completions API, and make requests to it using the Playground.\n",
      "After your job first completes, it may take several minutes for your model to become ready to handle requests. If completion requests to your model time out, it is likely because your model is still being loaded. If this happens, try again in a few minutes.\n",
      "You can start making requests by passing the model name as the model parameter of a completion request:\n",
      "OpenAI CLI:\n",
      "25 --------------------------------------------------------------------------------\n",
      "7 OpenAI CLI:\n",
      "openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>\n",
      "cURL:\n",
      "1 2 3 4 curl https://api.openai.com/v1/completions \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}'\n",
      "Python:\n",
      "1 2 3 4 import openai openai.Completion.create( model=FINE_TUNED_MODEL, prompt=YOUR_PROMPT)\n",
      "Node.js:\n",
      "1 2 3 4 const response = await openai.createCompletion({ model: FINE_TUNED_MODEL prompt: YOUR_PROMPT, });\n",
      "You may continue to use all the other Completions parameters like temperature, frequency_penalty, presence_penalty, etc, on these requests to fine-tuned models.\n",
      "Delete a fine-tuned model\n",
      "To delete a fine-tuned model, you must be designated an \"owner\" within your organization.\n",
      "OpenAI CLI:\n",
      "openai api models.delete -i <FINE_TUNED_MODEL>\n",
      "cURL:\n",
      "curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\ -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
      "Python:\n",
      "import openai openai.Model.delete(FINE_TUNED_MODEL)\n",
      "25 --------------------------------------------------------------------------------\n",
      "8 Preparing your dataset\n",
      "Fine-tuning is a powerful technique to create a new model that's specific to your use case. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.\n",
      "Data formatting\n",
      "To fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our base models, where you might input detailed instructions or multiple examples in a single prompt.\n",
      "Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.\n",
      "Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
      "25 --------------------------------------------------------------------------------\n",
      "9 Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.\n",
      "For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.\n",
      "General best practices\n",
      "Fine-tuning performs better with more high-quality examples. To fine-tune a model that performs better than using a high-quality prompt with our base models, you should provide at least a few hundred high-quality examples, ideally vetted by human experts. From there, performance tends to linearly increase with every doubling of the number of examples. Increasing the number of examples is usually the best and most reliable way of improving performance.\n",
      "25 --------------------------------------------------------------------------------\n",
      "10 Classifiers are the easiest models to get started with. For classification problems we suggest using ada, which generally tends to perform only very slightly worse than more capable models once fine-tuned, whilst being significantly faster and cheaper.\n",
      "If you are fine-tuning on a pre-existing dataset rather than writing prompts from scratch, be sure to manually review your data for offensive or inaccurate content if possible, or review as many random samples of the dataset as possible if it is large.\n",
      "Specific guidelines\n",
      "Fine-tuning can solve a variety of problems, and the optimal way to use it may depend on your specific use case. Below, we've listed the most common use cases for fine-tuning and corresponding guidelines.\n",
      "Classification\n",
      "Is the model making untrue statements?\n",
      "Sentiment analysis\n",
      "Categorization for email triage\n",
      "Conditional generation\n",
      "Write an engaging ad based on a Wikipedia article\n",
      "Entity extraction\n",
      "Customer support chatbot\n",
      "Product description based on a technical list of properties\n",
      "25 --------------------------------------------------------------------------------\n",
      "11 Classification\n",
      "In classification problems, each input in the prompt should be classified into one of the predefined classes. For this type of problem, we recommend:\n",
      "Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.\n",
      "Choose classes that map to a single token. At inference time, specify max_tokens=1 since you only need the first token for classification.\n",
      "Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator\n",
      "Aim for at least ~100 examples per class\n",
      "To get class log probabilities you can specify logprobs=5 (for 5 classes) when using your model\n",
      "Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n",
      "Case study: Is the model making untrue statements?\n",
      "25 --------------------------------------------------------------------------------\n",
      "12 Let's say you'd like to ensure that the text of the ads on your website mention the correct product and company. In other words, you want to ensure the model isn't making things up. You may want to fine-tune a classifier which filters out incorrect ads.\n",
      "The dataset might look something like the following:\n",
      "{\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"} {\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"}\n",
      "In the example above, we used a structured input containing the name of the company, the product, and the associated ad. As a separator we used \\nSupported: which clearly separated the prompt from the completion. With a sufficient number of examples, the separator doesn't make much of a difference (usually less than 0.4%) as long as it doesn't appear within the prompt or the completion.\n",
      "25 --------------------------------------------------------------------------------\n",
      "13 For this use case we fine-tuned an ada model since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.\n",
      "Now we can query our model by making a Completion request.\n",
      "1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\", \"max_tokens\": 1, \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\" }'\n",
      "Which will return either yes or no.\n",
      "Case study: Sentiment analysis\n",
      "Let's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following:\n",
      "{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"} {\"prompt\":\"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\", \"completion\":\" negative\"}\n",
      "25 --------------------------------------------------------------------------------\n",
      "14 Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment.\n",
      "Now we can query our model by making a Completion request.\n",
      "1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" \\ -d '{ \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\", \"max_tokens\": 1, \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\" }'\n",
      "Which will return:\n",
      "25 --------------------------------------------------------------------------------\n",
      "15 Which will return:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"id\": \"cmpl-COMPLETION_ID\", \"object\": \"text_completion\", \"created\": 1589498378, \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\", \"choices\": [ { \"logprobs\": { \"text_offset\": [19], \"token_logprobs\": [-0.03597255], \"tokens\": [\" positive\"], \"top_logprobs\": [ { \" negative\": -4.9785037, \" positive\": -0.03597255 } ] }, \"text\": \" positive\", \"index\": 0, \"finish_reason\": \"length\" } ] }\n",
      "Case study: Categorization for Email triage\n",
      "Let's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows:\n",
      "25 --------------------------------------------------------------------------------\n",
      "16 1 2 3 4 { \"prompt\": \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\", \"completion\": \" <numerical_category>\" }\n",
      "For example:\n",
      "1 2 3 4 { \"prompt\": \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\", \"completion\": \" 4\" }\n",
      "In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email.\n",
      "Conditional generation\n",
      "Conditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend:\n",
      "25 --------------------------------------------------------------------------------\n",
      "17 Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.\n",
      "Use an ending token at the end of the completion, e.g. END\n",
      "Remember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]\n",
      "Aim for at least ~500 examples\n",
      "Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator\n",
      "Ensure the examples are of high quality and follow the same desired format\n",
      "Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n",
      "Using Lower learning rate and only 1-2 epochs tends to work better for these use cases\n",
      "Case study: Write an engaging ad based on a Wikipedia article\n",
      "25 --------------------------------------------------------------------------------\n",
      "18 This is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this:\n",
      "1 2 3 4 { \"prompt\": \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\", \"completion\": \" <engaging ad> END\" }\n",
      "For example:\n",
      "25 --------------------------------------------------------------------------------\n",
      "19 1 2 3 4 { \"prompt\": \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with\n",
      "25 --------------------------------------------------------------------------------\n",
      "20 point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\", \"completion\": \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\" }\n",
      "25 --------------------------------------------------------------------------------\n",
      "21 Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.\n",
      "Case study: Entity extraction\n",
      "This is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows:\n",
      "1 2 3 4 { \"prompt\": \"<any text, for example news article>\\n\\n###\\n\\n\", \"completion\": \" <list of entities, separated by a newline> END\" }\n",
      "For example:\n",
      "25 --------------------------------------------------------------------------------\n",
      "22 For example:\n",
      "1 2 3 4 { \"prompt\": \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\", \"completion\": \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\" }\n",
      "A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.\n",
      "Case study: Customer support chatbot\n",
      "25 --------------------------------------------------------------------------------\n",
      "23 Case study: Customer support chatbot\n",
      "A chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows:\n",
      "25 --------------------------------------------------------------------------------\n",
      "24 {\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\", \"completion\":\" <response2>\\n\"} {\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\", \"completion\":\" <response3>\\n\"}\n",
      "Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference.\n",
      "Case study: Product description based on a technical list of properties\n",
      "Here it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format:\n",
      "25 --------------------------------------------------------------------------------\n",
      "25 1 2 3 4 { \"prompt\": \"Item=handbag, Color=army_green, price=$99, size=S->\", \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\" }\n",
      "Won't work as well as:\n",
      "1 2 3 4 { \"prompt\": \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\", \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\" }\n",
      "For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference.\n",
      "Advanced usage\n",
      "Customize your model name\n",
      "You can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter.\n",
      "OpenAI CLI:\n",
      "25 --------------------------------------------------------------------------------\n",
      "26 OpenAI CLI:\n",
      "openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\"\n",
      "The resulting name would be:\n",
      "ada:ft-your-org:custom-model-name-2022-02-15-04-21-04\n",
      "Analyzing your fine-tuned model\n",
      "We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files:\n",
      "OpenAI CLI:\n",
      "openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>\n",
      "CURL:\n",
      "curl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\ -H \"Authorization: Bearer $OPENAI_API_KEY\" > results.csv\n",
      "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:\n",
      "elapsed_tokens: the number of tokens the model has seen so far (including repeats)\n",
      "25 --------------------------------------------------------------------------------\n",
      "27 elapsed_examples: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.\n",
      "training_loss: loss on the training batch\n",
      "training_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67\n",
      "training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83\n",
      "Classification specific metrics\n",
      "25 --------------------------------------------------------------------------------\n",
      "28 Classification specific metrics\n",
      "We also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file.\n",
      "To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification.\n",
      "OpenAI CLI:\n",
      "25 --------------------------------------------------------------------------------\n",
      "29 OpenAI CLI:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # For multiclass classification openai api fine_tunes.create \\ -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_OR_PATH> \\ -m <MODEL> \\ --compute_classification_metrics \\ --classification_n_classes <N_CLASSES> # For binary classification openai api fine_tunes.create \\ -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_OR_PATH> \\ -m <MODEL> \\ --compute_classification_metrics \\ --classification_n_classes 2 \\ --classification_positive_class <POSITIVE_CLASS_FROM_DATASET>\n",
      "The following metrics will be displayed in your results file if you set --compute_classification_metrics:\n",
      "For multiclass classification\n",
      "classification/accuracy: accuracy\n",
      "classification/weighted_f1_score: weighted F-1 score\n",
      "For binary classification\n",
      "The following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.)\n",
      "classification/accuracy\n",
      "classification/precision\n",
      "classification/recall\n",
      "25 --------------------------------------------------------------------------------\n",
      "30 classification/precision\n",
      "classification/recall\n",
      "classification/f{beta}\n",
      "classification/auroc - AUROC\n",
      "classification/auprc - AUPRC\n",
      "Note that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.\n",
      "Validation\n",
      "You can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive.\n",
      "If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training.\n",
      "OpenAI CLI:\n",
      "1 2 3 openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_ID_OR_PATH> \\ -m <MODEL>\n",
      "25 --------------------------------------------------------------------------------\n",
      "31 If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file:\n",
      "validation_loss: loss on the validation batch\n",
      "validation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67\n",
      "validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83\n",
      "Hyperparameters\n",
      "25 --------------------------------------------------------------------------------\n",
      "32 Hyperparameters\n",
      "We've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file.\n",
      "That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following:\n",
      "model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation.\n",
      "n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.\n",
      "batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets.\n",
      "25 --------------------------------------------------------------------------------\n",
      "33 learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes.\n",
      "compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.\n",
      "To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example:\n",
      "1 2 3 4 openai api fine_tunes.create \\ -t file-JD89ePi5KMsB3Tayeli5ovfW \\ -m ada \\ --n_epochs 1\n",
      "Continue fine-tuning from a fine-tuned model\n",
      "25 --------------------------------------------------------------------------------\n",
      "34 Continue fine-tuning from a fine-tuned model\n",
      "If you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch.\n",
      "To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft-<org>-<date>). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.\n",
      "Weights & Biases\n",
      "You can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets.\n",
      "To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run:\n",
      "pip install --upgrade openai wandb\n",
      "To sync your fine-tunes with Weights & Biases, run:\n",
      "25 --------------------------------------------------------------------------------\n",
      "35 You can read the Weights & Biases documentation for more information on this integration.\n",
      "Example notebooks\n",
      "finetuning-classification.ipynb\n",
      "This notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook:\n",
      "Data exploration will give an overview of the data source and what an example looks like\n",
      "Data preparation will turn our data source into a jsonl file that can be used for fine-tuning\n",
      "Fine-tuning will kick off the fine-tuning job and explain the resulting model's performance\n",
      "Using the model will demonstrate making requests to the fine-tuned model to get predictions.\n",
      "25 --------------------------------------------------------------------------------\n",
      "36 The idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.\n",
      "To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".\n",
      "We will perform this task in three notebooks:\n",
      "25 --------------------------------------------------------------------------------\n",
      "37 We will perform this task in three notebooks:\n",
      "The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.\n",
      "The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.\n",
      "The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.\n",
      "25 --------------------------------------------------------------------------------\n",
      "0 Page not found\n",
      "We couldn't find the page you were looking for.\n",
      "Was this expected?\n",
      "26 --------------------------------------------------------------------------------\n",
      "0 How to build an AI that can answer questions about your website\n",
      "This tutorial walks through a simple example of crawling a website (in this example, the OpenAI website), turning the crawled pages into embeddings using the Embeddings API, and then creating a basic search functionality that allows a user to ask questions about the embedded information. This is intended to be a starting point for more sophisticated applications that make use of custom knowledge bases.\n",
      "Getting started\n",
      "Some basic knowledge of Python and GitHub is helpful for this tutorial. Before diving in, make sure to set up an OpenAI API key and walk through the quickstart tutorial. This will give a good intuition on how to use the API to its full potential.\n",
      "Python is used as the main programming language along with the OpenAI, Pandas, transformers, NumPy, and other popular packages. If you run into any issues working through this tutorial, please ask a question on the OpenAI Community Forum.\n",
      "27 --------------------------------------------------------------------------------\n",
      "1 To start with the code, clone the full code for this tutorial on GitHub. Alternatively, follow along and copy each section into a Jupyter notebook and run the code step by step, or just read along. A good way to avoid any issues is to set up a new virtual environment and install the required packages by running the following commands:\n",
      "1 2 3 4 5 python -m venv env source env/bin/activate pip install -r requirements.txt\n",
      "Setting up a web crawler\n",
      "The primary focus of this tutorial is the OpenAI API so if you prefer, you can skip the context on how to create a web crawler and just download the source code. Otherwise, expand the section below to work through the scraping mechanism implementation.\n",
      "Learn how to build a web crawler\n",
      "Acquiring data in text form is the first step to use embeddings. This tutorial creates a new set of data by crawling the OpenAI website, a technique that you can also use for your own company or personal website.\n",
      "27 --------------------------------------------------------------------------------\n",
      "2 While this crawler is written from scratch, open source packages like Scrapy can also help with these operations.\n",
      "This crawler will start from the root URL passed in at the bottom of the code below, visit each page, find additional links, and visit those pages as well (as long as they have the same root domain). To begin, import the required packages, set up the basic URL, and define a HTMLParser class.\n",
      "27 --------------------------------------------------------------------------------\n",
      "3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import requests import re import urllib.request from bs4 import BeautifulSoup from collections import deque from html.parser import HTMLParser from urllib.parse import urlparse import os # Regex pattern to match a URL HTTP_URL_PATTERN = r'^http[s]*://.+' domain = \"openai.com\" # <- put your domain to be crawled full_url = \"https://openai.com/\" # <- put your domain to be crawled with https or http # Create a class to parse the HTML and get the hyperlinks class HyperlinkParser(HTMLParser): def __init__(self): super().__init__() # Create a list to store the hyperlinks self.hyperlinks = [] # Override the HTMLParser's handle_starttag method to get the hyperlinks def handle_starttag(self, tag, attrs): attrs = dict(attrs) # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks if tag == \"a\" and \"href\" in attrs: self.hyperlinks.append(attrs[\"href\"])\n",
      "27 --------------------------------------------------------------------------------\n",
      "4 The next function takes a URL as an argument, opens the URL, and reads the HTML content. Then, it returns all the hyperlinks found on that page.\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Function to get the hyperlinks from a URL def get_hyperlinks(url): # Try to open the URL and read the HTML try: # Open the URL and read the HTML with urllib.request.urlopen(url) as response: # If the response is not HTML, return an empty list if not response.info().get('Content-Type').startswith(\"text/html\"): return [] # Decode the HTML html = response.read().decode('utf-8') except Exception as e: print(e) return [] # Create the HTML Parser and then Parse the HTML to get hyperlinks parser = HyperlinkParser() parser.feed(html) return parser.hyperlinks\n",
      "The goal is to crawl through and index only the content that lives under the OpenAI domain. For this purpose, a function that calls the get_hyperlinks function but filters out any URLs that are not part of the specified domain is needed.\n",
      "27 --------------------------------------------------------------------------------\n",
      "5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Function to get the hyperlinks from a URL that are within the same domain def get_domain_hyperlinks(local_domain, url): clean_links = [] for link in set(get_hyperlinks(url)): clean_link = None # If the link is a URL, check if it is within the same domain if re.search(HTTP_URL_PATTERN, link): # Parse the URL and check if the domain is the same url_obj = urlparse(link) if url_obj.netloc == local_domain: clean_link = link # If the link is not a URL, check if it is a relative link else: if link.startswith(\"/\"): link = link[1:] elif link.startswith(\"#\") or link.startswith(\"mailto:\"): continue clean_link = \"https://\" + local_domain + \"/\" + link if clean_link is not None: if clean_link.endswith(\"/\"): clean_link = clean_link[:-1] clean_links.append(clean_link) # Return the list of hyperlinks that are within the same domain return list(set(clean_links))\n",
      "27 --------------------------------------------------------------------------------\n",
      "6 The crawl function is the final step in the web scraping task setup. It keeps track of the visited URLs to avoid repeating the same page, which might be linked across multiple pages on a site. It also extracts the raw text from a page without the HTML tags, and writes the text content into a local .txt file specific to the page.\n",
      "27 --------------------------------------------------------------------------------\n",
      "7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def crawl(url): # Parse the URL and get the domain local_domain = urlparse(url).netloc # Create a queue to store the URLs to crawl queue = deque([url]) # Create a set to store the URLs that have already been seen (no duplicates) seen = set([url]) # Create a directory to store the text files if not os.path.exists(\"text/\"): os.mkdir(\"text/\") if not os.path.exists(\"text/\"+local_domain+\"/\"): os.mkdir(\"text/\" + local_domain + \"/\") # Create a directory to store the csv files if not os.path.exists(\"processed\"): os.mkdir(\"processed\") # While the queue is not empty, continue crawling while queue: # Get the next URL from the queue url = queue.pop() print(url) # for debugging and to see the progress # Save text from the url to a <url>.txt file with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f: # Get the text from the URL using\n",
      "27 --------------------------------------------------------------------------------\n",
      "8 as f: # Get the text from the URL using BeautifulSoup soup = BeautifulSoup(requests.get(url).text, \"html.parser\") # Get the text but remove the tags text = soup.get_text() # If the crawler gets to a page that requires JavaScript, it will stop the crawl if (\"You need to enable JavaScript to run this app.\" in text): print(\"Unable to parse page \" + url + \" due to JavaScript being required\") # Otherwise, write the text to the file in the text directory f.write(text) # Get the hyperlinks from the URL and add them to the queue for link in get_domain_hyperlinks(local_domain, url): if link not in seen: queue.append(link) seen.add(link) crawl(full_url)\n",
      "27 --------------------------------------------------------------------------------\n",
      "9 The last line of the above example runs the crawler which goes through all the accessible links and turns those pages into text files. This will take a few minutes to run depending on the size and complexity of your site.\n",
      "Building an embeddings index\n",
      "CSV is a common format for storing embeddings. You can use this format with Python by converting the raw text files (which are in the text directory) into Pandas data frames. Pandas is a popular open source library that helps you work with tabular data (data stored in rows and columns).\n",
      "Blank empty lines can clutter the text files and make them harder to process. A simple function can remove those lines and tidy up the files.\n",
      "1 2 3 4 5 6 def remove_newlines(serie): serie = serie.str.replace('\\n', ' ') serie = serie.str.replace('\\\\n', ' ') serie = serie.str.replace(' ', ' ') serie = serie.str.replace(' ', ' ') return serie\n",
      "27 --------------------------------------------------------------------------------\n",
      "10 Converting the text to CSV requires looping through the text files in the text directory created earlier. After opening each file, remove the extra spacing and append the modified text to a list. Then, add the text with the new lines removed to an empty Pandas data frame and write the data frame to a CSV file.\n",
      "Extra spacing and new lines can clutter the text and complicate the embeddings process. The code used here helps to remove some of them but you may find 3rd party libraries or other methods useful to get rid of more unnecessary characters.\n",
      "27 --------------------------------------------------------------------------------\n",
      "11 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import pandas as pd # Create a list to store the text files texts=[] # Get all the text files in the text directory for file in os.listdir(\"text/\" + domain + \"/\"): # Open the file and read the text with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f: text = f.read() # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces. texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text)) # Create a dataframe from the list of texts df = pd.DataFrame(texts, columns = ['fname', 'text']) # Set the text column to be the raw text with the newlines removed df['text'] = df.fname + \". \" + remove_newlines(df.text) df.to_csv('processed/scraped.csv') df.head()\n",
      "27 --------------------------------------------------------------------------------\n",
      "12 Tokenization is the next step after saving the raw text into a CSV file. This process splits the input text into tokens by breaking down the sentences and words. A visual demonstration of this can be seen by checking out our Tokenizer in the docs.\n",
      "A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words).\n",
      "The API has a limit on the maximum number of input tokens for embeddings. To stay below the limit, the text in the CSV file needs to be broken down into multiple rows. The existing length of each row will be recorded first to identify which rows need to be split.\n",
      "27 --------------------------------------------------------------------------------\n",
      "13 1 2 3 4 5 6 7 8 9 10 11 12 13 import tiktoken # Load the cl100k_base tokenizer which is designed to work with the ada-002 model tokenizer = tiktoken.get_encoding(\"cl100k_base\") df = pd.read_csv('processed/scraped.csv', index_col=0) df.columns = ['title', 'text'] # Tokenize the text and save the number of tokens to a new column df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x))) # Visualize the distribution of the number of tokens per row using a histogram df.n_tokens.hist()\n",
      "The newest embeddings model can handle inputs with up to 8191 input tokens so most of the rows would not need any chunking, but this may not be the case for every subpage scraped so the next code chunk will split the longer lines into smaller chunks.\n",
      "27 --------------------------------------------------------------------------------\n",
      "14 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 max_tokens = 500 # Function to split the text into chunks of a maximum number of tokens def split_into_many(text, max_tokens = max_tokens): # Split the text into sentences sentences = text.split('. ') # Get the number of tokens for each sentence n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences] chunks = [] tokens_so_far = 0 chunk = [] # Loop through the sentences and tokens joined together in a tuple for sentence, token in zip(sentences, n_tokens): # If the number of tokens so far plus the number of tokens in the current sentence is greater # than the max number of tokens, then add the chunk to the list of chunks and reset # the chunk and tokens so far if tokens_so_far + token > max_tokens: chunks.append(\". \".join(chunk) + \".\") chunk = [] tokens_so_far = 0 # If the number of tokens in the current sentence is greater than the max\n",
      "27 --------------------------------------------------------------------------------\n",
      "15 in the current sentence is greater than the max number of # tokens, go to the next sentence if token > max_tokens: continue # Otherwise, add the sentence to the chunk and add the number of tokens to the total chunk.append(sentence) tokens_so_far += token + 1 return chunks shortened = [] # Loop through the dataframe for row in df.iterrows(): # If the text is None, go to the next row if row[1]['text'] is None: continue # If the number of tokens is greater than the max number of tokens, split the text into chunks if row[1]['n_tokens'] > max_tokens: shortened += split_into_many(row[1]['text']) # Otherwise, add the text to the list of shortened texts else: shortened.append( row[1]['text'] )\n",
      "27 --------------------------------------------------------------------------------\n",
      "16 Visualizing the updated histogram again can help to confirm if the rows were successfully split into shortened sections.\n",
      "1 2 3 df = pd.DataFrame(shortened, columns = ['text']) df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x))) df.n_tokens.hist()\n",
      "The content is now broken down into smaller chunks and a simple request can be sent to the OpenAI API specifying the use of the new text-embedding-ada-002 model to create the embeddings:\n",
      "1 2 3 4 5 6 import openai df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding']) df.to_csv('processed/embeddings.csv') df.head()\n",
      "This should take about 3-5 minutes but after you will have your embeddings ready to use!\n",
      "Building a question answer system with your embeddings\n",
      "27 --------------------------------------------------------------------------------\n",
      "17 The embeddings are ready and the final step of this process is to create a simple question and answer system. This will take a user's question, create an embedding of it, and compare it with the existing embeddings to retrieve the most relevant text from the scraped website. The gpt-3.5-turbo-instruct model will then generate a natural sounding answer based on the retrieved text.\n",
      "Turning the embeddings into a NumPy array is the first step, which will provide more flexibility in how to use it given the many functions available that operate on NumPy arrays. It will also flatten the dimension to 1-D, which is the required format for many subsequent operations.\n",
      "1 2 3 4 5 6 7 import numpy as np from openai.embeddings_utils import distances_from_embeddings df=pd.read_csv('processed/embeddings.csv', index_col=0) df['embeddings'] = df['embeddings'].apply(eval).apply(np.array) df.head()\n",
      "27 --------------------------------------------------------------------------------\n",
      "18 The question needs to be converted to an embedding with a simple function, now that the data is ready. This is important because the search with embeddings compares the vector of numbers (which was the conversion of the raw text) using cosine distance. The vectors are likely related and might be the answer to the question if they are close in cosine distance. The OpenAI python package has a built in distances_from_embeddings function which is useful here.\n",
      "27 --------------------------------------------------------------------------------\n",
      "19 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def create_context( question, df, max_len=1800, size=\"ada\" ): \"\"\" Create a context for a question by finding the most similar context from the dataframe \"\"\" # Get the embeddings for the question q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding'] # Get the distances from the embeddings df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine') returns = [] cur_len = 0 # Sort by distance and add the text to the context until the context is too long for i, row in df.sort_values('distances', ascending=True).iterrows(): # Add the length of the text to the current length cur_len += row['n_tokens'] + 4 # If the context is too long, break if cur_len > max_len: break # Else add it to the text that is being returned returns.append(row[\"text\"]) # Return the context return \"\\n\\n###\\n\\n\".join(returns)\n",
      "27 --------------------------------------------------------------------------------\n",
      "20 The text was broken up into smaller sets of tokens, so looping through in ascending order and continuing to add the text is a critical step to ensure a full answer. The max_len can also be modified to something smaller, if more content than desired is returned.\n",
      "The previous step only retrieved chunks of texts that are semantically related to the question, so they might contain the answer, but there's no guarantee of it. The chance of finding an answer can be further increased by returning the top 5 most likely results.\n",
      "The answering prompt will then try to extract the relevant facts from the retrieved contexts, in order to formulate a coherent answer. If there is no relevant answer, the prompt will return “I don’t know”.\n",
      "A realistic sounding answer to the question can be created with the completion endpoint using gpt-3.5-turbo-instruct.\n",
      "27 --------------------------------------------------------------------------------\n",
      "21 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def answer_question( df, model=\"gpt-3.5-turbo-instruct\", question=\"Am I allowed to publish model outputs to Twitter, without a human review?\", max_len=1800, size=\"ada\", debug=False, max_tokens=150, stop_sequence=None ): \"\"\" Answer a question based on the most similar context from the dataframe texts \"\"\" context = create_context( question, df, max_len=max_len, size=size, ) # If debug, print the raw model response if debug: print(\"Context:\\n\" + context) print(\"\\n\\n\") try: # Create a completions using the question and context response = openai.Completion.create( prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\", temperature=0, max_tokens=max_tokens, top_p=1, frequency_penalty=0, presence_penalty=0, stop=stop_sequence, model=model, ) return\n",
      "27 --------------------------------------------------------------------------------\n",
      "22 stop=stop_sequence, model=model, ) return response[\"choices\"][0][\"text\"].strip() except Exception as e: print(e) return \"\"\n",
      "27 --------------------------------------------------------------------------------\n",
      "23 It is done! A working Q/A system that has the knowledge embedded from the OpenAI website is now ready. A few quick tests can be done to see the quality of the output:\n",
      "1 2 3 4 5 answer_question(df, question=\"What day is it?\", debug=False) answer_question(df, question=\"What is our newest embeddings model?\") answer_question(df, question=\"What is ChatGPT?\")\n",
      "The responses will look something like the following:\n",
      "1 2 3 4 5 \"I don't know.\" 'The newest embeddings model is text-embedding-ada-002.' 'ChatGPT is a model trained to interact in a conversational way. It is able to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.'\n",
      "27 --------------------------------------------------------------------------------\n",
      "24 If the system is not able to answer a question that is expected, it is worth searching through the raw text files to see if the information that is expected to be known actually ended up being embedded or not. The crawling process that was done initially was setup to skip sites outside the original domain that was provided, so it might not have that knowledge if there was a subdomain setup.\n",
      "Currently, the dataframe is being passed in each time to answer a question. For more production workflows, a vector database solution should be used instead of storing the embeddings in a CSV file, but the current approach is a great option for prototyping.\n",
      "27 --------------------------------------------------------------------------------\n",
      "0 In this tutorial, we'll harness the power of OpenAI's Whisper and GPT-4 models to develop an automated meeting minutes generator. The application transcribes audio from a meeting, provides a summary of the discussion, extracts key points and action items, and performs a sentiment analysis.\n",
      "Getting started\n",
      "This tutorial assumes a basic understanding of Python and an OpenAI API key. You can use the audio file provided with this tutorial or your own.\n",
      "Additionally, you will need to install the python-docx and OpenAI libraries. You can create a new Python environment and install the required packages with the following commands:\n",
      "1 2 3 4 5 6 python -m venv env source env/bin/activate pip install openai pip install python-docx\n",
      "Transcribing audio with Whisper\n",
      "28 --------------------------------------------------------------------------------\n",
      "1 Transcribing audio with Whisper\n",
      "The first step in transcribing the audio from a meeting is to pass the audio file of the meeting into our /v1/audio API. Whisper, the model that powers the audio API, is capable of converting spoken language into written text. To start, we will avoid passing a prompt or temperature (optional parameters to control the model's output) and stick with the default values.\n",
      "Next, we import the required packages and define a function that uses the Whisper model to take in the audio file and transcribe it:\n",
      "1 2 3 4 5 6 7 import openai from docx import Document def transcribe_audio(audio_file_path): with open(audio_file_path, 'rb') as audio_file: transcription = openai.Audio.transcribe(\"whisper-1\", audio_file) return transcription['text']\n",
      "28 --------------------------------------------------------------------------------\n",
      "2 In this function, audio_file_path is the path to the audio file you want to transcribe. The function opens this file and passes it to the Whisper ASR model (whisper-1) for transcription. The result is returned as raw text. It’s important to note that the openai.Audio.transcribe function requires the actual audio file to be passed in, not just the path to the file locally or on a remote server. This means that if you are running this code on a server where you might not also be storing your audio files, you will need to have a preprocess step that first downloads the audio files onto that device.\n",
      "Summarizing and analyzing the transcript with GPT-4\n",
      "Having obtained the transcript, we now pass it to GPT-4 via the Chat completions API. GPT-4 is OpenAI's state-of-the-art large language model which we'll use to generate a summary, extract key points, action items, and perform sentiment analysis.\n",
      "28 --------------------------------------------------------------------------------\n",
      "3 This tutorial uses distinct functions for each task we want GPT-4 to perform. This is not the most efficient way to do this task - you can put these instructions into one function, however, splitting them up can lead to higher quality summarization.\n",
      "To split the tasks up, we define the meeting_minutes function which will serve as the main function of this application:\n",
      "1 2 3 4 5 6 7 8 9 10 11 def meeting_minutes(transcription): abstract_summary = abstract_summary_extraction(transcription) key_points = key_points_extraction(transcription) action_items = action_item_extraction(transcription) sentiment = sentiment_analysis(transcription) return { 'abstract_summary': abstract_summary, 'key_points': key_points, 'action_items': action_items, 'sentiment': sentiment }\n",
      "28 --------------------------------------------------------------------------------\n",
      "4 In this function, transcription is the text we obtained from Whisper. The transcription can be passed to the four other functions, each designed to perform a specific task: abstract_summary_extraction generates a summary of the meeting, key_points_extraction extracts the main points, action_item_extraction identifies the action items, and sentiment_analysis performs a sentiment analysis. If there are other capabilities you want, you can add those in as well using the same framework shown above.\n",
      "Here is how each of these functions works:\n",
      "Summary extraction\n",
      "28 --------------------------------------------------------------------------------\n",
      "5 Summary extraction\n",
      "The abstract_summary_extraction function takes the transcription and summarizes it into a concise abstract paragraph with the aim to retain the most important points while avoiding unnecessary details or tangential points. The main mechanism to enable this process is the system message as shown below. There are many different possible ways of achieving similar results through the process commonly referred to as prompt engineering. You can read our GPT best practices guide which gives in depth advice on how to do this most effectively.\n",
      "28 --------------------------------------------------------------------------------\n",
      "6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def abstract_summary_extraction(transcription): response = openai.ChatCompletion.create( model=\"gpt-4\", temperature=0, messages=[ { \"role\": \"system\", \"content\": \"You are a highly skilled AI trained in language comprehension and summarization. I would like you to read the following text and summarize it into a concise abstract paragraph. Aim to retain the most important points, providing a coherent and readable summary that could help a person understand the main points of the discussion without needing to read the entire text. Please avoid unnecessary details or tangential points.\" }, { \"role\": \"user\", \"content\": transcription } ] ) return response['choices'][0]['message']['content']\n",
      "Key points extraction\n",
      "28 --------------------------------------------------------------------------------\n",
      "7 Key points extraction\n",
      "The key_points_extraction function identifies and lists the main points discussed in the meeting. These points should represent the most important ideas, findings, or topics crucial to the essence of the discussion. Again, the main mechanism for controlling the way these points are identified is the system message. You might want to give some additional context here around the way your project or company runs such as “We are a company that sells race cars to consumers. We do XYZ with the goal of XYZ”. This additional context could dramatically improve the models ability to extract information that is relevant.\n",
      "28 --------------------------------------------------------------------------------\n",
      "8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def key_points_extraction(transcription): response = openai.ChatCompletion.create( model=\"gpt-4\", temperature=0, messages=[ { \"role\": \"system\", \"content\": \"You are a proficient AI with a specialty in distilling information into key points. Based on the following text, identify and list the main points that were discussed or brought up. These should be the most important ideas, findings, or topics that are crucial to the essence of the discussion. Your goal is to provide a list that someone could read to quickly understand what was talked about.\" }, { \"role\": \"user\", \"content\": transcription } ] ) return response['choices'][0]['message']['content']\n",
      "Action item extraction\n",
      "28 --------------------------------------------------------------------------------\n",
      "9 Action item extraction\n",
      "The action_item_extraction function identifies tasks, assignments, or actions agreed upon or mentioned during the meeting. These could be tasks assigned to specific individuals or general actions the group decided to take. While not covered in this tutorial, the Chat completions API provides a function calling capability which would allow you to build in the ability to automatically create tasks in your task management software and assign it to the relevant person.\n",
      "28 --------------------------------------------------------------------------------\n",
      "10 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def action_item_extraction(transcription): response = openai.ChatCompletion.create( model=\"gpt-4\", temperature=0, messages=[ { \"role\": \"system\", \"content\": \"You are an AI expert in analyzing conversations and extracting action items. Please review the text and identify any tasks, assignments, or actions that were agreed upon or mentioned as needing to be done. These could be tasks assigned to specific individuals, or general actions that the group has decided to take. Please list these action items clearly and concisely.\" }, { \"role\": \"user\", \"content\": transcription } ] ) return response['choices'][0]['message']['content']\n",
      "Sentiment analysis\n",
      "28 --------------------------------------------------------------------------------\n",
      "11 Sentiment analysis\n",
      "The sentiment_analysis function analyzes the overall sentiment of the discussion. It considers the tone, the emotions conveyed by the language used, and the context in which words and phrases are used. For tasks which are less complicated, it may also be worthwhile to try out gpt-3.5-turbo in addition to gpt-4 to see if you can get a similar level of performance. It might also be useful to experiment with taking the results of the sentiment_analysis function and passing it to the other functions to see how having the sentiment of the conversation impacts the other attributes.\n",
      "28 --------------------------------------------------------------------------------\n",
      "12 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def sentiment_analysis(transcription): response = openai.ChatCompletion.create( model=\"gpt-4\", temperature=0, messages=[ { \"role\": \"system\", \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following text. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Indicate whether the sentiment is generally positive, negative, or neutral, and provide brief explanations for your analysis where possible.\" }, { \"role\": \"user\", \"content\": transcription } ] ) return response['choices'][0]['message']['content']\n",
      "Exporting meeting minutes\n",
      "28 --------------------------------------------------------------------------------\n",
      "13 Exporting meeting minutes\n",
      "Once we've generated the meeting minutes, it's beneficial to save them into a readable format that can be easily distributed. One common format for such reports is Microsoft Word. The Python docx library is a popular open source library for creating Word documents. If you wanted to build an end-to-end meeting minute application, you might consider removing this export step in favor of sending the summary inline as an email followup.\n",
      "To handle the exporting process, define a function save_as_docx that converts the raw text to a Word document:\n",
      "1 2 3 4 5 6 7 8 9 10 def save_as_docx(minutes, filename): doc = Document() for key, value in minutes.items(): # Replace underscores with spaces and capitalize each word for the heading heading = ' '.join(word.capitalize() for word in key.split('_')) doc.add_heading(heading, level=1) doc.add_paragraph(value) # Add a line break between sections doc.add_paragraph() doc.save(filename)\n",
      "28 --------------------------------------------------------------------------------\n",
      "14 In this function, minutes is a dictionary containing the abstract summary, key points, action items, and sentiment analysis from the meeting. Filename is the name of the Word document file to be created. The function creates a new Word document, adds headings and content for each part of the minutes, and then saves the document to the current working directory.\n",
      "Finally, you can put it all together and generate the meeting minutes from an audio file:\n",
      "1 2 3 4 5 6 audio_file_path = \"Earningscall.wav\" transcription = transcribe_audio(audio_file_path) minutes = meeting_minutes(transcription) print(minutes) save_as_docx(minutes, 'meeting_minutes.docx')\n",
      "This code will transcribe the audio file Earningscall.wav, generates the meeting minutes, prints them, and then saves them into a Word document called meeting_minutes.docx.\n",
      "Now that you have the basic meeting minutes processing setup, consider trying to optimize the performance with prompt engineering or build an end-to-end system with native function calling.\n",
      "28 --------------------------------------------------------------------------------\n",
      "0 Supported countries and territories\n",
      "We’re proud to be able to provide API access to many countries, regions, and territories, and are always adding more. \n",
      "Geographic diversity and broadly distributed benefits are very important to us and we are working hard to increase the number of locations we can provide safe access to. If your location does not currently support API access, please check back for updates at a later date.\n",
      "Here are the countries, regions, and territories we can currently support access from:\n",
      "Albania\n",
      "Algeria\n",
      "Andorra\n",
      "Angola\n",
      "Antigua and Barbuda\n",
      "Argentina\n",
      "Armenia\n",
      "Australia\n",
      "Austria\n",
      "Azerbaijan\n",
      "Bahamas\n",
      "Bangladesh\n",
      "Barbados\n",
      "Belgium\n",
      "Belize\n",
      "Benin\n",
      "Bhutan\n",
      "Bolivia\n",
      "Bosnia and Herzegovina\n",
      "Botswana\n",
      "Brazil\n",
      "Brunei\n",
      "Bulgaria\n",
      "Burkina Faso\n",
      "Cabo Verde\n",
      "Canada\n",
      "Chile\n",
      "Colombia\n",
      "Comoros\n",
      "Congo (Congo-Brazzaville)\n",
      "Costa Rica\n",
      "Côte d'Ivoire\n",
      "Croatia\n",
      "Cyprus\n",
      "Czechia (Czech Republic)\n",
      "Denmark\n",
      "Djibouti\n",
      "Dominica\n",
      "Dominican Republic\n",
      "Ecuador\n",
      "El Salvador\n",
      "Estonia\n",
      "Fiji\n",
      "Finland\n",
      "France\n",
      "Gabon\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n",
      "Greece\n",
      "Grenada\n",
      "29 --------------------------------------------------------------------------------\n",
      "1 Gabon\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n",
      "Greece\n",
      "Grenada\n",
      "Guatemala\n",
      "Guinea\n",
      "Guinea-Bissau\n",
      "Guyana\n",
      "Haiti\n",
      "Holy See (Vatican City)\n",
      "Honduras\n",
      "Hungary\n",
      "Iceland\n",
      "India\n",
      "Indonesia\n",
      "Iraq\n",
      "Ireland\n",
      "Israel\n",
      "Italy\n",
      "Jamaica\n",
      "Japan\n",
      "Jordan\n",
      "Kazakhstan\n",
      "Kenya\n",
      "Kiribati\n",
      "Kuwait\n",
      "Kyrgyzstan\n",
      "Latvia\n",
      "Lebanon\n",
      "Lesotho\n",
      "Liberia\n",
      "Liechtenstein\n",
      "Lithuania\n",
      "Luxembourg\n",
      "Madagascar\n",
      "Malawi\n",
      "Malaysia\n",
      "Maldives\n",
      "Mali\n",
      "Malta\n",
      "Marshall Islands\n",
      "Mauritania\n",
      "Mauritius\n",
      "Mexico\n",
      "Micronesia\n",
      "Moldova\n",
      "Monaco\n",
      "Mongolia\n",
      "Montenegro\n",
      "Morocco\n",
      "Mozambique\n",
      "Myanmar\n",
      "Namibia\n",
      "Nauru\n",
      "Nepal\n",
      "Netherlands\n",
      "New Zealand\n",
      "Nicaragua\n",
      "Niger\n",
      "Nigeria\n",
      "North Macedonia\n",
      "Norway\n",
      "Oman\n",
      "Pakistan\n",
      "Palau\n",
      "Palestine\n",
      "Panama\n",
      "Papua New Guinea\n",
      "Paraguay\n",
      "Peru\n",
      "Philippines\n",
      "Poland\n",
      "Portugal\n",
      "Qatar\n",
      "Romania\n",
      "Rwanda\n",
      "Saint Kitts and Nevis\n",
      "Saint Lucia\n",
      "Saint Vincent and the Grenadines\n",
      "Samoa\n",
      "San Marino\n",
      "Sao Tome and Principe\n",
      "Saudi Arabia\n",
      "Senegal\n",
      "Serbia\n",
      "Seychelles\n",
      "Sierra Leone\n",
      "Singapore\n",
      "Slovakia\n",
      "Slovenia\n",
      "Solomon Islands\n",
      "South Africa\n",
      "South Korea\n",
      "Spain\n",
      "Sri Lanka\n",
      "Suriname\n",
      "Sweden\n",
      "Switzerland\n",
      "Taiwan\n",
      "Tanzania\n",
      "Thailand\n",
      "Timor-Leste (East Timor)\n",
      "Togo\n",
      "Tonga\n",
      "29 --------------------------------------------------------------------------------\n",
      "2 Thailand\n",
      "Timor-Leste (East Timor)\n",
      "Togo\n",
      "Tonga\n",
      "Trinidad and Tobago\n",
      "Tunisia\n",
      "Turkey\n",
      "Tuvalu\n",
      "Uganda\n",
      "Ukraine (with certain exceptions)\n",
      "United Arab Emirates\n",
      "United Kingdom\n",
      "United States of America\n",
      "Uruguay\n",
      "Vanuatu\n",
      "Zambia\n",
      "29 --------------------------------------------------------------------------------\n",
      "0 ChatGPT-User\n",
      "ChatGPT-User is used by plugins in ChatGPT. This user-agent will only be used to take direct actions on behalf of ChatGPT users and is not used for crawling the web in any automatic fashion.\n",
      "User agent token: ChatGPT-User\n",
      "Full user-agent string: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot\n",
      "To allow plugins to access your site you can explicitly add the ChatGPT-User to your site’s robots.txt:\n",
      "User-agent: ChatGPT-User Disallow:\n",
      "To allow plugins to access your only parts of your site you can add the ChatGPT-User to your site’s robots.txt like this:\n",
      "1 2 3 4 User-agent: ChatGPT-User Disallow: Allow: /directory-1/ Allow: /directory-2/\n",
      "To disallow plugins to access your site you can add the ChatGPT-User to your site’s robots.txt:\n",
      "User-agent: ChatGPT-User Disallow: /\n",
      "IP egress ranges\n",
      "For OpenAI's web browsing plugin, calls to websites will be made from the 23.98.142.176/28 IP address block.\n",
      "Usage\n",
      "30 --------------------------------------------------------------------------------\n",
      "1 Usage\n",
      "Web pages crawled with the ChatGPT-User/1.0 user agent is primarily used to answer a live query from a user that submits a question or prompt to ChatGPT through its browsing feature. When content is retrieved from web pages by our user agent, it is used by our models to provide an answer to the user, our models will include a link to the websource. ChatGPTis trained to not repeat information verbatim from the data (although this is not perfect), but it will paraphrase, translate, summarize, and abstract information as requested by the user.\n",
      "Retrieved content is only used in the training process to teach our models how to respond to a user request given this content (i.e., to make our models better at browsing), not to make our models better at creating responses. The content may be seen by OpenAI employees and trainers who are improving our models. Retrieved content is not used to train our models to be able to reproduce the content verbatim or to build a search engine.\n",
      "30 --------------------------------------------------------------------------------\n",
      "2 We fetch the data on nearly every user request, using caching to reduce load, striving to show the freshest content to users. The data is stored for training purposes as described above and so users can access the conversation history in their accounts.\n",
      "ChatGPT-User and GPTBot\n",
      "OpenAI has two separate user agents for web crawling and user browsing, so you know which use-case a given request is for. Our opt-out system currently treats both user agents the same, so any robots.txt disallow for one agent will cover both. Read more about GPTBot here.\n",
      "30 --------------------------------------------------------------------------------\n",
      "0 GPTBot\n",
      "GPTBot is OpenAI’s web crawler and can be identified by the following user agent and string.\n",
      "User agent token: GPTBot Full user-agent string: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0; +https://openai.com/gptbot)\n",
      "Usage\n",
      "Web pages crawled with the GPTBot user agent may potentially be used to improve future models and are filtered to remove sources that require paywall access, are known to primarily aggregate personally identifiable information (PII), or have text that violates our policies. Allowing GPTBot to access your site can help AI models become more accurate and improve their general capabilities and safety. Below, we also share how to disallow GPTBot from accessing your site.\n",
      "Disallowing GPTBot\n",
      "To disallow GPTBot to access your site you can add the GPTBot to your site’s robots.txt:\n",
      "User-agent: GPTBot Disallow: /\n",
      "Customize GPTBot access\n",
      "To allow GPTBot to access only parts of your site you can add the GPTBot token to your site’s robots.txt like this:\n",
      "31 --------------------------------------------------------------------------------\n",
      "1 1 2 3 User-agent: GPTBot Allow: /directory-1/ Disallow: /directory-2/\n",
      "GPTBot and ChatGPT-User\n",
      "OpenAI has two separate user agents for web crawling and user browsing, so you know which use-case a given request is for. Our opt-out system currently treats both user agents the same, so any robots.txt disallow for one agent will cover both. Read more about ChatGPT-User here.\n",
      "IP egress ranges\n",
      "For OpenAI's crawler, calls to websites will be made from the IP address block documented on the OpenAI website.\n",
      "31 --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text_embedding = []\n",
    "metadata_vectordb = []\n",
    "\n",
    "#chunking strategy\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,  # Set your desired chunk size\n",
    "        chunk_overlap=50  # Set your desired overlap size\n",
    "    )\n",
    "\n",
    "for i in range(len(openai_docs)):\n",
    "    # Split the text content into chunks using the text splitter\n",
    "    chunks = text_splitter.split_text(openai_docs.loc[i,\"text\"])\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        #creating metadata to store it in the vector database\n",
    "        metadata = {\"Chunk_Text\": chunk, \"URL\":openai_docs.loc[i,\"url\"], \"Title\":openai_docs.loc[i,\"metadata/title\"]}\n",
    "        metadata_vectordb.append(metadata)\n",
    "        print(j,chunk)        \n",
    "        #get the embeddings for the chunk\n",
    "        text_embedding.append(get_embedding(chunk))\n",
    "        print(i,\"--\"*40)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "90bd5c44-e872-43e4-b221-33165419960e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fdbd1346-7181-493b-9877-ff07a9bb2906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 455 entries, 0 to 454\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Chunk_Text       455 non-null    object\n",
      " 1   URL              455 non-null    object\n",
      " 2   Title            455 non-null    object\n",
      " 3   Text Embeddings  455 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 14.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert to a DataFrame\n",
    "search_data_DB_format = pd.DataFrame.from_dict(metadata_vectordb)\n",
    "search_data_DB_format[\"Text Embeddings\"] = text_embedding\n",
    "# Print the DataFrame\n",
    "print(search_data_DB_format.info())\n",
    "\n",
    "search_data_DB_format.to_csv(\"./data/data_stored_pinecone_OpenAI_1024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cddbda-f483-4141-8a32-228529f921d8",
   "metadata": {},
   "source": [
    "#### STORING THE DATA IN PINECONE DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8f4d929d-9204-4e8d-8efc-5764af33f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone (replace 'your-api-key' with your actual Pinecone API key)\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33f5706b-af93-4946-9ee6-e6f5d1e2dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Retrieve an Index (replace 'metadata_index' with your preferred index name)\n",
    "index_name = 'searchopenaidocs'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8961cb95-85e9-4ad3-9a4c-70c550464566",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(\"searchopenaidocs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a8f18421-700b-4a52-80f5-bd2d561c5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#InsertinG the embeddings along with its metadat into vector db\n",
    "start_idx = 0\n",
    "for i in range(len(text_embedding)):    \n",
    "    meta = {\"text\":str(metadata_vectordb[i])}\n",
    "    index.upsert(vectors= [(\"searchdocs_OpenAI{}\".format(start_idx), text_embedding[i], meta)])\n",
    "    start_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c47e2435-632b-4a73-9490-9ebbffac61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to search similar matchings in pinecone db\n",
    "def semantic_matching(text, topk): \n",
    "    embed = get_embedding(text)\n",
    "    response = index.query(vector=embed, top_k = topk, include_metadata=True)\n",
    "    return response\n",
    "\n",
    "# function to extract the response using llm chain\n",
    "def get_llm_response(llm_input):\n",
    "    template = f\"\"\" Extract the answer for the question from the given context: {llm_input} \n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"input_query\"])\n",
    "\n",
    "    repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\" # opens-source LLM\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_new_tokens\":2000})\n",
    "    \n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    overall_chain = SequentialChain(chains=[llm_chain], input_variables= [\"llm_input\"], verbose=True)\n",
    "    return overall_chain.run(\"llm_input\")\n",
    "\n",
    "#function to get the recommended questions\n",
    "def get_recommended_questions(query):\n",
    "    #loading the file which has the set of questions\n",
    "    loader = CSVLoader(file_path=\"./data/Questions_list.csv\")\n",
    "    data = loader.load()\n",
    "       \n",
    "    #Defining hugging face embeddings via langchain \n",
    "    embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "        api_key=os.getenv(\"HF_API\"), model_name=\"BAAI/bge-base-en-v1.5\"\n",
    "    )\n",
    "    \n",
    "    #Indexing and storing the data into FAISS\n",
    "    db = FAISS.from_documents(data, embeddings)\n",
    "    \n",
    "    #saving the data in local in which we can use it later \n",
    "    db.save_local(\"faiss_index_que_suggestion\")\n",
    "    \n",
    "    #Finding the similar match for the query\n",
    "    results = db.similarity_search(query, k=5)\n",
    "    print(\"--\"*70)\n",
    "    print(\"\\nRecommended Questions:\\n\")\n",
    "    for doc in results:\n",
    "        print(f\" {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea150b2-c7a8-4e6e-913c-bdd9c571d177",
   "metadata": {},
   "source": [
    "#### RETRIEVING ANSWER FOR THE GIVEN QUERY & QUESTION RECOMMENDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "13392227-4109-498e-bd65-0aac48400f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: Fine-tuning GPT models can make them better for specific applications, but it requires a careful investment of time and effort. The key reasons for using prompt engineering, prompt chaining (breaking complex tasks into multiple prompts), and function calling are that there are many tasks at which our models may not initially appear to perform well, but results can be improved with the right prompts, iterating over prompts and other tactics has a much faster feedback loop than iterating with fine-tuning, and initial prompt engineering work is not wasted - we typically see best results when using a good prompt in the fine-tuning data (or combining prompt chaining / tool use with fine-tuning).\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Recommended Questions:\n",
      "\n",
      " Questions: What are the best practices for maximizing the effectiveness of GPTs in different applications?\n",
      " Questions: How can I fine-tune my usage of GPT-4 to get superior results in my projects?\n",
      " Questions: What strategies and tactics can be used to improve the results obtained from GPT models?\n",
      " Questions: What are some practical ways to automate the evaluation of GPT model outputs?\n",
      " Questions: How can automated evaluations with objective criteria be employed in GPT system optimization?\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I improve the performance of GPTs in my tasks?\"\n",
    "response = semantic_matching(query, 5)\n",
    "content = 'content: '\n",
    "for match in response['matches']:\n",
    "    content+=match['metadata']['text']\n",
    "\n",
    "#combining semantic matching results and query\n",
    "input_query = f\"Question: {query}\\n Content: {content}\".replace(\"{\",'').replace(\"}\",'')\n",
    "\n",
    "print(get_llm_response(input_query))\n",
    "\n",
    "get_recommended_questions(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7331564f-a2fb-4cac-bc96-ebea5db80594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time. It helps protect against abuse or misuse of the API, ensures that everyone has fair access to the API, and helps prevent disruptions in service.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Recommended Questions:\n",
      "\n",
      " Questions: Why do APIs have rate limits, and what is their primary purpose?\n",
      " Questions: How do rate limits help protect against abuse and misuse of APIs?\n",
      " Questions: How can I effectively batch requests to make the most of API rate limits?\n",
      " Questions: What are some error mitigation strategies for handling rate limits in API requests?\n",
      " Questions: What happens if I exceed my API rate limit, and how can I avoid rate limit errors?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a rate limit in the context of APIs?\"\n",
    "response = semantic_matching(query, 5)\n",
    "content = 'content: '\n",
    "for match in response['matches']:\n",
    "    content+=match['metadata']['text']\n",
    "\n",
    "#combining semantic matching results and query\n",
    "input_query = f\"Question: {query}\\n Content: {content}\".replace(\"{\",'').replace(\"}\",'')\n",
    "\n",
    "print(get_llm_response(input_query))\n",
    "\n",
    "get_recommended_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "eb9dee6d-e572-4f66-a7a8-78a865a72483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: To disambiguate complex tasks for GPTs, you can specify the steps required to complete a task.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Recommended Questions:\n",
      "\n",
      " Questions: What are the best practices for maximizing the effectiveness of GPTs in different applications?\n",
      " Questions: What are some practical ways to automate the evaluation of GPT model outputs?\n",
      " Questions: How can I fine-tune my usage of GPT-4 to get superior results in my projects?\n",
      " Questions: What strategies and tactics can be used to improve the results obtained from GPT models?\n",
      " Questions: How can automated evaluations with objective criteria be employed in GPT system optimization?\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I disambiguate complex tasks for GPTs?\"\n",
    "response = semantic_matching(query, 5)\n",
    "content = 'content: '\n",
    "for match in response['matches']:\n",
    "    content+=match['metadata']['text']\n",
    "\n",
    "#combining semantic matching results and query\n",
    "input_query = f\"Question: {query}\\n Content: {content}\".replace(\"{\",'').replace(\"}\",'')\n",
    "print(get_llm_response(input_query))\n",
    "\n",
    "get_recommended_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f6bbfdf3-1a26-45a3-841f-de989d1d487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: ChatGPT plugins are third-party applications that connect ChatGPT to APIs defined by developers, enabling ChatGPT to interact with these APIs and perform a wide range of actions. These plugins can retrieve real-time information, knowledge-base information, and assist users with actions. They are in a beta and developer access may not be accessible to everyone. To connect a plugin via the ChatGPT UI, you can either run it locally in a development environment or on a remote server. If you have a local version of your API running, you can point the plugin interface to your localhost server. To register your plugin in the ChatGPT UI, you must manually activate it in the ChatGPT UI.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Recommended Questions:\n",
      "\n",
      " Questions: How can developers create plugins for ChatGPT to enhance its functionality?\n",
      " Questions: What types of actions can ChatGPT perform using plugins, and can you provide some examples?\n",
      " Questions: How do users activate and use plugins within the ChatGPT UI?\n",
      " Questions: What is the status of ChatGPT plugins during the beta phase, and how can developers gain access to them?\n",
      " Questions: How does ChatGPT consume the manifest file and make calls to the developer-defined APIs?\n"
     ]
    }
   ],
   "source": [
    "query = \"What are ChatGPT plugins, and how do they enable interactions with third-party applications?\"\n",
    "response = semantic_matching(query, 5)\n",
    "content = 'content: '\n",
    "for match in response['matches']:\n",
    "    content+=match['metadata']['text']\n",
    "\n",
    "#combining semantic matching results and query\n",
    "input_query = f\"Question: {query}\\n Content: {content}\".replace(\"{\",'').replace(\"}\",'')\n",
    "print(get_llm_response(input_query))\n",
    "\n",
    "get_recommended_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b970ff0d-2630-4305-abfd-67bafd4fdf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: To transcribe audio using the transcriptions endpoint of the Whisper API in Python, you can use the following code:\n",
      "\n",
      "    import openai\n",
      "    from docx import Document\n",
      "\n",
      "    def transcribe_audio(audio_file_path):\n",
      "        with open(audio_file_path, 'rb') as audio_file:\n",
      "            transcription = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
      "        return transcription['text']\n",
      "\n",
      "    audio_file_path = \"/path/to/file/audio.mp3\"\n",
      "    transcription = transcribe_audio(audio_file_path)\n",
      "    print(transcription)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Recommended Questions:\n",
      "\n",
      " Questions: Are there specific Python libraries required for transcribing audio with the Whisper API?\n",
      " Questions: What audio file formats are supported for transcription with the Whisper API in Python?\n",
      " Questions: How can I improve the accuracy of audio transcriptions when using the Whisper API?\n",
      " Questions: Can you explain the difference between the transcriptions and translations endpoints in the Whisper API?\n",
      " Questions: How does ChatGPT consume the manifest file and make calls to the developer-defined APIs?\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I transcribe audio using the transcriptions endpoint of the Whisper API in Python?\"\n",
    "response = semantic_matching(query, 5)\n",
    "content = 'content: '\n",
    "for match in response['matches']:\n",
    "    content+=match['metadata']['text']\n",
    "\n",
    "#combining semantic matching results and query\n",
    "input_query = f\"Question: {query}\\n Content: {content}\".replace(\"{\",'').replace(\"}\",'')\n",
    "\n",
    "print(get_llm_response(input_query))\n",
    "\n",
    "get_recommended_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "de5626ac-c280-49d2-97bc-a8fa649924dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: To transcribe audio using the transcriptions endpoint of the Whisper API in Python, you can use the following code:\n",
      "\n",
      "    import openai\n",
      "    from docx import Document\n",
      "\n",
      "    def transcribe_audio(audio_file_path):\n",
      "        with open(audio_file_path, 'rb') as audio_file:\n",
      "            transcription = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
      "        return transcription['text']\n",
      "\n",
      "    audio_file_path = \"/path/to/file/audio.mp3\"\n",
      "    transcription = transcribe_audio(audio_file_path)\n",
      "    print(transcription)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Recommended Questions:\n",
      "\n",
      " Questions: What is the purpose of the plugin manifest file, and where should it be hosted?\n",
      " Questions: What is the recommended approach for developers when exposing endpoints in a plugin?\n",
      " Questions: How do users activate and use plugins within the ChatGPT UI?\n",
      " Questions: What role does the AI model play in the plugin system, and how does it interact with APIs?\n",
      " Questions: How does ChatGPT consume the manifest file and make calls to the developer-defined APIs?\n"
     ]
    }
   ],
   "source": [
    "query =\"Can you explain the key components included in the plugin manifest file?\"\n",
    "response = semantic_matching(query, 5)\n",
    "content = 'content: '\n",
    "for match in response['matches']:\n",
    "    content+=match['metadata']['text']\n",
    "#combining semantic matching results and query\n",
    "input_qery = f\"Question: {query}\\n Content: {content}\".replace(\"{\",'').replace(\"}\",'')\n",
    "\n",
    "print(get_llm_response(input_query))\n",
    "\n",
    "get_recommended_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be320fde-0f70-4e61-8961-57c29da5123e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13a553-ba43-45f7-8e59-6e0da8452f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f92385-bf4b-4c9e-823b-21e1dfc02242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
